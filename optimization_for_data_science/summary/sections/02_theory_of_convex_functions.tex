\section{Theory of convex functions}

\subsection{Mathematical background}

\begin{theorem}[Cauchy-Schwarz inequality]
    Let $\vec{u},\vec{v}\in \R^d$, then \[
        |\transpose{\vec{u}} \vec{v}| \leq \|\vec{u}\| \|\vec{v}\|.
    \]
\end{theorem}

\begin{theorem}[Special case of H\"older's inequality]
    Let $\vec{u}, \vec{v} \in \R^d$, then \[
        |\transpose{\vec{u}} \vec{v}| \leq \| \vec{u} \|_\infty \| \vec{v} \|_1.
    \]
\end{theorem}

\begin{theorem}[Cosine theorem]
    Let $\vec{u}, \vec{v} \in \R^d$, then \[
        \| \vec{u} - \vec{v} \|^2 = \| \vec{u} \|^2 + \| \vec{v} \|^2 - 2 \transpose{\vec{u}}\vec{v}.
    \]
\end{theorem}

\begin{definition}[Spectral norm]
    Let $\mat{A} \in \R^{m \times d}$ , then \[
        \|\mat{A}\|_2 \doteq \max_{\vec{v}\in \R^d, \vec{v}\neq \vec{0}} \frac{\|\mat{A}\vec{v}\|}{\|\vec{v}\|} = \max_{\|\vec{v}\| = 1} \|\mat{A}\vec{v}\|.
    \]
\end{definition}

\begin{definition}[Positive semi-definiteness]
    A matrix $\mat{A} \in \R^{m \times d}$ is positive semi-definite over $\mathcal{X}$ if \[
        \transpose{\vec{x}} \mat{A} \vec{x} \geq 0, \quad \forall \vec{x} \in \mathcal{X}.
    \]
\end{definition}

Intuitively, this means that $\|\mat{A}\|_2$ is the largest factor by which a vector can be
stretched in length under the mapping $\vec{v}\mapsto\mat{A}\vec{v}$. It is equal to the principal
eigenvalue.

\begin{theorem}[Mean value theorem]
    Let $a<b$ be real numbers, and let $h: [a,b]\to \R$ be a continuous function that is differentiable
    on $(a,b)$. Then, there exists $c\in(a,b)$ such that \[
        h'(c) = \frac{h(b) - h(a)}{b-a}.
    \]
\end{theorem}

\begin{marginfigure}
    \centering
    \incfig{mean-value-theorem}
    \caption{Illustration of the mean value theorem.}
    \label{fig:mean-value-theorem}
\end{marginfigure}

Geometrically, this means that between $a$ and $b$, there is a tangent to the graph of $h$ that has
the same slope; see \Cref{fig:mean-value-theorem}.

\begin{theorem}[Fundamental theorem of calculus]
    Let $a<b$ be real numbers, and let $h: \dom{h} \to \R$ be a differentiable function on an open domain $\dom{h} \supset [a,b]$, and such that $h'$ is continuous on $[a,b]$. Then, \[
        h(b) - h(a) = \int_a^b h'(t)dt.
    \]
\end{theorem}

\subsection{Convex sets}

\begin{definition}[Convex set]
    A set $\mathcal{C}\subseteq \R^d$ is convex if the line segment between any two points of
    $\mathcal{C}$ lies in $\mathcal{C}$. \Ie, if \[
        \lambda \vec{x} + (1-\lambda)\vec{y} \in \mathcal{C}, \quad \forall \vec{x}, \vec{y} \in \mathcal{C}, \lambda \in [0,1].
    \]
\end{definition}

\begin{marginfigure}
    \centering
    \incfig{convex-set-example}
    \caption{Example of a convex set in $\R^2$.}
    \label{fig:convex-set-example}
\end{marginfigure}

\begin{marginfigure}
    \centering
    \incfig{non-convex-set-example}
    \caption{Example of a set that is not convex in $\R^2$.}
    \label{fig:non-convex-set-example}
\end{marginfigure}

\begin{lemma}
    Let $\mathcal{C}_1, \ldots, \mathcal{C}_n$ be convex sets, where $n$ can be infinitely large, then \[
        \mathcal{C} = \bigcap_{i=1}^n \mathcal{C}_i,
    \]
    is a convex set.
\end{lemma}

\subsection{Convex functions}

\begin{marginfigure}
    \centering
    \incfig{convexity}
    \caption{Illustration of the classic definition of convexity.}
    \label{fig:convexity}
\end{marginfigure}

\begin{definition}[Convexity]
    A function $f: \dom{f} \to \R$ is convex if (i) $\dom{f}$ is convex and (ii) we have \[
        f(\lambda \vec{x} + (1-\lambda) \vec{y}) \leq \lambda f(\vec{x}) + (1-\lambda) f(\vec{y}), \quad \forall \vec{x}, \vec{y} \in \dom{f}, \lambda\in[0,1].
    \]
\end{definition}

Geometrically, this condition means that the line segment connecting the two points
$(\vec{x},f(\vec{x}))$ and $(\vec{y},f(\vec{y})) \in \R^{d+1}$ lies pointwise above the graph of
$f$; see \Cref{fig:convexity}.

% \begin{marginfigure}
%     \centering
%     \incfig{epigraph}
%     \caption{Epigraph of a convex function.}
%     \label{fig:epigraph}
% \end{marginfigure}
%
% \begin{definition}[Epigraph]
%     The epigraph of a function $f: \R^d \to \R$ is defined as \[
%         \mathrm{epi}(f) \doteq \lft\{ (\vec{x},\alpha)\in \R^{d+1} \;\middle|\; \vec{x}\in\dom{f},\alpha\geq f(\vec{x}) \rgt\}.
%     \]
% \end{definition}
%
% \begin{lemma}
%     $f$ is a convex function if and only if $\mathrm{epi}(f)$ is a convex set.
% \end{lemma}

\begin{lemma}[Jensen's inequality]
    Let $f$ be convex, $\vec{x}_1,\ldots,\vec{x}_m\in\dom{f}$, $\lambda_1,\ldots,\lambda_m > 0$ such that $\sum_{i=1}^m \lambda_i = 1$, then \[
        f \lft( \sum_{i=1}^m \lambda_i \vec{x}_i \rgt) \leq \sum_{i=1}^{m} \lambda_i f(\vec{x}_i).
    \]
\end{lemma}

\begin{proof}
    We will prove Jensen's inequality by induction. The base case ($m=2$) is true by definition of
    convexity. Let Jensen's inequality hold for $m=k$. Consider $m=k+1$, then
    \begin{align*}
        f \lft( \sum_{i=1}^{k+1} \lambda_i \vec{x}_i \rgt) & = f \lft( \sum_{i=1}^k \lambda_i \vec{x}_i + \lambda_{k+1} \vec{x}_{k+1} \rgt)                                                                                                                              \\
                                                           & = f \lft( (1-\lambda_{k+1}) \sum_{i=1}^{k} \frac{\lambda_i}{1-\lambda_{k+1}} \vec{x}_i + \lambda_{k+1} \vec{x}_{k+1} \rgt)                                                                                  \\
                                                           & \leq (1-\lambda_{k+1}) f \lft( \sum_{i=1}^k \frac{\lambda_i}{1-\lambda_{k+1}} \vec{x}_i \rgt) + \lambda_{k+1} f(\vec{x}_{k+1}) \margintag{By definition of convexity.}                                      \\
                                                           & \leq (1-\lambda_{k+1}) \sum_{i=1}^k \frac{\lambda_i}{1-\lambda_{k+1}} f(\vec{x}_i) + \lambda_{k+1} f(\vec{x}_{k+1}) \margintag{Induction assumption, $\sum_{i=1}^k \frac{\lambda_i}{1-\lambda_{k+1}} = 1$.} \\
                                                           & = \sum_{i=1}^{k+1} \lambda_i f(\vec{x}_i).
    \end{align*}
    Since it holds for the base case and the induction step, Jensen's inequality must hold for all $m$.
\end{proof}

\begin{lemma} \label{lem:convex-continuous}
    Let $f$ be convex and suppose that $\dom{f}\subseteq \R^d$ is open, then $f$ is continuous.
\end{lemma}

\begin{definition}[Differentiable functions]
    Let $f: \dom{f}\to \R^m$ where $\dom{f}\subseteq \R^d$ is open. $f$ is called differentiable at
    $\vec{x}\in\dom{f}$ if there exists $\mat{A} \in \R^{m\times d}$ and an error function $r: \R^d
        \to \R^m$ defined around $\vec{0}\in \R^d$ such that for all $\vec{y}$ in some neighborhood
    of $\vec{x}$, \[
        f(\vec{y}) = f(\vec{x}) + \mat{A} (\vec{y} - \vec{x}) + r(\vec{y} - \vec{x}),
    \]
    where $\lim_{\vec{v}\to\vec{0}} \nicefrac{\| r(\vec{v}) \|}{\| \vec{v} \|} = \vec{0}$ (error
    function $r$ is sublinear around $\vec{0}$). $\mat{A}$ is unique and called the Jacobian matrix of
    $f$ at $\vec{x}$.
\end{definition}

\begin{marginfigure}[-6cm]
    \centering
    \incfig{differentiable-function}
    \caption{Graph of the affine function $f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}}(\vec{y}-\vec{x})$ is a tangent hyperplane to the graph of $f$ at $(\vec{x},f(\vec{x}))$.}
    \label{fig:differentiable-function}
\end{marginfigure}

\begin{marginfigure}
    \centering
    \incfig{first-order-convexity}
    \caption{Illustration of the first-order characterization of convexity (\Cref{lem:first-order-convexity}).}
    \label{fig:first-order-convexity}
\end{marginfigure}

\begin{lemma}[First-order convexity] \label{lem:first-order-convexity}
    Suppose that $\dom{f}$ is open and that $f$ is differentiable. In particular, the gradient \[
        \grad{f(\vec{x})}{} \doteq \lft[ \pdv{f(\vec{x})}{x_1}, \ldots, \pdv{f(\vec{x})}{x_d} \rgt]
    \]
    exists at every point $\vec{x}\in\dom{f}$.

    Then, $f$ is convex if and only if (i) $\dom{f}$ is convex and (ii) we have \[
        f(\vec{y}) \geq f(\vec{x}) + \grad{f(\vec{x})}{}^\top (\vec{y}-\vec{x}), \quad \forall \vec{x}, \vec{y} \in \dom{f}.
    \]
\end{lemma}

Geometrically, this means that the graph is above all tangent hyperplanes; see
\Cref{fig:first-order-convexity}.

\begin{proof}
    We will first prove that convexity implies that the first-order definition holds. Then, we will prove that the first-order definition implies convexity, making the definitions equivalent for differentiable functions $f$.

    \begin{enumerate}
        \item[$\Rightarrow$:] Suppose $f$ is convex. Then, for all $t\in(0,1)$,
              \begin{align*}
                       &  & f((1-t)\vec{x} + t \vec{y})     & \leq (1-t) f(\vec{x}) + t f(\vec{y}) \margintag{Definition of convexity.}                                                                                                                                                            \\
                  \iff &  & f(\vec{x} + t(\vec{y}-\vec{x})) & \leq f(\vec{x}) + t(f(\vec{y}) - f(\vec{x}))                                                                                                                                                                                         \\
                  \iff &  & f(\vec{y})                      & \geq f(\vec{x}) + \frac{f(\vec{x} + t(\vec{y}-\vec{x}))-f(\vec{x})}{t}                                                                                                                                                               \\
                       &  &                                 & = f(\vec{x}) + \frac{\transpose{\grad{f(\vec{x})}{}} t(\vec{y} - \vec{x}) + r(t(\vec{y}-\vec{x}))}{t} \margintag{$r(\cdot)$ is an error function such that $\lim_{\vec{v} \to \vec{0}} \frac{\| r(\vec{v}) \|}{\| \vec{v} \|} = 0$.} \\
                       &  &                                 & = f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}}(\vec{y}-\vec{x}) + \underbrace{\frac{r(t(\vec{y}-\vec{x}))}{t}}_{\to 0 \text{ for } t\to 0}.
              \end{align*}

        \item[$\Leftarrow$:] Define $\vec{z} \doteq \lambda \vec{x} + (1-\lambda) \vec{y} \in \dom{f}$ for $\lambda \in[0,1]$ by convexity of $\dom{f}$. Then, we have the following inequalities,
              \begin{align*}
                  f(\vec{x}) & \geq f(\vec{z}) + \transpose{\grad{f(\vec{z})}{}} (\vec{x}-\vec{z})  \\
                  f(\vec{y}) & \geq f(\vec{z}) + \transpose{\grad{f(\vec{z})}{}} (\vec{y}-\vec{z}).
              \end{align*}
              This implies the following,
              \begin{align*}
                  \lambda f(\vec{x}) + (1-\lambda) f(\vec{y}) & \geq \lambda \lft( f(\vec{z}) + \transpose{\grad{f(\vec{z})}{}} (\vec{x}-\vec{z}) \rgt) + (1-\lambda) \lft( f(\vec{z}) + \transpose{\grad{f(\vec{z})}{}} (\vec{y}-\vec{z}) \rgt) \\
                                                              & = f(\vec{z}) + \lambda \transpose{\grad{f(\vec{z})}{}} (\vec{x}-\vec{z}) + (1-\lambda) \transpose{\grad{f(\vec{z})}{}} (\vec{y}-\vec{z})                                         \\
                                                              & = f(\vec{z}) + \transpose{\grad{f(\vec{z})}{}} \lft( \lambda (\vec{x}-\vec{z}) + (1-\lambda) (\vec{y}-\vec{z}) \rgt)                                                             \\
                                                              & = f(\vec{z}) + \transpose{\grad{f(\vec{z})}{}} \lft( \lambda \vec{x} + (1-\lambda) \vec{y} - (\lambda \vec{z} + (1-\lambda) \vec{z}) \rgt)                                       \\
                                                              & = f(\lambda \vec{x} + (1-\lambda) \vec{y}).
              \end{align*}
    \end{enumerate}
\end{proof}

\begin{lemma}[First-order convexity alternative]
    \label{lem:first-order-convexity-alt}

    Suppose that $\dom{f}$ is open and that $f$ is differentiable. Then, $f$ is convex if and only if
    (i) $\dom{f}$ is convex and (ii) \[
        \transpose{(\grad{f(\vec{y})}{} - \grad{f(\vec{x})}{})} (\vec{y} - \vec{x}) \geq 0, \quad \forall \vec{x}, \vec{y} \in \dom{f}.
    \]
\end{lemma}

\begin{proof}
    We will first prove that it holds from left to right, and then from right to left.

    \begin{enumerate}
        \item[$\Rightarrow$:] If $f$ is convex, the first-order characterization of convexity (\Cref{lem:first-order-convexity}) yields the following $\forall \vec{x}, \vec{y} \in \dom{f}$,
              \begin{align*}
                  f(\vec{x}) & \geq f(\vec{y}) + \transpose{\grad{f(\vec{y})}{}} (\vec{x}-\vec{y})  \\
                  f(\vec{y}) & \geq f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}} (\vec{y}-\vec{x}),
              \end{align*}
              for all $\vec{x},\vec{y}\in\dom{f}$. Adding up these two inequalities yields
              \begin{align*}
                       &  & f(\vec{x}) + f(\vec{y}) & \geq f(\vec{y}) + \transpose{\grad{f(\vec{y})}{}} (\vec{x}-\vec{y}) + f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}} (\vec{y}-\vec{x}) \\
                  \iff &  & 0                       & \geq \transpose{\grad{f(\vec{y})}{}} (\vec{x}-\vec{y}) + \transpose{\grad{f(\vec{x})}{}} (\vec{y}-\vec{x})                           \\
                       &  &                         & = \transpose{(\grad{f(\vec{y})}{} - \grad{f(\vec{x})}{})} (\vec{x} - \vec{y})                                                        \\
                  \iff &  & 0                       & \leq \transpose{(\grad{f(\vec{y})}{} - \grad{f(\vec{x})}{})} (\vec{y} - \vec{x}).
              \end{align*}
        \item[$\Leftarrow$:] Define $\vec{z} \doteq (1-t) \vec{x} + t\vec{y} \in \dom{f}$ for
              $\vec{x},\vec{y}\in\dom{f}, t \in (0,1)$ by convexity of $\dom{f}$. Observe that
              $\vec{z} = \vec{x} + t(\vec{y}-\vec{x})$. Then, we have the following inequality, according
              to the monotonicity of the gradient,
              \begin{align*}
                  \transpose{(\grad{f(\vec{z})}{} - \grad{f(\vec{x})}{})} (\vec{z} - \vec{x})                                           & \geq 0                            \\
                  \transpose{(\grad{f(\vec{x} + t(\vec{y}-\vec{x}))}{} - \grad{f(\vec{x})}{})} (\vec{x} + t(\vec{y}-\vec{x}) - \vec{x}) & \geq 0                            \\
                  \transpose{(\grad{f(\vec{x} + t(\vec{y}-\vec{x}))}{} - \grad{f(\vec{x})}{})} (\vec{y}-\vec{x})                        & \geq 0. \margintag{Divide by $t$}
              \end{align*}
              Let $h(t) = f(\vec{x} + t(\vec{y}-\vec{x}))$, then
              $h'(t) = \transpose{\grad{f(\vec{x} + t(\vec{y}-\vec{x}))}{}} (\vec{y}-\vec{x})$. Hence,
              we can rewrite the inequality as the following, \[
                  h'(t) \geq \transpose{\grad{f(\vec{x})}{}} (\vec{y} - \vec{x}).
              \]
              By the mean value theorem, there exists $c\in (0,1)$ such that $h'(c) = h(1) - h(0)$. \Ie, \[
                  h'(c) = f(\vec{y}) - f(\vec{x}).
              \]
              Thus, we can rewrite the inequality to the following,
              \begin{align*}
                  f(\vec{y}) & = f(\vec{x}) + h'(c)                                                   \\
                             & \geq f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}} (\vec{y} - \vec{x}),
              \end{align*}
              which is the first-order characterization of convexity (\Cref{lem:first-order-convexity}).
    \end{enumerate}
\end{proof}

Geometrically, this means that the gradient is monotonic.

\begin{lemma}[Second-order convexity]
    \label{lem:second-order-convexity}

    Suppose that $\dom{f}$ is open and that $f$ is twice differentiable. In particular, the Hessian \[
        \hess{f(\vec{x})}{} = \begin{bmatrix}
            \pdv[order=2]{f(\vec{x})}{x_1}         & \cdots & \pdv[order={1,1}]{f(\vec{x})}{x_1,x_d} \\
            \vdots                                 & \ddots & \vdots                                 \\
            \pdv[order={1,1}]{f(\vec{x})}{x_d,x_1} & \cdots & \pdv[order=2]{f(\vec{x})}{x_d}
        \end{bmatrix}
    \]
    exists at every point $\vec{x}\in\dom{f}$ and is symmetric.

    Then, $f$ is convex if and only if (i) $\dom{f}$ is convex and (ii) the Hessian
    $\hess{f(\vec{x})}{}$ is positive semi-definite for all $\vec{x} \in \dom{f}$.
\end{lemma}

Geometrically, this means that $f$ has non-negative curvature everywhere. \Ie, the growth rate
should be growing.

\begin{corollary}[Operations that preserve convexity]
    Let $f_1,\ldots,f_m$ be convex functions, $\lambda_1,\ldots,\lambda_m \geq 0$, then \[
        f \doteq \max_{i=1}^m f_i,
    \]
    and \[
        f \doteq \sum_{i=1}^m \lambda_i f_i.
    \]
    are convex on $\dom{f} \doteq \bigcap_{i=1}^m \dom{f_i}$.

    Let $f$ be a convex function with $\dom{f} \subseteq \R^d, g: \R^m \to \R^d$ an affine function,
    \ie $g(\vec{x}) = \mat{A}\vec{x} + \vec{b}$ for some $\mat{A}\in \R^{d\times m},\vec{b}\in \R^d$.
    Then, the function $f \circ g$ is convex on $\dom{f \circ g} \doteq \{ \vec{x}\in R^m \mid
        g(\vec{x}) \in \dom{f} \}$.
\end{corollary}

\begin{marginfigure}
    \centering
    \incfig{max-convexity}
    \caption{The maximum operator over $m$ convex functions is a convex function. As can be seen, the epigraph of $f$ is convex.}
    \label{fig:max-convexity}
\end{marginfigure}

\subsection{Minimizing convex functions}

\begin{definition}[Local minimum]
    A local minimum of $f: \dom{f} \to \R$ is a point $\vec{x}$ such that there exists $\epsilon > 0$ with \[
        f(\vec{x}) \leq f(\vec{y}), \quad \forall \vec{y} \in \dom{f} \text{ s.t. } \| \vec{y}-\vec{x} \| < \epsilon.
    \]
\end{definition}

\begin{lemma}
    Let $\vec{x}^\star$ be a local minimum of a convex function $f: \dom{f} \to \R$. Then,
    $\vec{x}^\star$ is a global minimum, meaning that
    $f(\vec{x}^\star) \leq f(\vec{y}) \;\; \forall \vec{y} \in \dom{f}$.
\end{lemma}

\begin{proof}
    Proof by contradiction. Suppose there exists $\vec{y}\in\dom{f}$ such that
    $f(\vec{y}) < f(\vec{x}^\star)$. Define $\vec{y}' \doteq \lambda \vec{x}^\star + (1-\lambda) \vec{y}$
    for $\lambda\in(0,1)$. From convexity, we get that $f(\vec{y}') < f(\vec{x}^\star)$, because
    $f(\vec{y}) < f(\vec{x}^\star)$. If we choose $\lambda$ so close to 1 such that
    $\| \vec{y}' - \vec{x}^\star \| < \epsilon$ gives the inequality $f(\vec{x}^\star) \leq f(\vec{y}')$.
    This yields a contradiction, thus there cannot exist a $\vec{y}\in\dom{f}$ such that
    $f(\vec{y}) < f(\vec{x}^\star)$, meaning that $f(\vec{x}^\star) \leq f(\vec{y})$ for all $\vec{y}\in\dom{f}$.
\end{proof}

\begin{lemma}
    Suppose that $f$ is convex and differentiable over an open domain $\dom{f}$. Let $\vec{x}\in\dom{f}$. If $\grad{f(\vec{x})}{} = \vec{0}$ (critical point), then $\vec{x}$ is a global minimum.
\end{lemma}

\begin{proof}
    Suppose that $\grad{f(\vec{x})}{} = \vec{0}$. According to the first-order characterization of convexity, we have \[
        f(\vec{y}) \geq f(\vec{x}) + \underbrace{\transpose{\grad{f(\vec{x})}{}} (\vec{y}-\vec{x})}_{=0} = f(\vec{x})
    \]
    for all $\vec{y} \in \dom{f}$, so $\vec{x}$ is a global minimum.
\end{proof}

\begin{lemma}[Constrained first-order optimality condition]
    \label{lem:optim}

    Suppose that $f: \dom{f} \to \R$ is convex and differentiable over an open domain $\dom{f}
        \subseteq \R^d$ and let $\mathcal{X} \subseteq \dom{f}$ be a convex set. A point $\vec{x}^\star \in
        \mathcal{X}$ is a minimizer of $f$ over $\mathcal{X}$ if and only if \[
        \transpose{\grad{f(\vec{x}^\star)}{}}(\vec{x} - \vec{x}^\star) \geq 0, \forall \vec{x} \in \mathcal{X}.
    \]
\end{lemma}

\begin{definition}[Strict convexity]
    A function $f: \dom{f}\to \R$ is strictly convex if (i) $\dom{f}$ is convex and (ii) for all $\vec{x} \neq \vec{y}\in \dom{f}$ and all $\lambda\in(0,1)$, we have \[
        f(\lambda \vec{x} + (1-\lambda) \vec{y}) < \lambda f(\vec{x}) + (1-\lambda) f(\vec{y}).
    \]
\end{definition}

An example of a strictly convex function can be found in \Cref{fig:convexity}, while an example of
a function that is not strictly convex can be found in \Cref{fig:non-strict-convexity}.

\begin{marginfigure}
    \centering
    \incfig{non-strict-convexity}
    \caption{A non-strictly convex function with one global minimum.}
    \label{fig:non-strict-convexity}
\end{marginfigure}

\begin{lemma}
    Let $f: \dom{f} \to \R$ be strictly convex. Then, $f$ has --- at most --- one global minimum.
\end{lemma}

% But, how do we know a minimizer exists? To be able to find this out, we must use the Weierstrass
% theorem.
%
% \begin{definition}[Sublevel set]
%     $f: \R^d \to \R, \alpha\in \R$. The set $f^{\leq \alpha} \doteq \{ \vec{x}\in \R^d \mid f(\vec{x}) \leq \alpha \}$ is the $\alpha$-sublevel set of $f$.
% \end{definition}
%
% \begin{marginfigure}
%     \centering
%     \incfig{sublevel-set}
%     \caption{Illustration of a sublevel set of a non-convex function.}
%     \label{fig:sublevel-set}
% \end{marginfigure}
%
% \begin{theorem}[Weierstrass]
%     Let $f: \R^d \to \R$ be a continuous function, and suppose there is a non-empty and bounded
%     sublevel set $f^{\leq \alpha}$. Then, $f$ has a global minimum.
% \end{theorem}
%
% \begin{proof}
%     We know that $f$ attains a minimum over the closed and bounded set $f^{\leq \alpha}$ at some
%     $\vec{x}^\star$, because $f$ is continuous. This $\vec{x}^\star$ is also a global minimum as it
%     has value $f(\vec{x}^\star) \leq \alpha$, while any $\vec{x} \not\in f^{\leq \alpha}$ has value
%     $f(\vec{x}) > \alpha \geq f(\vec{x}^\star)$.
% \end{proof}
%
% Since convex functions are continuous by \Cref{lem:convex-continuous}, the Weierstrass theorem also
% applies to convex functions. Thus, to figure out whether a convex function has a minimizer, we need
% to find any non-empty and bounded sublevel set of this function.

\subsection{Convex programming}

In standard form, optimization problems look like the following, \[
    \begin{aligned}
         & \text{minimize}   &  & f_0(\vec{x})                                 \\
         & \text{subject to} &  & f_i(\vec{x}) \leq 0, \quad i = 1,\ldots,m    \\
         &                   &  & h_j(\vec{x}) = 0,    \quad j = 1, \ldots, p.
    \end{aligned}
\]
The problem domain is then \[
    \mathcal{X} = \lft( \bigcap_{i=0}^m \dom{f_i} \rgt) \cap \lft( \bigcap_{j=1}^p \dom{h_j} \rgt).
\]
In a convex program, all $f_i$ are convex functions, and all $h_j$ are affine functions with domain
$\R^d$.

\begin{definition}[Lagrange dual function]
    Given an optimization problem in standard form, its Lagrangian is the function $L: \mathcal{X} \times \R^m \times \R^p \to \R$ given by \[
        L(\vec{x},\vec{\lambda},\vec{\nu}) = f_0(\vec{x}) + \sum_{i=1}^{m} \lambda_i f_i(\vec{x}) + \sum_{j=1}^{p} \nu_j h_j(\vec{x}).
    \]
    The $\lambda_i,\nu_j$ are called Lagrange multipliers. The Lagrange dual function is the function
    $g: \R^m \times \R^p \to \R \cup \{ -\infty \}$ defined by \[
        g(\vec{\lambda},\vec{\nu}) = \inf_{\vec{x}\in \mathcal{X}} L(\vec{x},\vec{\lambda},\vec{\nu}).
    \]
\end{definition}

Only the $(\vec{\lambda},\vec{\nu})$ pairs with $g(\vec{\lambda},\vec{\nu}) > -\infty$ are
interesting.

\begin{lemma}[Weak Lagrange duality]
    Let $\vec{x}$ be a feasible solution --- $f_i(\vec{x}) \leq 0, \forall i \in [m]$ and $h_j(\vec{x})
        = 0, \forall j \in [p]$. Let $g$ be the Lagrange dual function and $\vec{\lambda}\in \R^m,
        \vec{\nu}\in \R^p$ such that $\vec{\lambda} \geq \vec{0}$. Then, \[
        g(\vec{\lambda},\vec{\nu}) \leq f_0(\vec{x}).
    \]
\end{lemma}

\begin{proof}
    \[
        g(\vec{\lambda},\vec{\nu}) \leq L(\vec{x},\vec{\lambda},\vec{\nu}) = f_0(\vec{x}) + \underbrace{\sum_{i=1}^{m} \lambda_i f_i(\vec{x})}_{\leq 0} + \underbrace{\sum_{j=1}^{p} \nu_j h_j(\vec{x})}_{=0} \leq f_0(\vec{x}).
    \]
\end{proof}

However, we want to know what the best lower bound is that we can get in this way. For this, we
must choose $\vec{\lambda} \geq \vec{0}, \vec{\nu}$ such that $g(\vec{\lambda},\vec{\nu})$ is
maximized. This can be phrased as another optimization problem, called the Lagrange dual, \[
    \begin{aligned}
         & \text{maximize}   &  & g(\vec{\lambda},\vec{\nu})  \\
         & \text{subject to} &  & \vec{\lambda} \geq \vec{0}.
    \end{aligned}
\]

\begin{corollary}
    The Lagrange dual is a convex program, even if the primal is not.
\end{corollary}

\begin{corollary}
    By weak duality, the supremum value of the Lagrange dual is a lower bound of the infimum value of the primal problem, \[
        \sup g(\vec{\lambda}, \vec{\nu}) \leq \inf f_0(\vec{x}).
    \]
\end{corollary}

\begin{theorem}
    Suppose that a convex program has a feasible solution $\tilde{\vec{x}}$ that in addition satisfies
    $f_i(\tilde{\vec{x}}) < 0, \forall i \in [m]$ (Slater point). Then, the infimum value of the primal
    equals the supremum value of its Lagrange dual. Moreover, if the value is finite, it is attained
    by a feasible solution of the dual, \[
        \inf f_0(\vec{x}) = \max g(\vec{\lambda},\vec{\nu}).
    \]
\end{theorem}

A case of particular interest is that strong duality holds and the joint value is attained in both
the primal and dual problem.\sidenote{This would mean that $\min f_0(\vec{x}) = \max
        g(\vec{\lambda},\vec{\nu})$.} If all $f_i$ and $h_j$ are differentiable, then the
Karush-Kuhn-Tucker (KKT) conditions provide necessary and, under convexity, also sufficient
conditions for this case to occur.

\marginnote[1cm]{If the primal and dual have zero duality gap, then $\min f_0(\vec{x}) = \max g(\vec{\lambda}, \vec{\nu})$.}

\begin{definition}[Zero duality gap]
    Let $\tilde{\vec{x}}$ be feasible for the primal and $(\tilde{\vec{\lambda}},\tilde{\vec{\nu}})$
    feasible for the Lagrange dual. The primal and dual solutions $\tilde{\vec{x}}$ and
    $(\tilde{\vec{\lambda}},\tilde{\vec{\nu}})$ are said to have zero duality gap if
    $f_0(\tilde{\vec{x}}) = g(\tilde{\vec{\lambda}}, \tilde{\vec{\nu}})$.
\end{definition}

The consequence of zero duality gap is the \textit{master equation},
\begin{align*}
    f_0(\tilde{\vec{x}}) & = g(\tilde{\vec{\lambda}},\tilde{\vec{\nu}}) \margintag{Zero duality gap.}                                                                                                          \\
                         & = \inf_{\vec{x}\in \mathcal{X}} \lft( f_0(\vec{x}) + \sum_{i=1}^{m} \tilde{\lambda}_i f_i(\vec{x}) + \sum_{j=1}^{p} \tilde{\nu}_j h_j(\vec{x}) \rgt) \margintag{Definition of $g$.} \\
                         & \leq f_0(\tilde{\vec{x}}) + \sum_{i=1}^{m} \tilde{\lambda}_i f_i(\tilde{\vec{x}}) + \sum_{j=1}^{p} \tilde{\nu}_j h_j(\tilde{\vec{x}}) \margintag{Infimum is a lower bound.}         \\
                         & \leq f_0(\tilde{\vec{x}}), \margintag{All constraints are less than or equal to zero.}
\end{align*}
which means that all inequalities turn into equalities. Thus, \[
    \tilde{\lambda}_i f_i(\tilde{\vec{x}}) = 0, \quad i = 1,\ldots,m,
\]
which is called complementary slackness, because if $\tilde{\lambda}_i \neq 0$, then
$f_i(\tilde{\vec{x}}) = 0$, and vice versa. Furthermore, if all $f_i$ and $h_j$ are differentiable,
then \[
    \grad{f_0(\tilde{\vec{x}})}{} + \sum_{i=1}^{m} \tilde{\lambda}_i \grad{f_i(\tilde{\vec{x}})}{} + \sum_{j=1}^{p} \tilde{\nu}_j \grad{h_j(\tilde{\vec{x}})}{} = \vec{0},
\]
which is called the vanishing Lagrangian gradient condition.

In summary, we get the following result.

\begin{theorem}[Karush-Kuhn-Tucker necessary conditions]
    Let $\tilde{\vec{x}}$ and $(\tilde{\vec{\lambda}}, \tilde{\vec{\nu}})$ be feasible solutions of
    the primal optimization problem and its Lagrange dual, respectively, with zero duality gap. If
    all $f_i$ and $h_j$ are differentiable, then
    \begin{align*}
        \tilde{\lambda}_i f_i(\tilde{\vec{x}}) = 0, \quad i = 1,\ldots,m \margintag{Complementary slackness} \\
        \grad{f_0(\tilde{\vec{x}})}{} + \sum_{i=1}^{m} \tilde{\lambda}_i \grad{f_i(\tilde{\vec{x}})}{} + \sum_{j=1}^{p} \tilde{\nu}_j \grad{h_j(\tilde{\vec{x}})}{} = \vec{0}. \margintag{Vanishing Lagrangian gradient}
    \end{align*}
\end{theorem}

If we have a convex program, then the KKT conditions are sufficient for zero duality gap. The
motivation behind this is that they may be easier to solve for than solving the primal optimization
problem. However, we cannot always count on the KTT conditions to be solvable, because they are
only guaranteed if there are primal and dual solutions of zero duality gap. But, if the primal has
a Slater point\sidenote{A point $\tilde{\vec{x}}$ such that $f_i(\tilde{\vec{x}}) < 0$ for all
    $i=1,\ldots,m$.}, then the KTT conditions are equivalent to the existence of a primal minimizer.

The interior-point method is a general algorithm for solving convex programs, which has a high
iteration cost. During this course, we will consider simple algorithms with a low cost per
iteration (but possibly a high number of iterations).
