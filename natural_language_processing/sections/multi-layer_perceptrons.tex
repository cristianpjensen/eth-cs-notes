\section{Multi-layer perceptrons} \label{sec:mlp}

For log-linear models to find an appropriate model, the data has to be linearly
separable.\sidenote{If the data is non-linear, we can use a feature function
$\vec{f}$ that makes the feature vector linearly separable by hacking the
non-linearity into the feature function (\eg, if the data is ellipsoidal, we
would choose an ellipsoidal feature function.) But, this requires us to know
the decision boundary's shape a priori to set the feature function.} The
solution to this problem is the \textit{multi-layer perceptron}
(MLP) \citep{haykin1994neural}. They jointly learn a non-linear feature function
with the model's parameters. MLPs consist of $n$ alternating linear projections
and non-linearities,\sidenote{The non-linearities are very important, because
otherwise we would just get a linear model, since a stack of linear
transformations are equal to some single linear transformation. Thus, if the
model would not contain non-linearities, we would not gain any expressiveness.}
\begin{align*}
  \vec{h}_n &= \sigma_n \lft( \transpose{\mat{W}_n} \vec{h}_{n-1} \rgt) \\
  \bm{h}_1 &= \sigma_1 \lft( \transpose{\mat{W}_1} \vec{e}(x) \rgt)
,\end{align*}
where $\vec{e}(x)\in\R^d$ is a continuous vector encoding of the input $x$.
Then, we can combine this non-linear feature representation of the input with
the parameters to obtain a categorical probability distribution, \[
  \text{softmax} \lft( \transpose{\vec{\theta}} \vec{h}_n \rgt)
.\]
Basically, the only difference is that we now learn the feature function.

Using backpropagation, we can compute the derivative of a weight matrix
$\mat{W}_k$ of an MLP as follows,
\begin{align*}
  \pdv{\ell}{\mat{W}_k} &= \pdv{\ell}{y} \pdv{y}{\vec{h}_n} \lft( \prod_{m=k+1}^n \pdv{\vec{h}_m}{\vec{h}_{m-1}} \rgt) \pdv{\vec{h}_k}{\mat{W}_k} \\
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_n} \lft( \prod_{m=k+1}^n \pdv*{\sigma_m \lft( \transpose{\mat{W}_m} \vec{h}_{m-1} \rgt)}{\vec{h}_{m-1}} \rgt) \pdv*{\sigma_k \lft( \transpose{\mat{W}_k} \vec{h}_{k-1} \rgt)}{\mat{W}_k} \\
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_n} \lft( \prod_{m=k+1}^n \sigma_m' \lft( \transpose{\mat{W}_m} \vec{h}_{m-1} \rgt) \mat{W}_m \rgt) \sigma_k' \lft( \transpose{\mat{W}_k} \vec{h}_{k-1} \rgt) \vec{h}_{k-1}
  \intertext{If we use the ReLU activation function, this becomes the following,}
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_n} \lft( \prod_{m=k+1}^n \mathbb{1} \{ \transpose{\mat{W}_m} \vec{h}_{m-1} > 0 \} \cdot \mat{W}_m \rgt) \mathbb{1} \{ \transpose{\mat{W}_k} \vec{h}_{k-1} > 0 \} \cdot \vec{h}_{k-1}
.\end{align*}
This is the cause of ``dead`` neurons. When their values become negative, their
derivative becomes 0. Thus, there is no way for gradient descent to learn
anymore. Also, the derivative of all children of ``dead`` neurons become 0,
because of the chain rule.

\subsection{Word embeddings}

To be able to use MLPs, we would need a continuous vector encoding of
words/sentences. A naive idea would be to encode one-hot word counts of
sentences. However, this approach discards word ordering information. A naive
solution would be to then encode $n$-grams to regain some word ordering
information, however the vectors would explode in size to $\bigo{|V|^n}$, where
$|V|$ is the vocabulary size.

A great idea is to use unsupervised learning to train word embeddings on large
corpora.\sidenote{Easy to get due to the internet.} The first model that made
use of this idea is Skip-Gram \citep{mikolov2013efficient}. Its high-level idea
is that it predicts whether a word $w'$ is in the context of a word
$w$.\sidenote{``You shall know a word by the company it keeps.``} The weights
of this model are the word embeddings and they are used to predict the model's
objective. The idea is then that the model needs a good representation of the
words to be able to do this task successfully, thus we can use the embeddings
that this model trains for other tasks.

The first step of Skip-Gram is to preprocess the corpus. This is done by
collecting positive and negative samples.\sidenote{A reason for collecting
negative samples is because then we do not need to compute the normalizing
constant $Z(w)$. This is due to the fact that we are simply maximizing the
output for positive samples and minimizing the output for negative samples
while training. But, if we did not have negative samples, we would have to
normalize the output to be a probability between 0 and 1, because otherwise we
would not know whether the output is good or not.} To collect positive samples,
it goes through all words $w$ and collects all words $w'$ in the context of
$w$. Then, it randomly generates other words $w'$ as negative samples, \ie,
they are not in the context of $w$, and adds them to the dataset.

Then, we use the embeddings to predict whether a context word is in a word's
context,
\begin{align*}
  p(c\mid w) &= \frac{1}{Z(w)} \exp \lft( \transpose{\vec{e}_w(w)} \vec{e}_c(c) \rgt) \\
  Z(w) &= \sum_{c \in V} \exp \lft( \transpose{\vec{e}_w(w)} \vec{e}_c(c) \rgt)
.\end{align*}
We use two different embedding types, because if $w=c$, then
$\transpose{\vec{e}(w)} \vec{e}(c)$ would be positive and thus the probability
very high, even though this case is very unlikely, because words do usually not
appear in their own context. Thus, to mitigate this, we use two different
context and word embeddings.

\subsection{Sentiment analysis}

\begin{figure}[h!]
    \centering
    \incfig{sentiment-analysis}
    \caption{Architecture of a simple sentiment classifier, where
    $f_{\vec{\theta}}$ is a multi-layer perceptron.}
    \label{fig:sentiment-analysis}
\end{figure}

An example application is sentiment analysis \citep{pang2008opinion}, which
classifies sentences as positive or negative. MLPs and word embeddings can be
used for this and work very well \citep{iyyer2015deep}. The model first embeds
all the words in the sentence and pools them together into one vector
representation of the sentence.\sidenote{Pooling together discards word order,
but for this simple task, it will still work very well.} Then, this vector is
passed into an MLP, which is used as input to a softmax with a single output
that tells us how positive the sentence is.
