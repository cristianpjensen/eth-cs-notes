\section{Reinforcement learning} \label{sec:rl}

\begin{marginfigure}
    \centering
    \incfig{reinforcement-learning}
    \caption{In reinforcement learning, an agent interacts with its environment.
    After playing an action $a_t$, it observes reward $r_t$ and its new state
    $x_{t+1}$.}
    \label{fig:reinforcement-learning}
\end{marginfigure}

In \textit{reinforcement learning} (RL), we are concerned with acting in
unknown environments. These environments are still modeled by MDPs, but in RL,
we do not have access to the transition probabilities $p$ and reward function
$r$. Thus, RL is at the intersection of probabilistic planning (MDPs) and
learning, \ie, everything we have learned thus far comes together here.

\begin{remark}
  We will start by assuming that the state-action space is finite. Then, we will
  move on to potentially infinite state spaces. After that, we will learn about
  infinite action spaces.
\end{remark}

Since the environment is unknown, we need to explore the state-action space to
find where the reward lies. However, we also want to act optimally by
exploiting what we have learned thus far. This is called the
exploration/exploitation dilemma and is what algorithms need to solve.

Another way that reinforcement learning differs from supervised learning is
that data depends on past actions. Trajectory data looks like the following, \[
  \tau = (\langle x_0, a_0, r_0, x_1 \rangle, \langle x_1, a_1, r_1, x_2 \rangle, \ldots)
.\]

We differentiate between RL algorithms in two major ways. The first is whether
the algorithm has control over its data: an algorithm is called
\textit{on-policy} if it controls its own actions from which it learns, and
\textit{off-policy} if it can learn from any data. Furthermore, we
differentiate between \textit{model-based} and \textit{model-free} algorithms.
Model-based algorithms learn the underlying MDP and solve it using value or
policy iteration. Model-free algorithms only learn the value function, since,
due to Bellman's theorem, that is all that is needed to act optimally.

\begin{table}[t]
  \caption{RL algorithms covered in this text with their types.}
  \label{tab:rl-algos}
  \centering
  \begin{tabular}[c]{llll}
    \toprule
    \textbf{Algorithm} & \multicolumn{2}{c}{\textbf{Classification}} & \textbf{Space compl.} \\
    \midrule
    $\epsilon$-greedy & On/off-policy & Model-based & $\bigo{|\mathcal{A}| \cdot |\X|^2}$ \\
    $R_{\max}$ & On/off-policy & Model-based & $\bigo{|\mathcal{A}| \cdot |\X|^2}$ \\
    TD-learning & On-policy & Model-free & $\bigo{|\X|}$ \\
    Q-learning & Off-policy & Model-free & $\bigo{|\mathcal{A}| \cdot |\X|}$ \\
    Deep Q Network & Off-policy & Model-free \\
    REINFORCE & On-policy & Model-free \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Model-based}

In model-based RL, we learn the MDP, \ie, we estimate the transition
probabilities $p(x'\mid x,a)$ and reward function $r(x,a)$ from the data,
\begin{align*}
  \hat{p}(x'\mid x, a) &= \frac{\mathrm{count}(x' \mid x, a)}{\mathrm{count}(x, a)} \\
  \hat{r}(x, a) &= \frac{1}{\mathrm{count}(x,a)} \sum_{\substack{t=0 \\ x_t = x \\ a_t = a}}^\infty r_t
\end{align*}
Then, we optimize the policy by value or policy iteration, based on the
estimated MDP.

\paragraph{$\epsilon$-greedy.}

\marginnote{A sequence $x_t$ satisfies the Robbins Monro conditions if \[
  \sum_{t=0}^\infty x_t = \infty, \hspace{1em} \sum_{t=0}^\infty x_t^2 < \infty .\]
\Eg, $x_t = \frac{1}{t}.$}

At iteration $t$, pick random action with probability $\epsilon_t$, or best
action (according to internal MDP) with probability $1-\epsilon_t$. Guaranteed
to converge to optimal policy if the $\epsilon_t$ sequence satisfies the
Robbins Monro conditions. The advantage of this method is that it is extremely
simple and has a clear interpretation \wrt the exploration-exploitation
dilemma. The disadvantage is that it does not quickly eliminate clearly
suboptimal actions. This is because it explores the state space in an
uninformed manner. In other words, it explores while ignoring all past
experience.

\paragraph{$R_{\max}$ algorithm.}

$R_{\max}$ solves the problem of $\epsilon$-greedy by using the
\textit{Optimism in the face of uncertainty} principle. It assumes that any
unexplored states are ``fairy tale`` states with high reward. More formally, if
$r(x,a)$ is unknown, we set $\hat{r}(x,a)=R_{\max}$. Similarly, if $p(x'\mid
x,a)$ is unknown, we set $\hat{p}(x^\star\mid x,a)=1$ for some ``fairy tale`` state,
\begin{align*}
  \hat{p}(x^\star\mid x^\star, a) &= 1 & \forall a \in \mathcal{A} \\
  \hat{r}(x^\star, a) &= R_{\max} & \forall a \in \mathcal{A}
.\end{align*}
This gives us an algorithm that has a bias toward exploring, but once it has
explored a part of the state-action space, and observed it to be suboptimal, it
can quickly eliminate it. Furthermore, the algorithm does not have to
explicitly choose between exploration and exploitation, because it is done by
assuming that the unexplored states are optimal.

\subsection{Model-free}

The problem with model-based RL is that it has high space requirements for
storing the MDP, \ie, $\bigo{|\mathcal{A}|\cdot |\mathcal{X}|^2}$. Furthermore,
it requires repeatedly solving the underlying MDP, which is expensive with
policy or value iteration. In \textit{model-free} RL, we estimate the value
function directly, because that is all we need to act optimally, according to
Bellman's theorem. Thus, we also do not need to do any planning, eliminating
much computational complexity.

\paragraph{Temporal difference-learning.}

TD-learning directly computes the value function. Recall the Bellman
expectation equation, 
\begin{align*}
  V^\pi(x) &= r(x,\pi(x)) + \gamma \sum_{x'\in\X} p(x'\mid x,\pi(x)) V^\pi(x'). \\
  \intertext{Since we do not have access to $r$ and $p$, we have to make a
  Monte Carlo estimate given a single data point $\langle x,a,r,x' \rangle$,}
  &\approx r + \gamma V^\pi(x')
.\end{align*}
The idea is that we make this approximation repeatedly as the agent collects
new data, which achieves the same effect as averaging over many data points.
However, there is still a problem: $V^\pi$ depends on the unknown $V^\pi$.

The key idea is to use a bootstrapping estimate of the value function. In other
words, instead of the true value function $V^\pi$, we will use a running
estimate $\hat{V}^\pi$. However, due to relying on a single sample, the value
function will have a high variance, which is why we mix the new estimate with
the previous one using a learning rate $\alpha_t$, \[
  \hat{V}^\pi(x) \gets (1-\alpha_t) \hat{V}^\pi(x) + \alpha_t(r + \gamma \hat{V}^\pi(x'))
.\]
If the learning rate $\alpha_t$ satisfies the Robbins Monro conditions and all
states are chosen infinitely often, $\hat{V}^\pi$ is guaranteed to converge to
the optimal value function $V^\pi$.

Note that, due to the Monte Carlo approximation \wrt transitions attained by
following policy $\pi$, TD-learning is a fundamentally on-policy method.
Further note that the space requirement of this algorithm is $\bigo{|\X|}$.

\paragraph{Q-learning.}

A generalization of TD-learning is Q-learning. Instead of directly learning the
value function, which makes it inherently on-policy, it learns state-action
values $Q(x,a)$. Then, we can compute $V^\pi(x) = \max_{a} Q(x,a)$. Like in
TD-learning, we mix in the new esimate with the previous one according to
learning rate $\alpha_t$, \[
  \hat{Q}(x,a) \gets (1-\alpha_t) \hat{Q}(x,a) + \alpha_t \lft( r + \gamma \max_{a'\in\mathcal{A}} \hat{Q}(x',a') \rgt)
.\]
The advantage of Q-learning is that it is off-policy, because the value is
conditioned on the action. Thus, we can generate as much data as we need using a
different algorithm, such as $\epsilon$-greedy, and then estimate the Q-values
from there.

Again, Q-learning is optimal if it satisfies the Robbins Monro conditions and
all state-action pairs are chosen infinitely often. The space complexity of
Q-learning is $\bigo{|\mathcal{A}| \cdot |\mathcal{X}|}$.

\subsection{Model-free deep RL}

\marginnote{In deep RL, the input could be anything that machine learning can
process, \eg, video game frames with CNN or language with RNN.}

The problem with all previously discussed methods is that they are only feasible
in a small finite domain. In continuous domains, we would need an infinite
amount of memory to store all values. Thus, we need to approximate this
regression problem with function approximators, a.k.a. machine learning.

Let $V^\pi(x;\vec{\theta})$ be the function approximator that approximates the
value function. Just like in TD-learning, we make the following Monte Carlo
estimation for a given data point $\langle x, a, r, x' \rangle$, \[
  V^\pi(x) \approx r + \gamma V^\pi(x')
.\]
Then, we can define the loss function of our value function as the squared error
from the true value function,
\begin{align*}
  \ell(\vec{\theta} ; x, r, x') &\doteq \frac{1}{2} \lft( V^\pi(x;\vec{\theta}) - V^\pi(x) \rgt), \\
  \intertext{which we estimate by using the Monte Carlo estimation,}
  &\approx \frac{1}{2} \lft( V^\pi(x;\vec{\theta}) - \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt) \rgt)^2
.\end{align*}

The gradient of this loss is equal to the following, \[
  \grad{\ell(\vec{\theta} ; x,r,x')}{V^\pi(x;\vec{\theta})} = V^\pi(x;\vec{\theta}) - \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt)
.\]
Using stochastic gradient descent, we then get the following update rule,
\begin{align*}
  V^\pi(x;\vec{\theta}) &\gets V^\pi(x;\vec{\theta}) - \alpha_t \lft(V^\pi(x;\vec{\theta}) - \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt)\rgt) \\
  &= (1-\alpha_t) V^\pi(x;\vec{\theta}) + \alpha_t \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt)
,\end{align*}
which is the same as the TD-learning update rule. Thus, the TD-learning update
rule can be viewed as gradient descent on the squared loss!

\paragraph{Deep Q-network.}

The same result
holds for the Q-learning update rule, where we do gradient descent on \[
  \ell(\vec{\theta}; x,a,x',r) = \frac{1}{2} \lft( Q(x,a;\vec{\theta}) - \lft(r + \gamma \max_{a'} Q\lft(x',a; \vec{\theta}^{\mathrm{old}}\rgt) \rgt) \rgt)^2
.\]
This equation is called the \textit{Bellman error}.

\begin{algorithm}
  \begin{algorithmic}
    \Function{DeepQNetwork}{$\tau$}
      \State {initialize $\vec{\theta}$}
      \While {not converged}
        \State {pop $\langle x,a,r,x' \rangle$ from $\tau$}
        \State {$\vec{\theta} \gets \vec{\theta} - \alpha_t \delta \grad{Q(x,a;\vec{\theta})}{\vec{\theta}}$}
        \State {where $\delta \doteq \lft( Q(x,a;\vec{\theta}) - \lft(r + \gamma \max_{a'} Q\lft(x',a; \vec{\theta}^{\mathrm{old}}\rgt) \rgt) \rgt)$}
      \EndWhile
    \EndFunction
  \end{algorithmic}
  \caption{Q-learning with function approximation.}
\end{algorithm}

However, this algorithm is quite slow to converge. To accelerate, we clone the
network and maintain a constant ``target`` value across episodes, which we
update once in a while.

Furthermore, deep Q networks suffer from ``maximization bias``, which means that
it tends to overestimate the actual Q value. This is caused by the
$\max$-operator used in the update rule, which also maximizes noise in the data.
This is solved by the double deep Q network, where we use two target networks.
We then take the minimum at each iteration of the two predictions by the two
target networks.

\paragraph{Policy search methods.}

The problem with deep Q networks is that if the action-space is large or
infinite, the $\max_{a'} Q(x',a;\vec{\theta})$ is no longer feasible. The
solution to this is learning a parametrized policy $\pi(x;\vec{\theta})$, where
the output is the action. In this case, we want to maximize the expected
trajectory reward,
\begin{align*}
  J(\vec{\theta}) &= \E_{x_{0:T},a_{0:T} \sim \pi_{\vec{\theta}}} \lft[ \sum_{t=0}^T \gamma^t r(x_t,a_t) \rgt] \\
                  &= \E_{\tau \sim \pi_{\vec{\theta}}} \lft[ r(\tau) \rgt]
.\end{align*}

\begin{theorem}
  The following holds, \[
    \grad{J(\vec{\theta})}{\vec{\theta}} = \grad{\E_{\tau \sim \pi_{\vec{\theta}}} [r(\tau)]}{\vec{\theta}} = \E_{\tau\sim \pi_{\vec{\theta}}}[r(\tau)\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}]
  .\]

  \label{thm:reinforce-grad}
\end{theorem}

\begin{proof}
  \begin{align*}
    \grad{J(\vec{\theta})}{\vec{\theta}} &= \grad{\int \pi_{\vec{\theta}}(\tau) r(\tau) d\tau}{\vec{\theta}} \\
    &= \int \grad{\pi_{\vec{\theta}}(\tau) r(\tau)}{\vec{\theta}} d\tau \\
    &= \int \pi_{\vec{\theta}}(\tau) r(\tau) \grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}} d\tau & \text{\footnotesize chain rule} \\
    &= \E_{\tau\sim \pi_{\vec{\theta}}}[r(\tau)\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}]
  .\end{align*}
\end{proof}

Using \Cref{thm:reinforce-grad}, we do not need to use the reparameterization
trick to be able to compute gradients. We only need to compute $\grad{\log
\pi_{\vec{\theta}}(\tau)}{\vec{\theta}}$. We can compute this gradient as
follows,
\begin{align*}
  \pi_{\vec{\theta}}(\tau) &= p(x_0) \prod_{t=0}^T \pi_{\vec{\theta}}(a_t\mid x_t) p(x_{t+1}\mid x_t,a_t) \\
  \grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}} &= \grad{\log p(x_0)}{\vec{\theta}} + \sum_{t=0}^T \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} + \sum_{t=0}^T \grad{\log p(x_{t+1}\mid x_t,a_t)}{\vec{\theta}} \\
  &= \sum_{t=0}^T \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}}
.\end{align*}
So, to be able to compute gradients \wrt $\vec{\theta}$, we do not even need to
know the underlying MDP! Putting this together, we get the following gradient, \[
  \grad{J(\vec{\theta})}{\vec{\theta}} = \E_{\tau\sim \pi_{\vec{\theta}}} \lft[ r(\tau) \sum_{t=0}^T \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} \rgt]
.\]

Even though these gradients are unbiased, they typically have large variance. We
can reduce the variance by introducing \textit{baselines}, \[
  \E_{\tau\sim \pi_{\vec{\theta}}}[r(\tau)\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}] = \E_{\tau\sim \pi_{\vec{\theta}}}[(r(\tau) - b(\tau))\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}]
,\]
which have the same gradient. Thus, we are able to shift the reward up or down
without influencing the gradient.

\begin{algorithm}
  \begin{algorithmic}
    \Function{REINFORCE}{}
      \State {initialize $\vec{\theta}$}
      \Repeat
        \State {generate an episode $\tau$}
        \For {$t=1,\ldots,T$}
          \State {$G_t \gets r_t$}
          \State {$\vec{\theta} \gets \vec{\theta} + \eta \gamma^t G_t
          \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}}$}
        \EndFor
      \Until {done}
    \EndFunction
  \end{algorithmic}
  \caption{The REINFORCE algorithm, where the baseline at timestep $t$ is set to
  be $\sum_{t'=0}^{t-1} \gamma^{t'}r_{t'}$. \[ G_t = r(\tau) - b_t =
  \sum_{t'=t}^T \gamma^{t'-t} r_t.\] Intuitively, $G_t$ is the reward to go
  following action $a_t$.}
\end{algorithm}

\paragraph{Actor-critic methods.}

Learn both a parametrized policy and a value function estimator. They both
induce each other, so they can be part of the other's loss function.

\subsection{Model-based deep RL}

Main benefit: Learning a model can help dramatically reduce the sample
complexity compared to model-free techniques (though not necessarily
computational cost).

\end{document}
