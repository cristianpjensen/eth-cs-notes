\section{Risk minimization}

\subsection{Algorithms in data science}

In classical algorithm theory, an optimization problem solves a well-defined problem. For example,
Kruskal's algorithm computes the minimum spanning tree of a graph. In data science, it is not as
well-defined. The starting point is a learning problem, and the optimization typically happens on
training data. However, even a perfect result may fail to solve the learning problem, which is a
failure of the model in which the optimization algorithm was applied, rather than the optimization
algorithm itself.

\subsection{Empirical and expected risk}

Typically, we have a data source $\mathcal{X}$, equipped with an unknown probability distribution.
However, we do have access to independent samples $X_1,\ldots,X_n \sim \mathcal{X}$. The goal is to
``explain`` $\mathcal{X}$ through these samples. More specifically, we have a class $\mathcal{H}$
of hypotheses (possible explanations of $\mathcal{X}$). The goal is then to select the hypothesis
$H\in \mathcal{H}$ that best explains $\mathcal{X}$, which we measure by a \textit{risk} (or loss)
function $\ell: \mathcal{H}\times \mathcal{X}\to \R$.

The expected risk can be computed by \[
    \ell(H) \doteq \E_{\mathcal{X}}[\ell(H, X)].
\]
The best explanation is the \textit{expected risk minimizer}, \[
    H^\star = \argmin_{H\in \mathcal{H}} \ell(H).
\]

However, since we do not have access to the distribution over $\mathcal{X}$, we cannot compute
$\ell(H)$ or $H^\star$. Thus, we need to compute the \textit{probably approximately correct} (PAC)
hypothesis. This means that given $\delta,\epsilon > 0$, we want to produce, with probability
$1-\delta$, a hypothesis $\tilde{H}\in \mathcal{H}$ such that \[
    \ell(\tilde{H}) \leq \inf_{H\in \mathcal{H}} \ell(H) + \epsilon,
\]
meaning that with high probability, we approximately solve the expected risk minimization problem.

However, we can still not compute this, thus we must use our training data to compute the
\textit{empirical risk}, \[
    \ell_n(H) = \frac{1}{n} \sum_{i=1}^{n} \ell(H,X_i). \margintag{This is a random variable, because it depends on the training data, which are all random variables, distributed according to the probability distribution of $\mathcal{X}$.}
\]

\begin{lemma}[Weak law of large numbers]
    Let $H \in \mathcal{H}$ be a hypothesis. For any $\delta,\epsilon > 0$, there exists
    $n_0\in \mathbb{N}$ such that for $n \geq n_0$, \[
        | \ell_n(H) - \ell(H) | \leq \epsilon,
    \]
    with probability at least $1-\delta$.
\end{lemma}

Given $n\in \mathbb{N}$ and training data $X_1,\ldots,X_n$ from $\mathcal{X}$, we want to produce a
hypothesis $\tilde{H}_n$ such that \[
    \ell_n(\tilde{H}_n) \leq \inf_{H \in \mathcal{H}} \ell_n(H) + \epsilon.
\]
In an ideal world, $\tilde{H}_n$ is also almost the best explanation for the data source
$\mathcal{X}$, \[
    \ell(\tilde{H}_n) \leq \inf_{H \in \mathcal{H}} \ell(H) + \epsilon.
\]

\begin{remark}
    Note that the weak law of large numbers can only be applied to a \textit{fixed} hypothesis, but
    not to the data-dependent hypothesis $\tilde{H}_n$. Thus, we are not always in an ideal world scenario.
\end{remark}

A sufficient condition for an ideal world scenario is that the weak law of large numbers uniformly
holds for all hypotheses with high probability. This leads us to the following theorem.

\begin{theorem} \label{thm:weaker-llm}
    Assume that for any $\delta,\epsilon > 0$, there exists $n_0 \in \mathbb{N}$ such that for $n \geq
        n_0$, \[
        \sup_{H\in \mathcal{H}} |\ell_n(H) - \ell(H)| \leq \epsilon,
    \]
    with probability at least $1-\delta$. Then, for $n \geq n_0$, an approximate empirical risk
    minimizer $\tilde{H}_n$ is PAC for expected risk minimization, meaning that it satisfies \[
        \ell(\tilde{H}_n) \leq \inf_{H\in \mathcal{H}} \ell(H) + 3\epsilon,
    \]
    with probability at least $1-\delta$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \ell(\tilde{H}_n) & \leq \ell_n(\tilde{H}_n) + \epsilon \margintag{Follows from $\sup_{H\in \mathcal{H}} |\ell_n(H) - \ell(H)| \leq \epsilon$.}                 \\
                          & \leq \inf_{H \in \mathcal{H}} \ell_n(H) + 2 \epsilon \margintag{$\tilde{H}_n$ is an almost best explanation of the training data.}          \\
                          & \leq \inf_{H \in \mathcal{H}} \ell(H) + 3 \epsilon, \margintag{Follows from $\sup_{H\in \mathcal{H}} |\ell_n(H) - \ell(H)| \leq \epsilon$.}
    \end{align*}
    with probability at least $1-\delta$.
\end{proof}

It turns out that the assumption made by \Cref{thm:weaker-llm} holds in many relevant cases, but it
is not always true.

In this course, we will not learn how to pick the theory ($\mathcal{H}$ and $\ell$), but rather how
to solve the optimization problems that arise in empirical risk minimization after the theory has
been chosen.

\subsection{The map of learning}

\begin{marginfigure}
    \centering
    \incfig{map-of-learning}
    \caption{The map of learning.}
    \label{fig:map-of-learning}
\end{marginfigure}

The map of learning can be seen in \Cref{fig:map-of-learning}. It plots the empirical risk
($\ell_n(H_n)$, training data) against the expected risk ($\ell(H_n)$, estimated by a validation
set). We can only be in the area denoted by ``empirical risk minimization``, because
\begin{align*}
    \ell_n(\tilde{H}_n) & \leq \inf_{H\in \mathcal{H}} \ell_n + \epsilon \\
                        & \leq \ell_n(\tilde{H}) + \epsilon              \\
                        & \leq \ell(\tilde{H}) + 2 \epsilon              \\
                        & \leq \ell(\tilde{H}_n) + e \epsilon.
\end{align*}

A model is overfit when we have low empirical risk, while having high expected risk. This means
that the explanation quality on the data source is much worse than on the training data. The main
reason for this is that the theory ($\mathcal{H}$ and $\ell$) is so complex that it can explain
almost anything.

A model is underfit when we have high empirical risk, while having high expected risk. This means
that we neither explain the training data nor the data source. The main reason for this is that the
theory is too simple to capture the nature of the data.

The model is learning when we have low empirical risk and low expected risk. This means that the
training was successful. Generalization occurs when the expected risk is close to the empirical
risk. Note that this does not mean that the explanation is good, since any ``blind explanation``
will generalize well due to the weak law of large numbers. Ideally, we want generalization and
learning.
