\section{Active learning} \label{sec:active_learning}

Until now, we have only covered how to learn/represent aleatoric and epistemic
uncertainty in machine learning. \textit{Active learning} covers how to use
this measure for deciding which data to collect. Intuitively, we want to
collect data points in places where we would gain the most information, \ie,
where the uncertainty is high. This assumes that there is some cost associated
with collecting data points, making it important to be careful about which data
points to pick that maximize the information obtained.

Let $\mathcal{X}$ be a set of possible observations of $f$, and $y_{\vec{x}}$
the observation at $\vec{x}\in\mathcal{X}$, \[
  y_{\vec{x}} \doteq f(\vec{x}) + \epsilon_{\vec{x}}, \hspace{1cm} \vec{\epsilon}_{\vec{x}} \in \mathcal{N}(\bm{0},\sigma_n^2 \bm{I})
.\]
Then, we want to observe a subset $S\subseteq\mathcal{X}$ of a fixed size
that maximizes the information gain between the model $f$ and $\bm{y}_S$,
which yields the following maximization objective, \[
  I(S) \doteq I(\bm{f}_S;\bm{y}_S) = H[\bm{f}_S] - H(\bm{f}_S\mid\bm{y}_S)
,\]
where $H[\bm{f}_S]$ denotes the uncertainty about $\bm{f}_S$ before obtaining
the observations $\bm{y}_S$ and $H[\bm{f}_S\mid\bm{y}_S]$ corresponds to the
uncertainty about $\bm{f}_S$ after obtaining observations $\bm{y}_S$. This
problem is $\mathcal{NP}$-hard, thus we need to formulate a strategy.

\subsection{Sampling strategies}

\paragraph{Uncertainty sampling.}

The simplest strategy is to greedily pick points one by one, which entails
that we pick the locations, $\vec{x}_1,\ldots,\vec{x}_n$, individually by
greedily finding the location with maximal mutual information. That is, if we
have already picked locations $S_t = \{ \vec{x}_1,\ldots,\vec{x}_t \}$, then
the next point, $\vec{x}_{t+1}$, maximizes its mutual information, \[
  \vec{x}_{t+1} \doteq \argmax_{\vec{x}\in\mathcal{X}} I(f_{\vec{x}};y_{\vec{x}}\mid\bm{y}_{S_t})
.\]
Assuming that $f$ is modeled by a Gaussian,
\begin{align*}
  \vec{x}_{t+1} &= \argmax_{\vec{x}\in\mathcal{X}} \frac{1}{2}\log\lft( 1+\frac{\sigma_{\vec{x}\mid S_t}^2}{\sigma_n^2(\vec{x})} \rgt) \\
  &= \argmax_{\vec{x}\in\X} \frac{\sigma^2_{\vec{x}\mid S_t}}{\sigma_n^2(\vec{x})}
.\end{align*}
Assuming that the label noise is independent of $\vec{x}$, \ie, homoscedastic,
\[
  \vec{x}_{t+1} = \argmax_{\vec{x}\in\mathcal{X}} \sigma_{\vec{x}\mid S_t}^2
.\]
Thus, if $f$ is modeled by a Gaussian and we assume homoscedastic noise,
greedily maximizing mutual information corresponds to picking the point
$\vec{x}$ with the largest variance.

Due to the \textit{information never hurts} principle, mutual information is
monotone submodular, which means that adding data points can only increase
the mutual information in expectation. From this, it follows that the greedy
algorithm provides a constant-factor approximation, which means that
uncertainty sampling is near-optimal.

\paragraph{Heteroscedastic noise.}

If the data is not homoscedastic, uncertainty sampling will fail, because it
fails to distinguish between epistemic and aleatoric uncertainty. Therefore,
the most uncertain point is not necessarily the most informative one. Thus,
maximizing the mutual information yields the following, \[
  \vec{x}_{t+1} = \argmax_{\vec{x}\in\mathcal{X}} \frac{\sigma_{\vec{x}\mid S_t}^2}{\sigma_n^2(\vec{x})}
.\]
Here we make a trade-off between large epistemic uncertainty and large
aleatoric uncertainty. Ideally, we find an $\vec{x}$ where the epistemic
uncertainty is large, and the aleatoric uncertainty low, because we want to
make sure we get a lot of information (high epistemic uncertainty), but also
that we can have high confidence in the point we choose (low aleatoric
uncertainty).

\paragraph{Bayesian active learning by disagreement.}

Uncertainty sampling in classification corresponds to selecting samples that
maximize entropy of the predicted label, \ie, points that are close to the
decision boundary. However, the uncertainty in points around the decision
boundary are often due to aleatoric noise. Hence, we will not learn much from
observing points there.

Thus, we need to distinguish between aleatoric and epistemic uncertainty of
$f_{\vec{\theta}}$,
\begin{align*}
  \vec{x}_{t+1} &= \argmax_{\vec{x}\in\mathcal{X}} I(y_{\vec{x}};\vec{\theta}\mid \vec{x}_{1:t},y_{1:t}) \\
  &= \argmax_{\vec{x}\in\mathcal{X}} H[y_{\vec{x}}\mid \vec{x}_{1:t},y_{1:t}] - H[y_{\vec{x}}\mid\vec{\theta},\vec{x}_{1:t},y_{1:t}] \\
  &= \argmax_{\vec{x}\in\mathcal{X}} \underbrace{H[y_{\vec{x}}\mid \vec{x}_{1:t},y_{1:t}]}_{\substack{\text{entropy of} \\ \text{pred. posterior}}} - \underbrace{\mathbb{E}_{\vec{\theta}\mid\vec{x}_{1:t},y_{1:t}} [H[y_{\vec{x}}\mid\vec{\theta}]]}_{\text{entropy of likelihood}}
.\end{align*}
The first term measures the entropy of the average prediction, while the
second term measures the average entropy of predictions. Thus, the first term
looks for points where the average prediction is uncertain. In contrast, the
second term penalizes points where many of the sampled models $\vec{\theta}$
are uncertain about their prediction. Thus, we want to find points $\vec{x}$
where the posterior is uncertain (epistemic uncertainty) because of all
models $\vec{\theta}$ being extremely certain about their differing
predictions (aleatoric uncertainty).

Since $p(\vec{\theta}\mid\vec{x}_{1:t},y_{1:t})$ is intractable, we need to use
variational inference and Monte Carlo to approximate the second term,
\begin{align*}
  \mathbb{E}_{\vec{\theta}\mid\vec{x}_{1:t},y_{1:t}} [H[y_{\vec{x}}\mid\vec{\theta}]] &\approx \mathbb{E}_{q_{\lambda}} [H[y_{\vec{x}}\mid\vec{\theta}]] & \text{\footnotesize variational inference} \\
  &\approx \frac{1}{m} \sum_{i=1}^m H\lft[y_{\vec{x}}\mid\vec{\theta}^{(i)}\rgt], \hspace{0.5cm} \vec{\theta}^{(i)}\sim q_{\lambda} & \text{\footnotesize Monte Carlo}.
\end{align*}
We can use another approximation method, such as variational inference,
Markov chain Monte Carlo, or SWAG, to approximate the predictive posterior in
the first term.
