\section{Transliteration} \label{sec:transliteration}

Transliteration is the mapping of strings in one character set to strings in
another character set. An example of this is the phonetic translation of
English words. Formally, we want to develop a probabilistic model that can map
strings from input vocabulary $\Sigma$ to an output vocabulary $\Omega$, \ie,
we want to compute $p(y\mid x)$ for all $x\in \Sigma^*,y\in \Omega^*$. We can
use a WFST to specify the transliteration of $\Sigma^*$ to $\Omega^*$ as a
globally normalized model. The scoring function is then the semiring-sum over
all paths that aligns $x$ with $y$, \[
    \score(y,x) \doteq \sum_{\bm{\pi}\in\Pi(y)} w(\bm{\pi})
    .\]

To compute the normalizer, we need to design a WFST such that the input can
only be $x$ and that the output can be any element of $\Omega^*$. To compute
$\score(y,x)$, we need a WFST such that the input can only be $x$ and the
output only $y$. We can guarantee such behavior by defining three transducers,
\begin{itemize}
    \item $\mathcal{T}_x$ is the transducer that maps $x$ to $x$;
    \item $\mathcal{T}_{\vec{\theta}}$ is the transducer that maps any source string in $\Sigma^*$ to
          any target string $\Omega^*$ (\Cref{fig:mapping-wfst});
    \item $\mathcal{T}_y$ is the transducer that maps $y$ to $y$.
\end{itemize}

\begin{marginfigure}
    \centering
    \incfig{mapping-wfst}
    \caption{Mapping WFST between the alphabets $\{ \smallcaps{a},
            \smallcaps{b}, \smallcaps{c} \}$ and $\{ \alpha, \beta, \gamma \}$. The
        vertex label denotes the last added symbol to the output string. This WFST
        maps any string of the input alphabet to any string of the output alphabet.
        Intuitively, it has insertion, substitution, and deletion operations.}
    \label{fig:mapping-wfst}
\end{marginfigure}

We can compose $\mathcal{T}_x \circ \mathcal{T}_{\vec{\theta}}$ to get a
transducer that has as input only $x$ and output any target string in
$\Omega^*$. We can use Lehmann's algorithm to compute the normalizer
$Z_{\vec{\theta}}(x)$ using the real semiring, and the maximally scoring output
using the Viterbi semiring. We can then use the transducer composition
$\mathcal{T}_x \circ \mathcal{T}_{\vec{\theta}} \circ \mathcal{T}_y$ to compute
$\score_{\vec{\theta}}(y,x)$ by using the real semiring (or the log semiring to
keep it in log-space). Thus, we have all the components we need for training
and inference.
