\section{Log-linear modelling} \label{sec:log-linear-modelling}

Let's say we want to model the conditional probability $p(y\mid x)$. A naive
way of doing this is the following, \[
  p(y\mid x) \doteq \frac{\mathcount(x,y)}{\mathcount(x)}
.\]
There are two main problems with this interpretation of discrete conditional
probability,
\begin{itemize}
  \item Suppose $\mathcount(x,y)=0$, then the probability will be $0$, \ie,
    the model says that $y$ is impossible in context $x$;
  \item There is no way to look at finer-grained aspects of $x$, \ie, some
    values of $x$ might be related.
\end{itemize}

Thus, we need a more general framework for modelling conditional distributions.
One such general framework is to simply exponentiate some scoring function
$\score: \mathcal{X} \times \mathcal{Y} \to \R$ that we construct,\sidenote{The
exponentiation makes sure that it is non-negative.} and let the conditional
probability be proportional to it, \[
  p(y\mid x) \propto \exp \score(x, y) > 0
.\]
The linear scoring function looks like the following, \[
  \score(x,y) = \transpose{\vec{\theta}} \vec{f}(x, y)
\]
with feature weights $\vec{\theta}\in\R^K$ and $\vec{f}(x,y)\in\R^K$ as a
vector describing $y$ in context $x$. The conditional probability then looks
like the following,
\begin{align*}
  p_{\vec{\theta}}(y\mid x) &= \frac{1}{Z_{\vec{\theta}}} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y) \rgt) \\
  Z_{\vec{\theta}}(x) &= \sum_{y'\in\mathcal{Y}} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y') \rgt)
.\end{align*}
This is called log-linear modelling, because if we take the logarithm of the
conditional probability, we get a linear model, \[
  \log p_{\vec{\theta}}(y \mid x) = \transpose{\vec{\theta}} \vec{f}(x,y) - \log Z_{\vec{\theta}}(x)
.\]

The design of the \textit{feature function} $\vec{f}(x,y)$ is a big portion of
the work in log-linear modelling. It can be split into two parts: preprocessing
and extracting features. The preprocessing simply consists of steps such as tokenization,
lower-casing, stemming, stop-word removal, and reducing vocabulary. After the
preprocessing, we can obtain features. Examples include one-hot encoding,
bag-of-words, $n$-grams, and word embeddings.

\textit{Maximum likelihood estimation} (MLE) is a way of finding the parameters
$\vec{\theta}\in\Theta$ that minimize the \textit{negative log-likelihood} of
the training data, \ie, we want to minimize the following, \[
  \hat{\vec{\theta}} = \argmin_{\vec{\theta}\in\Theta} \sum_{(x,y)\in\mathcal{D}} -\log p_{\vec{\theta}}(y\mid x)
,\]
where $\Theta$ is a compact (bounded and closed) subset of
$\mathbb{R}^K$.\sidenote{The reason for not taking
$\bm{\theta}\in\mathbb{R}^K$ is that the weights will likely go to infinity.}
In log-linear modelling, this objective function is convex, thus any local
minimum is a global minimum. We usually optimize the log-likelihood with
gradient-based methods,
\begin{align*}
  \grad{- \log p_{\vec{\theta}}(y \mid x)}{\vec{\theta}} &= \grad{\log Z_{\vec{\theta}}(x) - \transpose{\vec{\theta}} \vec{f}(x,y)}{\vec{\theta}} \\ 
  &= \grad{\log Z_{\vec{\theta}}(x)}{\vec{\theta}} - \grad{\transpose{\vec{\theta}} \vec{f}(x,y)}{\vec{\theta}} \\
  &= \frac{1}{Z_{\vec{\theta}}(x)} \sum_{y'\in\mathcal{Y}} \grad{\exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y') \rgt)}{\vec{\theta}} - \vec{f}(x,y) \\
  &= \sum_{y'\in\mathcal{Y}} \frac{1}{Z_{\vec{\theta}}(x)} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y') \rgt) \grad{\transpose{\vec{\theta}} \vec{f}(x,y')}{\vec{\theta}} - \vec{f}(x,y) \\
  &= \sum_{y'\in\mathcal{Y}} p_{\vec{\theta}}(y'\mid x) \vec{f}(x,y') - \vec{f}(x,y) \\
  &= \E_{y'} [\vec{f}(x,y')] - \vec{f}(x,y)
.\end{align*}
Due to convexity, the global minimum is the only point that has its gradient
equal $0$. Thus, at the optimal parameters, the following is the case, \[
  \vec{f}(x,y) = \E_{y'} [\vec{f}(x,y')]
.\]
Therefore, the optimum is where the observed feature counts $\vec{f}(x,y)$ look
like the expected feature counts $\E_{y'} [\vec{f}(x,y')]$ through the lens of
the model. In other words, the training data looks exactly like what our model
predicts through the eyes of our feature function. This is referred to as
\textit{expectation matching}.

Furthemore, we can derive the Hessian of the negative log-likelihood to be the
covariance matrix of $\vec{f}(x,\cdot)$ \wrt $\vec{\theta}$,

\begin{align*}
  \grad{p_{\vec{\theta}}(y\mid x)}{\transpose{\vec{\theta}}} &= \grad{Z_{\vec{\theta}}(x)^{-1} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y) \rgt)}{\transpose{\vec{\theta}}} \\
  &= Z_{\vec{\theta}}(x)^{-1} \grad{\exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y) \rgt)}{\transpose{\vec{\theta}}} + \lft( \grad{Z_{\vec{\theta}}(x)^{-1}}{\transpose{\vec{\theta}}} \rgt) \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y) \rgt) \\
  &= \frac{1}{Z_{\vec{\theta}}(x)} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y) \rgt) \grad{\transpose{\vec{\theta}} \vec{f}(x,y)}{\transpose{\vec{\theta}}} - \frac{1}{Z_{\vec{\theta}}(x)^2} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y) \rgt) \grad{Z_{\vec{\theta}}(x)}{\transpose{\vec{\theta}}} \\
  &= p_{\vec{\theta}}(y\mid x) \transpose{\vec{f}(x,y)} - p_{\vec{\theta}}(y\mid x) \frac{1}{Z_{\vec{\theta}}(x)} \sum_{y'\in\mathcal{Y}} \grad{\exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y') \rgt)}{\transpose{\vec{\theta}}} \\
  &= p_{\vec{\theta}}(y\mid x) \transpose{\vec{f}(x,y)} - p_{\vec{\theta}}(y\mid x) \frac{1}{Z_{\vec{\theta}}(x)} \sum_{y'\in\mathcal{Y}} \exp \lft( \transpose{\vec{\theta}} \vec{f}(x,y') \rgt) \grad{\transpose{\vec{\theta}} \vec{f}(x,y')}{\transpose{\vec{\theta}}} \\
  &= p_{\vec{\theta}}(y\mid x) \transpose{\vec{f}(x,y)} - p_{\vec{\theta}}(y\mid x) \sum_{y'\in\mathcal{Y}} p_{\vec{\theta}}(y'\mid x) \transpose{\vec{f}(x,y')} \\
  &= p_{\vec{\theta}}(y\mid x) \lft( \transpose{\vec{f}(x,y)} - \E_{y'} \lft[ \transpose{\vec{f}(x,y')} \rgt] \rgt). \\
  \\
  \hess{-\log p_{\vec{\theta}}(y\mid x)}{\vec{\theta}} &= \grad{ \grad{-\log p_{\vec{\theta}}(y\mid x)}{\vec{\theta}} }{\transpose{\vec{\theta}}} \\
  &= \grad{\sum_{y'\in\mathcal{Y}} p_{\vec{\theta}}(y'\mid x) \vec{f}(x,y') - \vec{f}(x,y)}{\transpose{\vec{\theta}}} \\
  &= \sum_{y'\in\mathcal{Y}} \vec{f}(x,y') \grad{p_{\vec{\theta}}(y'\mid x)}{\transpose{\vec{\theta}}} \\
  &= \sum_{y'\in\mathcal{Y}} \vec{f}(x,y') p_{\vec{\theta}}(y'\mid x) \lft( \transpose{\vec{f}(x,y')} - \E_{y''} \lft[ \transpose{\vec{f}(x,y'')} \rgt] \rgt) \\
  &= \sum_{y'\in\mathcal{Y}} p_{\vec{\theta}}(y'\mid x) \vec{f}(x,y') \transpose{\vec{f}(x,y')}  - \E_{y''} \lft[ \transpose{\vec{f}(x,y'')} \rgt] \sum_{y'\in\mathcal{Y}} p_{\vec{\theta}}(y'\mid x) \vec{f}(x,y') \\
  &= \E_y \lft[ \vec{f}(x,y) \transpose{\vec{f}(x,y)} \rgt]  - \E_y \lft[ \transpose{\vec{f}(x,y)} \rgt] \E_y \lft[ \vec{f}(x,y) \rgt] \\
  &= \mathrm{Cov}_{y \sim p_{\vec{\theta}}(\cdot\mid x)} \lft[ \vec{f}(x,y) \rgt]
.\end{align*}

\subsection{Softmax}

\marginnote[1.5em]{The probability simplex $\Delta^{K-1}$ is a subspace
of $\R^K_{\geq 0}$ such that the sum of the components of its elements is $1$.
It is denoted as $\Delta^{K-1}$, because it has $K-1$ free parameters,
and looks like a triangle in three dimensions.}

The $\softmax: \R^K \to \Delta^{K-1}$ function is the default way of building
probabilistic models using neural networks, because it maps vectors to
categorical probability distributions. It is basically the same as log-linear
modelling. It is defined as \[
  \softmax(\vec{h})_y \doteq \frac{\exp \lft( \nicefrac{h_y}{T} \rgt)}{\sum_{y'\in\mathcal{Y}} \exp \lft( \nicefrac{h_{y'}}{T} \rgt)}
.\]
Usually, the temperature is set to $T=1$.\sidenote{As $T\rightarrow 0$,
softmax becomes the argmax function and as $T\rightarrow \infty$, softmax
becomes a uniform categorical distribution.} This is a generalization of
log-linear modelling, where, instead of $\transpose{\vec{\theta}}
\vec{f}(x,y)$, we can use any function of the input.

\subsection{The exponential family}

The \textit{exponential family} is a family of probability distributions of the
following form, \[
  p_{\vec{\theta}}(x) = \frac{1}{Z_{\vec{\theta}}} h(x) \exp \lft( \transpose{\vec{\theta}} \vec{\phi}(x) \rgt)
,\]
where $Z_{\vec{\theta}}$ is the partition function that normalizes the
probability distribution, $h(x)$ determines the support of the function,
$\vec{\theta}$ are the canonical parameters, and $\vec{\phi}(x)$ are the
sufficient statistics. Importantly, $h(x)$ may not depend on $\vec{\theta}$.
Any distribution that can be brought into this form is part of this
family.\sidenote{Notice that the form of the exponential family is a
generalization of softmax.}

The advantage of the exponential family is that they have conjugate priors,
which make intractable posteriors tractable. Intuitively, this is because the
posterior must be in the same form as the prior, thus we must be able to
summarize the data into a finite vector (sufficient statistics).
