\section{Dimensionality reduction}

The motivation behind dimensionality reduction is to find a low-dimensional representation of
high-dimensional data.\sidenote{Often, the original raw representation is high-dimensional and
    redundant, \eg, images, audio, time series.} Dimensionality reduction has two goals: (1)
compressing the data, while preserving as much relevant information as possible, and (2)
interpreting the data, which is easier in low-dimensional space.

Dimensionality reduction is often performed by an autoencoder, which typically has a bottleneck of
low dimensionality and aims to predict its input; see \Cref{fig:auto-encoder}. Let the original
data space be $d$-dimensional and the latent space be $k$-dimensional with $k \ll d$. An
autoencoder consists of an encoder $F$ and a decoder $G$, \[
    F: \R^d \to \R^k, \quad G: \R^k \to \R^d.
\]
The idea is that $\vec{x} \in \R^d$ is mapped to a latent vector $\vec{z} \in \R^k$ by the encoder,
which is mapped to a reconstruction $\hat{\vec{x}} \in \R^d$ by the decoder. The idea is that the
encoder must compress the information well for the decoder to be able to reconstruct its input. The
reconstruction function is then the following function, \[
    G \circ F: \R^d \to \R^d,
\]
which aims to resemble the identity function $(G \circ F)(\vec{x}) = \vec{x}$. Generally, this is
only possible if the data is intrinsically $k$-dimensional.

\subsection{Linear autoencoders}

\begin{marginfigure}[-8cm]
    \centering
    \incfig{auto-encoder}
    \caption{Diagram of a single layer linear autoencoder.}
    \label{fig:auto-encoder}
\end{marginfigure}

In order to build a nice theory, we will only consider a single layer linear
autoencoder.\sidenote{Considering non-linear parametrizations will result in a much more powerful
    autoencoder.} As a result, we have the following functions,
\begin{align*}
    F(\vec{x})           & = \mat{W}\vec{x}, \quad \mat{W} \in \R^{k\times d}  \\
    G(\vec{z})           & = \mat{V} \vec{z}, \quad \mat{V} \in \R^{d\times k} \\
    (G \circ F)(\vec{x}) & = \mat{V} \mat{W} \vec{x}.
\end{align*}
The objective to minimize is the following, \[
    \mathcal{R}(\mat{W},\mat{V}) = \mathcal{R}(\mat{P} = \mat{V}\mat{W}) \doteq \frac{1}{2} \E \| \vec{x}-\mat{P}\vec{x} \|^2,
\]
Intuitively, it is the difference between the identity function and $G \circ F$.

\begin{corollary}
    For centered data (\ie $E[\vec{x}] = 0$), optimal affine maps degenerate to linear ones.
\end{corollary}

\begin{proof}
    Let $F(\vec{x}) = \mat{W} \vec{x} + \vec{a}$ and $G(\vec{z}) = \mat{V} \vec{z} + \vec{b}$, then an affine model's function is computed by \[
        (G \circ F)(\vec{x}) = \mat{V}\mat{W}\vec{x} + \vec{c}, \quad \vec{c} = \mat{V} \vec{a} + \vec{b}.
    \]
    Proof by contradiction. Assume there exists a $\vec{c} \neq 0$ such that $\E \| \vec{x} -
        (\mat{P}\vec{x} + \vec{c}) \|^2 \leq \E \| \vec{x} - \mat{P}\vec{x} \|^2$. Let $\vec{c} \neq
        \vec{0}$, then
    \begin{align*}
        \E\| \vec{x} - (\mat{P}\vec{x} + \vec{c}) \|^2 & = \E \| \vec{x} - \mat{P}\vec{x} - \vec{c} \|^2                                                                                                               \\
                                                       & = \E \lft[ \| \vec{x} - \mat{P}\vec{x} \|^2 + \| \vec{c} \|^2 - 2 \langle \vec{x} - \mat{P}\vec{x}, \vec{c} \rangle \rgt] \margintag{Cosine theorem.}         \\
                                                       & = \E \| \vec{x} - \mat{P} \vec{x} \|^2 + \| \vec{c} \|^2 - 2 \langle \E[\vec{x}] - \mat{P} \E[\vec{x}], \vec{c} \rangle \margintag{Linearity of expectation.} \\
                                                       & = \E \| \vec{x} - \mat{P} \vec{x} \|^2 + \| \vec{c} \|^2 \margintag{Centered data: $\E[\vec{x}] = \vec{0}$}                                                   \\
                                                       & > \E \| \vec{x} - \mat{P} \vec{x} \|^2. \margintag{$\vec{c} \neq \vec{0}$.}
    \end{align*}
    This contradicts the assumption.
\end{proof}

Thus, we will assume that the data is centered, which makes the analysis easier, since we do not
need to consider the affine case. This is a reasonable assumption, because data can always be
centered by subtracting $\E[\vec{x}]$ from all data points.

Note that while the optimal linear reconstruction map $\mat{P}$ is unique, its parametrization
$\mat{V} \mat{W}$ is not unique, since for any invertible matrix $\mat{A} \in \R^{k \times k}$, we
can construct an optimal parametrization, \[
    \mat{V}\mat{W} = \mat{V} \mat{I} \mat{W} = \mat{V} (\mat{A} \inv{\mat{A}}) \mat{W} = (\mat{V}\mat{A})(\inv{\mat{A}}\mat{W}),
\]
with $\inv{\mat{A}}\mat{W}, \mat{V}\mat{A}$.

\begin{important}
    The weight matrices $\mat{V}, \mat{W}$ are \textit{non-identifiable}. As a result, we must not over-interpret the found representation.
\end{important}

Since $\mat{P}$ cannot be any $d\times d$ matrix, we want to know how the composition of $\mat{V}
    \in \R^{d\times k}$ and $\mat{W} \in \R^{k\times d}$ characterizes the matrix $\mat{P}$ and which
constraints they impose. The answer to this is that the weight matrices impose a rank constraint on
$\mat{P}$, \[
    \mathrm{rank}(\mat{P}) = \min \{ \mathrm{rank}(\mat{V}), \mathrm{rank}(\mat{W}) \} \leq \min \{ k, d \} = k.
\]

\begin{important}
    $\mat{P}$ is constrained to be, at most, a rank-$k$ matrix.
\end{important}

\subsection{Projection}

The rank constraint and linearity of $\mat{P}$ means that the image (column space) of $\mat{P}$ is
a linear subspace $\mathcal{U} \subseteq \R^d$ of dimension at most $k$. We will break the solution
to our problem into two parts: (1) finding the optimal subspace $\mathcal{U}$, and (2) finding the
optimal mapping to that subspace.\sidenote{We do not search for the weight matrices
    $\mat{W},\mat{V}$, since they are not unique, but $\mat{P}$ is unique.}

\paragraph{Finding the optimal mapping to a subspace.}

We will first focus on (2); given linear subspace $\mathcal{U}$, we need to determine the optimal
linear map $\mat{P}^\star$, such that \[
    \mat{P}^\star = \argmin_{\mathrm{col}(\mat{P}) = \mathcal{U}} \| \vec{x} - \mat{P}\vec{x} \|^2, \quad \forall \vec{x}.
\]

\begin{definition}[Orthogonal projection]
    \label{def:orthogonal-projection}

    A linear transformation $P: \mathcal{V} \to \mathcal{V}$ is called an orthogonal projection onto
    $\mathcal{U}$ if $\forall \vec{x}\in \mathcal{V}:$
    \begin{enumerate}
        \item \textit{Projection}: $P(\vec{x}) \in \mathcal{U}$;
        \item \textit{Idempotency}: $P(P(\vec{x})) = P(\vec{x})$.
        \item \textit{Orthogonality}: $\mathrm{null}(P) \bot \mathrm{col}(P)$, which is equivalent to
              the following holding, $\ang{P(\vec{x}), \vec{y}} = \ang{\vec{x},P(\vec{y})}$ (self-adjointness);
    \end{enumerate}
\end{definition}

\begin{marginfigure}
    \centering
    \incfig{orthogonal-projection}
    \caption{Orthogonal projection of $\vec{x}$ onto subspace plane $\mathcal{U}$.}
    \label{fig:orthogonal-projection}
\end{marginfigure}

\begin{lemma}
    $\Pi_{\mathcal{U}}$ defined as \[
        \Pi_{\mathcal{U}}(\vec{x}) = \argmin_{\vec{x}'\in \mathcal{U}} \|\vec{x}-\vec{x}'\|
    \]
    is an orthogonal projection of $\R^n$ onto $\mathcal{U}$.
\end{lemma}

\begin{proof}
    We need to show that the definition of $\Pi_{\mathcal{U}}$ indeed is an orthogonal projection
    by showing that it adheres to the properties of \Cref{def:orthogonal-projection} (linearity,
    projection, orthogonality, and idempotency).
    \begin{enumerate}
        \item \textit{Linearity}: A function $f$ is linear if it satisfies homogeneity and
              additivity, \[
                  f(\alpha \vec{x}) = \alpha f(\vec{x}), \quad f(\vec{x} + \vec{y}) = f(\vec{x}) + f(\vec{y}).
              \]
              This can easily be shown for $\Pi_{\mathcal{U}}$,
              \begin{align*}
                  \Pi_{\mathcal{U}}(\alpha \vec{x})    & = \argmin_{\vec{x}'' \in \mathcal{U}} \| \alpha \vec{x} - \vec{x}'' \|                                                                                                                                                                                                                                                      \\
                                                       & = \argmin_{\alpha \vec{x}' \in \mathcal{U}} \| \alpha \vec{x} - \alpha \vec{x}' \|                                                                                                                                                                                                                                          \\
                                                       & = \alpha \argmin_{\vec{x}' \in \mathcal{U}} |\alpha| \|\vec{x} - \vec{x}'\| \margintag{$\argmin_{\lambda \vec{x}} f(\vec{x}) = \lambda \argmin_{\vec{x}} f(\vec{x})$.}                                                                                                                                                      \\
                                                       & = \alpha \argmin_{\vec{x}' \in \mathcal{U}} \|\vec{x} - \vec{x}'\|                                                                                                                                                                                                                                                          \\
                                                       & = \alpha \Pi_{\mathcal{U}}(\vec{x}).                                                                                                                                                                                                                                                                                        \\
                  \Pi_{\mathcal{U}}(\vec{x} + \vec{y}) & = \argmin_{\vec{z} \in \mathcal{U}} \| \vec{z} - (\vec{x} + \vec{y}) \|                                                                                                                                                                                                                                                     \\
                                                       & = \argmin_{\substack{\Pi_{\mathcal{U}}(\vec{x}) + \vec{y}'                                                                                                                                                                                                                                                                  \\ \vec{y}' \in \mathcal{U}}} \| \Pi_{\mathcal{U}}(\vec{x}) + \vec{y}' - \vec{x} - \vec{y} \| \margintag{$\vec{z} \doteq \Pi_{\mathcal{U}}(\vec{x}) + \vec{y}'$ for some $\vec{y}' \in \mathcal{U}$.}                                                                                                                 \\
                                                       & = \Pi_{\mathcal{U}}(\vec{x}) + \argmin_{\vec{y}' \in \mathcal{U}} \| (\vec{y}' - \vec{y}) + (\Pi_{\mathcal{U}}(\vec{x}) - \vec{x}) \|^2 \margintag{$\argmin_{\vec{y} + \vec{x}} f(\vec{x}) = \vec{y} + \argmin_{\vec{x}} f(\vec{x})$.}                                                                                      \\
                                                       & = \Pi_{\mathcal{U}}(\vec{x}) + \argmin_{\vec{y}' \in \mathcal{U}} \| \vec{y}' - \vec{y} \|^2 + \| \Pi_{\mathcal{U}}(\vec{x}) - \vec{x} \|^2                                                                                                                                                                                 \\
                                                       & \qquad \qquad \qquad + 2\ang{\Pi_{\mathcal{U}}(\vec{x}) - \vec{x}, \vec{y}' - \vec{y}} \margintag{Cosine theorem.}                                                                                                                                                                                                          \\
                                                       & = \Pi_{\mathcal{U}}(\vec{x}) + \argmin_{\vec{y}' \in \mathcal{U}} \| \vec{y}' - \vec{y} \|^2 + 2 \langle \Pi_{\mathcal{U}}(\vec{x}) - \vec{x}, \vec{y}' \rangle \margintag{$\| \Pi_{\mathcal{U}}(\vec{x}) - \vec{x} \|^2$ and $\langle \Pi_{\mathcal{U}}(\vec{x}) - \vec{x}, \vec{y} \rangle$ do not depend on $\vec{y}'$.} \\
                                                       & = \Pi_{\mathcal{U}}(\vec{x}) + \Pi_{\mathcal{U}}(\vec{y}); \margintag{$\Pi_{\mathcal{U}}(\vec{x}) - \vec{x} \in \mathcal{U}^\bot$ and $\vec{y}' \in \mathcal{U}$, so their inner product is $0$.}
              \end{align*}

        \item \textit{Projection}: This is true by definition of the values that the $\argmin$ are allowed to take on;

        \item \textit{Idempotency}: For all $\vec{u}\in \mathcal{U}$, $\Pi_{\mathcal{U}}(\vec{u})
                  = \argmin_{\vec{x}'\in \mathcal{U}} \| \vec{u} - \vec{x}' \| = \vec{u}$. Thus,
              $\Pi_{\mathcal{U}} = \Pi_{\mathcal{U}} \circ \Pi_{\mathcal{U}}$;

        \item \textit{Orthogonality}: We need to show that $\Pi_{\mathcal{U}}(\vec{x}) - \vec{x} \in
                  \mathcal{U}^\bot$. Firstly, we have
              \begin{equation}
                  \label{eq:orth-proof}
                  \| \Pi_{\mathcal{U}}(\vec{x}) - \vec{x} \| = \min_{\vec{u} \in \mathcal{U}} \| \vec{u} - \vec{x} \| \leq \| \tilde{\vec{u}} - \vec{x} \|, \quad \forall \tilde{\vec{u}} \in \mathcal{U}.
              \end{equation}
              Decompose $\Pi_{\mathcal{U}}(\vec{x}) - \vec{x} = \vec{u} + \vec{u}^\bot$, where $\vec{u} \in
                  \mathcal{U}$ and $\vec{u}^\bot \in \mathcal{U}^\bot$. Then, we only need to show $\vec{u} =
                  \vec{0}$, which we will prove by contradiction. Let $\vec{u} \neq \vec{0}$, then
              \begin{align*}
                  \| \Pi_{\mathcal{U}}(\vec{x}) - \vec{x} \|^2 & = \| \vec{u} \|^2 + \| \vec{u}^\bot \|^2 + 2 \langle \vec{u}, \vec{u}^\bot \rangle \margintag{Cosine theorem.} \\
                                                               & > \| \vec{u}^\bot \|^2 \margintag{$\vec{u} \neq \vec{0}$ and $\langle \vec{u}, \vec{u}^\bot \rangle = 0$.}     \\
                                                               & = \| \Pi_{\mathcal{U}}(\vec{x}) - \vec{u} - \vec{x} \|^2.
              \end{align*}
              This contradicts with \Cref{eq:orth-proof}, because $\Pi_{\mathcal{U}}(\vec{x}) - \vec{u} \in \mathcal{U}$.
    \end{enumerate}
\end{proof}

So, we know that $\Pi_{\mathcal{U}}$ is a linear function. Now, we want to find the matrix
$\mat{P}$ representing that linear transformation.

\begin{lemma}
    \label{lem:orthonormal-basis-projection}
    Given an orthonormal basis $\mat{U}$ of $\mathcal{U}$ ($\vec{u}_i \in \mathcal{U}, \| \vec{u}_i \| = 1, \langle \vec{u}_i,
        \vec{u}_j \rangle = 0, \forall i \neq j$), we can compute the optimal projection matrix, \[
        \mat{P} = \mat{U}\transpose{\mat{U}}.
    \]
    Note that in this case $\mat{W}$ and $\mat{V}$ share parameters, and $\mat{U}\transpose{\mat{U}}$
    is the optimal weight matrix if we enforce parameter sharing via $\mat{V} = \transpose{\mat{W}}$.
\end{lemma}

\begin{proof}
    We need to show that $\mat{P}$ is the orthogonal projection matrix onto $\mathcal{U}$:
    \begin{enumerate}
        \item \textit{Projection}: $\mat{P} \vec{x} = \sum_{i=1}^{m} \vec{u}_i \transpose{\vec{u}_i} \vec{x} = \sum_{i=1}^{m} \ang{\vec{u}_i, \vec{x}} \vec{u}_i \in \mathcal{U}$;
        \item \textit{Self-adjointness}: $\transpose{\mat{P}} = \transpose{(\mat{U}\transpose{\mat{U}})} = \mat{U} \transpose{\mat{U}} = \mat{P}$;
        \item \textit{Idempotency}: $\mat{P} \mat{P} = \mat{U} \transpose{\mat{U}} \mat{U} \transpose{\mat{U}} = \mat{U} \transpose{\mat{U}} = \mat{P}$;
        \item \textit{Orthogonality}: $\langle \mat{P} \vec{x}, \vec{x} - \mat{P} \vec{x} \rangle = \langle \vec{x}, \mat{P} (\vec{x} - \mat{P} \vec{x}) \rangle = \langle \vec{x}, (\mat{P} - \mat{P}^2) \vec{x} \rangle = \langle \vec{x}, (\mat{P} - \mat{P}) \vec{x} \rangle = 0$.
    \end{enumerate}
\end{proof}

The problem is that we do not have an orthonormal basis of $\mathcal{U}$ in general. The next lemma
gives a more general result for any basis.

\begin{lemma}
    For a non-orthonormal basis $\mat{V} \in \R^{d \times k}$ of $\mathcal{U}$, we can recover the projection matrix, \[
        \mat{P} = \mat{V}\mat{V}^+, \quad \mat{V}^+ \doteq \inv{\lft( \transpose{\mat{V}} \mat{V} \rgt)} \transpose{\mat{V}}. \margintag{$\mat{V}^+$ is the left Moore-Penrose pseudo-inverse of $\mat{V}$.}
    \]
\end{lemma}

\begin{proof}
    We need to show that $\mat{P}$ is the orthogonal projection matrix of $\mathcal{U}$:
    \begin{enumerate}
        \item \textit{Projection}: $\mat{P} \mat{V} = \mat{V} \mat{V}^+ \mat{V} = \mat{V} \inv{\lft( \transpose{\mat{V}} \mat{V} \rgt)} \transpose{\mat{V}} \mat{V} = \mat{V}$. Together with the rank constraint, this yields $\mat{P} \vec{u}^\bot = \vec{0}$ for all $\vec{u}^\bot \in \mathcal{U}^\bot$;
        \item \textit{Self-adjointness}: $\transpose{\mat{P}} = \transpose{\lft( \mat{V} \inv{\lft( \transpose{\mat{V}} \mat{V} \rgt)} \transpose{\mat{V}} \rgt)} = \mat{V} \inv{\lft( \transpose{\mat{V}} \mat{V} \rgt)} \transpose{\mat{V}} = \mat{P}$;
        \item \textit{Idempotency}: $\mat{P} \mat{P} = \mat{V} \inv{\lft( \transpose{\mat{V}} \mat{V} \rgt)} \transpose{\mat{V}} \mat{V} \inv{\lft( \transpose{\mat{V}}\mat{V} \rgt)} \transpose{\mat{V}} = \mat{V} \mat{V}^+ = \mat{P}$.
    \end{enumerate}
\end{proof}

Note that we have not made any reference to the data $\vec{x} \sim \nu$ until now. This means that
all of the above results are a priori by the structure of the model and choice of objective.

\paragraph{Finding the optimal subspace.}

Now we need to find out which subspace of dimension $k$ or less is optimal to project onto. First,
we need to rewrite the objective function to find a new interpretation,
\begin{align*}
    \mathcal{R}(\mat{P}) & = \frac{1}{2} \E \| \vec{x}-\mat{P}\vec{x} \|^2                                                                                                                                       \\
                         & = \frac{1}{2} \E \lft[ \| \vec{x} \|^2 + \| \mat{P} \vec{x} \|^2 - 2 \langle \vec{x}, \mat{P} \vec{x} \rangle \rgt] \margintag{Cosine theorem.}                                       \\
                         & = \frac{1}{2} \E \|\vec{x}\|^2 + \frac{1}{2} \E \| \mat{P}\vec{x} \|^2 - \E \ang{\vec{x},\mat{P}^2\vec{x}} \margintag{Linearity of expectation and idempotency.}                      \\
                         & = \frac{1}{2} \E \|\vec{x}\|^2 + \frac{1}{2} \E \| \mat{P}\vec{x} \|^2 - \E \|\mat{P}\vec{x}\|^2 \margintag{$\ang{\vec{x},\mat{P}^2 \vec{x}} = \ang{\mat{P}\vec{x},\mat{P}\vec{x}}$.} \\
                         & = \frac{1}{2} \E \|\vec{x}\|^2 - \frac{1}{2} \E \| \mat{P}\vec{x} \|^2.
\end{align*}
Because our data is centered, we know the following,
\begin{align*}
    \mathrm{Var}[\vec{x}]        & = \E \|\vec{x}\|^2 - \| \E[\vec{x}] \|^2 = \E \|\vec{x}\|^2                                                                               \\
    \mathrm{Var}[\mat{P}\vec{x}] & = \E \|\mat{P}\vec{x}\|^2 - \| \E[\mat{P}\vec{x}] \|^2 = \E \|\mat{P}\vec{x}\|^2 - \| \mat{P} \E[\vec{x}] \|^2 = \E \|\mat{P}\vec{x}\|^2.
\end{align*}

\begin{marginfigure}[3cm]
    \centering
    \incfig{variance-preservation}
    \caption{Intuitively, the projection onto the shown line preserves the most information. This projection from $\R^2$ onto $\R$ maximizes variance.}
    \label{fig:variance-preservation}
\end{marginfigure}

\begin{important}
    Assuming centered data, \[
        \mathcal{R}(\mat{P}) = \frac{1}{2} (\mathrm{Var}[\vec{x}] - \mathrm{Var}[\mat{P}\vec{x}]) \propto -\mathrm{Var}[\mat{P}\vec{x}].
    \]
    Hence, minimizing $\mathcal{R}(\mat{P})$ is equivalent to maximizing the total variance of the
    projected data, $\mathrm{Var}[\mat{P}\vec{x}]$.
\end{important}

We can further simplify this expression to find a sufficient statistic for the objective function,
\begin{align*}
    -\frac{1}{2} \mathrm{Var}[\mat{P}\vec{x}] & = -\frac{1}{2} \E \|\mat{P}\vec{x}\|^2                                                                                                                                                      \\
                                              & = -\frac{1}{2} \E \ang{\vec{x},\mat{P}\vec{x}} \margintag{$\| \mat{P}\vec{x} \|^2 = \ang{\mat{P}\vec{x},\mat{P}\vec{x}} = \ang{\vec{x},\mat{P}^2 \vec{x}} = \ang{\vec{x},\mat{P}\vec{x}}$.} \\
                                              & = -\frac{1}{2} \E\lft[\tr{\transpose{\vec{x}}\mat{P}\vec{x}}\rgt]                                                                                                                           \\
                                              & = -\frac{1}{2} \tr{\E\lft[\mat{P}\vec{x}\transpose{\vec{x}}\rgt]} \margintag{Cyclic property of trace.}                                                                                     \\
                                              & = -\frac{1}{2} \tr{\mat{P} \E\lft[\vec{x}\transpose{\vec{x}}\rgt]}.
\end{align*}

\begin{important}
    $\E[\vec{x}\transpose{\vec{x}}]$ is a sufficient statistic for $\mathcal{R}(\mat{P})$. Hence,
    the optimal projection is fully determined by the covariance matrix
    $\E\lft[\vec{x}\transpose{\vec{x}}\rgt]$, together with $\E[\vec{x}]$ for centering.
\end{important}

\subsection{Principal component analysis}

Intuitively, eigenvectors represent axes along which matrices have the largest variance. Thus, we
want to project onto the $k$ eigenvectors of $\E[\vec{x}\transpose{\vec{x}}]$ that are associated
with the $k$ largest eigenvalues.

\begin{theorem}[Spectral theorem]
    Any symmetric and positive semidefinite matrix $\mat{\Sigma}$ can be non-negatively diagonalized with an orthogonal matrix, \[
        \mat{\Sigma} = \mat{Q} \mat{\Lambda} \transpose{\mat{Q}}, \quad \mat{\Lambda} = \diag{\lambda_1, \ldots, \lambda_d},
    \]
    where $\lambda_1 \geq \cdots \geq \lambda_d \geq 0$ and $\mat{Q} \in \R^d$ is orthogonal.
\end{theorem}

The spectral theorem gives us the eigendecomposition of $\E[\vec{x}\transpose{\vec{x}}]$, because
it is symmetric. The eigenvectors form an orthonormal basis, so the $k$ principal eigenvectors are
also orthonormal. Hence, we can use \Cref{lem:orthonormal-basis-projection} to form the projection
matrix, which is essentially what the PCA theorem tells us.

\begin{theorem}[PCA theorem]
    The variance maximizing projection matrix $\mat{P}$ for a covariance matrix $\E[\vec{x}\transpose{\vec{x}}] = \mat{Q} \mat{\Lambda} \transpose{\mat{Q}}$ as in the spectral theorem is given by \[
        \mat{P} = \mat{U}_k \transpose{\mat{U}_k}, \quad \mat{U}_k = \mat{Q} \begin{bmatrix} \mat{I}_k \\ \vec{0} \end{bmatrix},
    \]
    which means that $\mat{U}_k$ consists of the $k$ principal eigenvectors with padded zeroes at the
    bottom.
\end{theorem}

\begin{proof}
    \begin{align*}
        \mathrm{Var}[\mat{P}\vec{x}] & = \tr{\mat{P} \E[\vec{x}\transpose{\vec{x}}]}                                                                                                                                                                \\
                                     & = \tr{\mat{U}\transpose{\mat{U}} \mat{Q} \mat{\Lambda} \transpose{\mat{Q}}} \margintag{$\mat{P} = \mat{U} \transpose{\mat{U}}$, $\E[\vec{x}\transpose{\vec{x}}] = \mat{Q}\mat{\Lambda}\transpose{\mat{Q}}$.} \\
                                     & = \tr{\lft( \transpose{\mat{Q}} \mat{U} \rgt) \transpose{\lft( \transpose{\mat{Q}} \mat{U} \rgt)} \mat{\Lambda}}. \margintag{Cyclic property.}
    \end{align*}
    This term is maximized by $\transpose{\mat{Q}} \mat{U} = \transpose{\begin{bmatrix} \mat{I}_k & \vec{0} \end{bmatrix}}$.
\end{proof}

\begin{important}
    In conclusion, given a dataset of $n$ points $\{ \vec{x}_1, \ldots, \vec{x}_n \}$, we perform
    dimensionality reduction by first centering the data,
    \begin{align*}
        \vec{\mu}       & = \frac{1}{n} \sum_{i=1}^{n} \vec{x}_i \\
        \bar{\vec{x}}_i & = \vec{x} - \vec{\mu}.
    \end{align*}
    Then, we compute the covariance matrix $\mat{\Sigma} \in \R^{d \times d}$, \[
        \mat{\Sigma} = \frac{1}{n} \sum_{i=1}^{n} \bar{\vec{x}}_i \transpose{\bar{\vec{x}}_i}.
    \]
    For that matrix, we compute the eigendecomposition (which exists because $\mat{\Sigma}$ is
    symmetric), \[
        \mat{\Sigma} = \mat{Q} \mat{\Lambda} \transpose{\mat{Q}},
    \]
    where $\mat{Q} \in \R^{d \times d}$ is orthogonal and $\mat{\Lambda}$ is diagonal. We then discard
    the $d-k$ last dimensions to obtain $\mat{U}_k \in \R^{d \times k}$, \[
        \mat{U}_k = \mat{Q} \begin{bmatrix} \mat{I}_k \\ \mat{0} \end{bmatrix}.
    \]
    The latent vectors are then computed by \[
        \vec{z}_i = \transpose{\mat{U}_k} \bar{\vec{x}}_i.
    \]
    Their reconstructions are computed by \[
        \hat{\vec{x}}_i = \mat{U}_k \vec{z}_i.
    \]
    The squared reconstruction error is equal to the sum of the lower $d-k$ eigenvalues.
\end{important}

\subsection{Learning algorithms}

Eigenvalue decomposition of the (symmetric) sample covariance matrix has $\bigo{d^3}$ complexity.
Furthermore, the complexity of computing $\E[\vec{x}\transpose{\vec{x}}]$ is $\bigo{nd^2}$, where
$n$ is the number of data points.\sidenote{Typically, $n \gg d$.} This is quite costly, thus we
need to search for algorithms that have lower runtime complexity.

\paragraph{Power method.}

The power method is a recursive algorithm for computing principal eigenvectors. It initializes a
vector at random $\vec{v}^{(0)} \sim \mathcal{N}(\vec{0}, \mat{I})$. Then, it iteratively improves
this guess, \[
    \vec{v}^{(t+1)} = \frac{\mat{A}\vec{v}^{(t)}}{\| \mat{A} \vec{v}^{(t)} \|}.
\]
The computational complexity of this algorithm is $\bigo{Td^2}$.

\begin{lemma}
    Let $\vec{u}_1$ be the unique principal eigenvector of a diagonalizable matrix $\mat{A}$ with eigenvalues $\lambda_1 > \lambda_2 \geq \cdots \geq \lambda_n \geq 0$. If $\ang{\vec{v}^{(0)}, \vec{u}_1} \neq 0$, then \[
        \lim_{t\to\infty} \vec{v}^{(t)} = \vec{u}_1.
    \]
\end{lemma}

\begin{proof}
    We can decompose vectors as a linear combination of eigenvectors, $\vec{v}^{(0)} = \sum_{i=1}^n
        \alpha_i \vec{u}_i$. Then,
    \begin{align*}
        \vec{v}^{(k)} & \propto \mat{A}^k \vec{v}^{(0)}                                                                                                                                                      \\
                      & = \sum_{i=1}^{d} \alpha_i \lambda_i^k \vec{u}_i \margintag{$\mat{A}^k \vec{v}^{(0)} = \sum_{i=1}^{d} \alpha_i \mat{A}^k \vec{u}_i = \sum_{i=1}^{d} \alpha_i \lambda_i^k \vec{u}_i$.} \\
                      & \propto \alpha_1 \vec{u}_1 + \sum_{i=2}^{d} \alpha_i \lft( \frac{\lambda_i}{\lambda_1} \rgt)^k \vec{u}_i. \margintag{Divide by $\lambda_1^k$.}
    \end{align*}
    $\nicefrac{\lambda_i}{\lambda_1} < 1$ for $i > 1$, thus the sum goes to 0 and $\vec{v}^{(k)} \to \vec{u}_1$.
\end{proof}

We can use this algorithm to also compute the next principal eigenvectors by factoring out
$\vec{u}_1$ by $\mat{A}_2 = \mat{A} - \lambda_1 \vec{u}_1 \transpose{\vec{u}}_1$ and then applying
the algorithm again to $\mat{A}_2$ to recover $\vec{u}_2$, and continue doing that until we have
the $k$ principal eigenvectors.

Thus, the total complexity of finding the $k$ principal eigenvectors is $\bigo{Tkd^2}$ instead of
$\bigo{d^3}$. However, this does not get rid of the $\bigo{n d^2}$ complexity for computing the
sample covariance matrix, which is a bigger problem.

\paragraph{Gradient descent.}

By treating the autoencoder as a neural network, we can use deep learning techniques, such as
gradient descent. Gradient descent iteratively updates the weights by \[
    \mat{P}^{(t+1)} = \mat{P}^{(t)} - \eta \grad{\mathcal{R}(\mat{P})}{\mat{P}}.
\]
The gradient is computed by $(\mat{P} - \mat{I})\vec{x}\transpose{\vec{x}}$. The problem with this
is that we cannot constrain $\mat{P}$ to be a projection. Thus, we actually need to update
$\mat{V}$ and $\mat{W}$. Thus, by the chain rule for matrix derivatives,
\begin{align*}
    \grad{\mathcal{R}(\mat{W}, \mat{V})}{\mat{V}} & = (\mat{V} \mat{W}  - \mat{I}) \vec{x} \transpose{\vec{x}} \transpose{\mat{W}} \\
    \grad{\mathcal{R}(\mat{W}, \mat{V})}{\mat{W}} & = \transpose{\mat{V}} (\mat{V} \mat{W} - \mat{I}) \vec{x} \transpose{\vec{x}}.
\end{align*}
The complexity for $T$ iterations is then $\bigo{T(k+B)d^2}$, where $B$ is the batch size.
