\section{Probability review} \label{sec:prob-review}

Probability is formalized by a probability space $(\Omega, \mathcal{F}, P)$,
where $\Omega$ is a set of atomic events, $\mathcal{F} \subseteq 2^\Omega$ is
the set of non-atomic events, and $P: \mathcal{F}\to [0,1]$ is the
probability measure that assigns probabilities to events.

The following axioms hold:
\begin{align*}
  P(\Omega) &= 1 & \text{(Normalization)} \\
  P(A) &\geq 0 \;\forall A\in\mathcal{F} & \text{(Non-negativity)} \\
  A_1,\ldots,A_n\in\mathcal{F} \land \bigcup_{i=1}^n A_i = \emptyset &\implies P\left(\bigcup_{i=1}^n A_i\right) = \sum_{i=1}^n P(A_i) & \text{($\sigma$-additivity)}
.\end{align*}

\subsection{Random variables}

Events are cumbersome to work with, so we can define random variables
$X:\Omega\to D$ for some set $D$. Then, we can give a probability to $X$
assuming state $x$, \[
  P(X=x) = P(\{ \omega\in\Omega : X(\omega) = x \})
.\]

Instead of random variables $X$, we can also define random vectors
$\vec{X}=[X_1(\omega), \ldots, X_n(\omega)]$.  Then, we can specify the joint
distribution $P(X_1=x_1,\ldots,X_n=x_n)=P(\vec{X}=\vec{x})$ succinctly.

For random variables, we have the following rules,
\begin{itemize}
  \item \textit{Product rule}, \[
      P(X_{1:n}) = P(X_1)\prod_{i=2}^nP(X_i\mid X_{1:i-1})
    ;\]
  \item \textit{Sum rule}, \[
      P(X_{1:i-1}, X_{i+1:n}) = \sum_{x_i} P(X_{1:i-1}, X_i=x_i, X_{i+1:n})
    ;\]
  \item \textit{Bayes rule}, where we compute the \textit{posterior} $P(X\mid
    Y)$ from the \textit{likelihood} $P(Y\mid X)$, \textit{prior} $P(X)$, and
    \textit{marginal} $P(Y)$, \[
      P(X\mid Y) = \frac{P(Y\mid X)P(X)}{P(Y)}
    ;\]
  \item A random variable $X$ is \textit{independent} from $Y$ if the following
    holds for all values, \[
      P_{X_1\cdots X_n}(x_1,\ldots,x_n) = P_{X_1}(x_1)\cdots P_{X_n}(x_n)
    ;\]
  \item Random variables $X$ and $Y$ are \textit{conditionally independent}
    given $Z$ if the following holds for all $x,y,z$, \[
      P_{XY\mid Z}(x,y\mid z) = P_{X\mid Z}(x\mid z)P_{Y\mid Z}(y\mid z)
    .\]
\end{itemize}

\subsection{Multivariate Gaussians}

Suppose we have $n$ binary variables, then we need $2^n-1$
parameters.\sidenote{$-1$, because we do not need to specify the last parameter,
since it will be whatever is remaining of the total probability.}\sidenote{In
other words, the parametrization of the distribution grows exponentially.} Also,
if we want to compute the joint distribution over all $n$
variables,\sidenote{\Ie, do inference.} we would have to sum up $2^{n-1}$ terms
according to the sum rule. In conclusion, binary random variables scale poorly.
Furthermore, we would need a lot of data to estimate the distribution.

The solution to these problems are multivariate Gaussians, \[
  \mathcal{N}(\vec{x};\vec{\mu},\mat{\Sigma}) =
\frac{1}{2\pi\sqrt{\det{\mat{\Sigma}}}}\exp\lft( -\frac{1}{2}
\transpose{(\vec{x}-\vec{\mu})}\inv{\mat{\Sigma}}(\vec{x}-\vec{\mu}) \rgt)
,\]
where $\vec{\mu}\in\R^n$ and
$\mat{\Sigma}\in\mathbb{S}_{++}^n$.\sidenote{$\mat{\Sigma}$ is an $n\times n$
positive semi-definite matrix.} Thus, the joint distribution over $n$ Gaussian
variables requires only $n^2+n$ parameters.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=\textwidth,
        view={0}{90},
        scale only axis,
    ]
      \addplot3 [surf, mesh/rows=50, shader=interp] table[col sep=comma, x=x, y=y, z=identity_sigma] {data/probability_review/gaussian_2d.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{Bivariate Gaussian distribution with $\mat{\Sigma}=\mat{I}$.}
\end{marginfigure}

\begin{marginfigure}[0.5cm]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=\textwidth,
        view={0}{90},
        scale only axis,
    ]
      \addplot3 [surf, mesh/rows=50, shader=interp] table[col sep=comma, x=x, y=y,
        z=uneven_sigma] {data/probability_review/gaussian_2d.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{Bivariate Gaussian distribution with \[
      \mat{\Sigma}=\begin{bmatrix}1 & \nicefrac{1}{2} \\ \nicefrac{1}{2} & 1\end{bmatrix}
    .\]
    If $x_1$ increases, the probability of a higher $x_2$ increases as well.}
\end{marginfigure}

%\begin{figure*}[t]
  %\centering
  %\hfill
  %\caption{Bivariate Gaussian plots. As can be seen, Gaussians are independent if and only if their covariance is 0.}
  %\label{fig:gaussian}
%\end{figure*}

Let $\vec{X} \sim \mathcal{N}(\vec{\mu},\mat{\Sigma})$ be a $d$-dimensional
Gaussian random vector, then the following properties hold,
\begin{itemize}
  \item Let $A$ be an index set, then the marginal distribution of variables
    indexed by $A$ is the following, \[
      \vec{X}_A \sim \mathcal{N}(\vec{\mu}_A,\mat{\Sigma}_{AA})
    .\]
    Thus, it is simply a look-up to get a subset marginal distribution;

  \item Let $A$ and $B$ be index sets, then the marginal distribution of
    variables indexed by $A$, conditioned on $B$, is the following, \[
      \vec{X}_A\mid \vec{X}_B \sim \mathcal{N}(\vec{\mu}_{A\mid
      B},\mat{\Sigma}_{A\mid B})
    ,\]
    where
    \begin{align*}
      \vec{\mu}_{A\mid B} &= \vec{\mu}_A + \mat{\Sigma}_{AB}\inv{\mat{\Sigma}_{BB}} (\vec{x}_B - \vec{\mu}_B) \\
      \mat{\Sigma}_{A\mid B} &= \mat{\Sigma}_{AA} - \mat{\Sigma}_{AB}\inv{\mat{\Sigma}_{BB}}\mat{\Sigma}_{BA}
    .\end{align*}
    Notice that $\mat{\Sigma}_{AB}\inv{\mat{\Sigma}_{BB}}$ removes the interdependencies
    of $B$ and adds the dependencies of $B$ with $A$. Further notice that the
    dependency of $A$ and $B$ added scales linearly with the mean difference
    $\vec{x}_B - \vec{\mu}_B$.

    Further, notice that $\mat{\Sigma}_{A\mid B}$ only depends on which random
    variables are observed, not what values those random variables are, because
    it does not depend on $\vec{x}_B$;

  \item Let $\mat{M}\in\R^{m\times d}$ be a matrix, then $\vec{Y} =
    \mat{M}\vec{X}$ is a also a Gaussian, \[
      \vec{Y} \sim \mathcal{N}\lft( \mat{M}\vec{\mu}, \mat{M}\mat{\Sigma}\transpose{\mat{M}} \rgt)
    .\]
    Notice that $m$ is not necessarily equal to $d$, so we can transform a
    $d$-dimensional random vector to any dimensionality $m$;

  \item Let $\vec{X}'$ be another $d$-dimensional Gaussian, then
    $\vec{Y}=\vec{X}+\vec{X}'$ is also a Gaussian, \[
      \vec{Y} \sim \mathcal{N} \lft( \vec{\mu} + \vec{\mu}', \mat{\Sigma} + \mat{\Sigma}' \rgt)
    .\]
\end{itemize}

\subsection{Entropy}

TODO
