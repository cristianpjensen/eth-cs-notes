\section{Autoencoders}

\textit{Autoencoders} \citep{kramer1991nonlinear} are \textit{generative models}. This means that
their objective is to learn the underlying hidden structure of the data. They aim to model the
distribution $p_{\mathrm{model}}(\vec{x})$ that resembles $p_{\mathrm{data}}(\vec{x})$ to generate
new samples. Autoencoders are an \textit{explicit} generative model, which means that they explicitly
define the probability distribution $p_{\mathrm{model}}(\vec{x})$ and then sample from it to generate
new data points.

In machine learning, we often have high-dimensional data $\vec{x} \in \R^n$, such as images, audio,
or time-series. Hence, it is crucial to find a low-dimensional representation that can effectively
compress the data while preserving its essential information.

\begin{marginfigure}
    \centering
    \incfig{autoencoder}
    \caption{Autoencoder architecture.}
    \label{fig:autoencoder}
\end{marginfigure}

Autoencoders offer a solution by making use of the \textit{encoder-decoder structure}; see
\Cref{fig:autoencoder}. The \textit{encoder} $f$ projects the input space $\mathcal{X}$ into a
latent space $\mathcal{Z}$, while the \textit{decoder} $g$ maps the latent space $\mathcal{Z}$ back
to the input space $\mathcal{X}$. The assumption made by the autoencoder architecture is that if
the decoder is capable of reconstructing the original input solely from the compressed
representation, then this compressed representation must be meaningful. Consequently, the
composition $g \circ f$ aims to approximate the identity function on the data for a low
reconstruction error.

Furthermore, to enable the generation of new samples from the latent space, the latent space must
be well structured, characterized by \textit{continuity} and \textit{interpolation}. Continuity
means that the entire space must be covered by the data points, while interpolation means that if
we interpolate between two points, then the interpolation must also be a well behaved data point.

\subsection{Linear autoencoders}

If we restrict $f$ and $g$ to be linear, the encoder $f$ becomes equivalent to the projection
performed by \textit{principal component analysis}. The advantage of such a reconstruction is that
it can be found in a closed form, and it is interpretable. However, it is not very powerful.

\subsection{Non-linear autoencoders}

We can gain a lot of performance by allowing $f$ and $g$ to be non-linear. In this case, the
encoder and decoder are implemented as neural networks. To train these networks, we optimize for
the reconstruction error, \[
    \vec{\phi}^\star, \vec{\psi}^\star \in \argmin_{\vec{\phi}, \vec{\psi}} \sum_{n=1}^{N} \| \vec{x}_n - g_{\vec{\psi}} (f_{\vec{\phi}}(\vec{x}_n)) \|^2
\]

We can distinguish between \textit{undercomplete} and \textit{overcomplete} latent spaces. A latent
space is undercomplete if $\mathrm{dim}(\mathcal{Z}) < \mathrm{dim}(\mathcal{X})$, while it is
overcomplete if $\mathrm{dim}(\mathcal{Z}) > \mathrm{dim}(\mathcal{X})$. The idea of an
undercomplete hidden representation is to enable the network to learn the important features of the
data by reducing the dimensionality of the hidden space. This prevents the autoencoder from simply
copying the input and forces it to extract meaningful and discriminative features. Next,
overcomplete latent spaces are useful for denoising and inpainting autoencoders, where we have an
imperfect input and want a perfect output. The overcompleteness allows the model to extract more
features from the transformed input, leading to improved performance.

\subsection{Variational autoencoders}

While autoencoders are good at reconstruction, they struggle at generating new high quality
samples. This is due to the latent space not being ``well-structured'', meaning that there is no
continuity or interpolation. There are large regions in the latent space where there are no
observations, thus the model does not know what to output when it get an input from those regions.

\textit{Variational autoencoders} (VAE) \citep{kingma2013auto} are designed to have a continuous
latent space. It achieves this by making the encoder output a probability distribution over latent
vectors, rather than a single latent vector. Generally, it outputs a mean vector $\vec{\mu}$ and
standard deviation vector $\vec{\sigma}$ to form a Gaussian distribution over latents
$\mathcal{N}(\vec{\mu}, \mathrm{diag}(\vec{\sigma}))$. The idea is that even for the same input, the
latent vector can be different, but in the same area. This means that data points cover areas in the
latent space, rather than single points, ensuring continuity and interpolation.

However, since there are no limits on the values taken by $\vec{\mu}$ and $\vec{\sigma}$, the
encoder may learn to generate very different $\vec{\mu}$ for each class while minimizing
$\vec{\sigma}$. This would mean that the encoder essentially outputs points again to decrease the
reconstruction error. We can avoid this by minimizing the KL-divergence between the output
distribution and a standard normal distribution.\sidenote{The KL-divergence is defined as \[
        D_{\mathrm{KL}}(p \lVert q) \doteq \E \lft[ \log \lft( \frac{p(x)}{q(x)} \rgt) \rgt].
    \]
    It is not symmetric and non-negative.} Intuitively, this encourages the encoder to distribute the
encodings evenly around the center of the latent space.

To train the model, we want to maximize the likelihood of the training data, \[
    p(\vec{x}) = \int p_{\vec{\psi}}(\vec{x}\mid \vec{z}) p(\vec{z}) \mathrm{d}\vec{z}. \margintag{$p_{\vec{\psi}}(\vec{x} \mid \vec{z})$ is induced by the decoder.}
\]
However, this is intractable, because we cannot compute it for all $\vec{z} \in \mathcal{Z}$. Thus,
we define an approximation of the posterior, $q_{\vec{\phi}}(\vec{z}\mid \vec{x})$, which is
computed by the encoder. We can now derive the \textit{evidence lower bound} (ELBO),
\begin{align*}
    \log p(\vec{x}) & = \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} [\log p(\vec{x})] \margintag{$\vec{x}$ does not depend on $\vec{z}$.}                                                                                                                                                                                                    \\
                    & = \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} \lft[ \log \frac{p_{\vec{\psi}}(\vec{x}\mid \vec{z}) p(\vec{z})}{p(\vec{z} \mid \vec{x})} \rgt] \margintag{Bayes' rule.}                                                                                                                                                 \\
                    & = \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} \lft[ \log \lft( \frac{p_{\vec{\psi}}(\vec{x} \mid \vec{z}) p(\vec{z})}{p(\vec{z}\mid \vec{x})} \frac{q_{\vec{\phi}}(\vec{z}\mid \vec{x})}{q_{\vec{\phi}}(\vec{z}\mid \vec{x})} \rgt) \rgt] \margintag{$\nicefrac{q(\vec{z}\mid \vec{x})}{q(\vec{z}\mid \vec{x})} = 1$.} \\
                    & = \E_{\vec{z}\mid \vec{x}}[\log p_{\vec{\psi}}(\vec{x}\mid \vec{z})] - \E_{\vec{z}\mid \vec{x}} \lft[ \log \frac{q_{\vec{\phi}}(\vec{z}\mid \vec{x})}{p(\vec{z})} \rgt] + \E_{\vec{z}\mid \vec{x}} \lft[ \log \frac{q_{\vec{\phi}}(\vec{z}\mid \vec{x})}{p(\vec{z}\mid \vec{x})} \rgt]                                          \\
                    & = \E_{\vec{z}\mid \vec{x}}[\log p_{\vec{\psi}}(\vec{x}\mid \vec{z})] - D_{\mathrm{KL}}(q_{\vec{\phi}}(\vec{z}\mid \vec{x}) \rVert p(\vec{z})) + D_{\mathrm{KL}} (q_{\vec{\phi}}(\vec{z}\mid \vec{x}) \rVert p(\vec{z} \mid \vec{x}))                                                                                            \\
                    & \geq \E_{\vec{z}\mid \vec{x}}[\log p_{\vec{\psi}}(\vec{x}\mid \vec{z})] - D_{\mathrm{KL}}(q_{\vec{\phi}}(\vec{z}\mid \vec{x}) \rVert p(\vec{z})). \margintag{KL-divergence is non-negative.}
\end{align*}
The first term of the ELBO encourages low reconstruction error, while the second term makes sure that
the approximate posterior $q_{\vec{\phi}}$ does not deviate too far from the prior $p$. The second
term can be computed in a closed-form, since both arguments are Gaussian,
\begin{align*}
    q_{\vec{\phi}}(\vec{z} \mid \vec{x}) & = \mathcal{N}(\vec{z} ; \vec{\mu}_{\vec{\phi}}(\vec{x}), \mathrm{diag}(\vec{\sigma}_{\vec{\phi}}(\vec{x}))) \\
    p(\vec{z})                           & = \mathcal{N}(\vec{z}; \vec{0}, \mat{I}).
\end{align*}

A minor problem is that, during training, we cannot compute the derivative of expectations \wrt the
parameters that we wish to optimize. Thus, we must use the \textit{reparametrization trick}, which
involves treating the random sampling as a single noise term. In particular, instead of sampling
$\vec{z} \sim \mathcal{N}(\vec{\mu}, \mathrm{diag}(\vec{\sigma}))$, we sample $\vec{\epsilon} \sim
    \mathcal{N}(\vec{0},\mat{I})$ and compute $\vec{z} = \vec{\mu} + \vec{\sigma} \odot
    \vec{\epsilon}$. Using this trick, we can remove the mean and variance from the sampling operation,
meaning that we can differentiate \wrt the model parameters.

After training, we can sample a latent vector $\vec{z} \sim \mathcal{N}(\vec{0}, \mat{I})$ and pass
it to the decoder, which will give a good output, because the latent space is well-structured.

\subsection{$\beta$-VAE}

VAEs still have problems with their latent space; the representations are still
\textit{entangled}.\sidenote{The latent space is disentangled if every dimension changes a single
    feature of the output.} This means that we do not have an explicit way of controlling the output.
For example, in the MNIST dataset \citep{deng2012mnist}, we have no way of explicitly sampling a
specific number. The $\beta$-VAE \citep{higgins2017beta} solves this problem by giving more weight
to the KL term with an adjustable hyperparameter $\beta$ that balances latent channel capacity and
independence constraints with reconstruction accuracy. The intuition behind this is that if factors
are in practice independent from each other, the model should benefit from disentangling them.

In practice, we want to force the KL loss to be under a threshold $\delta > 0$,
\begin{align*}
    \underset{\vec{\phi},\vec{\psi}}{\text{maximize}} & \quad \E_{\vec{x} \sim \mathcal{D}} \lft[ \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} \lft[ \log p_{\vec{\psi}}(\vec{x} \mid \vec{z}) \rgt] \rgt] \\
    \text{subject to}                                 & \quad D_{\mathrm{KL}} \lft( q_{\vec{\phi}}(\vec{z} \mid \vec{x}) \lVert p(\vec{z}) \rgt) < \delta.
\end{align*}
Rewriting this as a Lagrangian, using the Karush-Kuhn-Tucker conditions, we get
\begin{align*}
    \mathcal{L}(\vec{\phi},\vec{\psi},\beta) & = \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} [\log p_{\vec{\psi}}(\vec{x} \mid \vec{z})] - \beta(D_{\mathrm{KL}}(q_{\vec{\phi}} \lVert p(\vec{z})) - \delta)      \\
                                             & = \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} [\log p_{\vec{\psi}}(\vec{x} \mid \vec{z})] - \beta D_{\mathrm{KL}}(q_{\vec{\phi}} \lVert p(\vec{z})) + \beta \delta \\
                                             & \geq \E_{\vec{z} \sim q_{\vec{\phi}}(\cdot \mid \vec{x})} [\log p_{\vec{\psi}}(\vec{x} \mid \vec{z})] - \beta D_{\mathrm{KL}}(q_{\vec{\phi}} \lVert p(\vec{z})).
\end{align*}
Thus, this becomes our new objective function that we wish to maximize.
