\section{Gradient descent}

\begin{marginfigure}
    \centering
    \incfig{gradient-descent}
    \caption{Gradient descent updates.}
    \label{fig:gradient-descent}
\end{marginfigure}

Gradient descent is an optimization algorithm that aims to find the global minimizer. We assume
that $f: \R^d \to \R$ is convex, differentiable, and has a global minimizer $\vec{x}^\star$. Then,
the goal of gradient descent is, for $\epsilon > 0$, to find a $\vec{x}\in\R^d$ such that \[
    f(\vec{x}) - f(\vec{x}^\star) \leq \epsilon.
\]

It works by first choosing a $\vec{x}_0 \in \R^d$ and then iteratively updating this value by \[
    \vec{x}_{t+1} = \vec{x}_t - \gamma \grad{f(\vec{x}_t)}{}, \margintag{Take a small step into the direction of the negative gradient to move toward the minimum.}
\]
for timesteps $t = 0,1,\ldots$ and stepsize $\gamma > 0$.

\subsection{Vanilla analysis}

We want to be able to bound $f(\vec{x}_t) - f(\vec{x}^\star)$, which tells us how far off we are
from the minimum at timestep $t$. For this, we can use the first-order characterization of
convexity, \[
    f(\vec{x}^\star) \geq f(\vec{x}_t) + \transpose{\grad{f(\vec{x}_t)}{}} (\vec{x}^\star - \vec{x}_t) \iff f(\vec{x}_t) - f(\vec{x}^\star) \leq \transpose{\grad{f(\vec{x}_t)}{}} (\vec{x}_t - \vec{x}^\star).
\]

Using the gradient descent update rule, we get \[
    \grad{f(\vec{x}_t)}{} = \frac{\vec{x}_t - \vec{x}_{t+1}}{\gamma},
\]
which gives the following bound
\begin{align*}
    f(\vec{x}_t) - f(\vec{x}^\star) & \leq \transpose{\grad{f(\vec{x}_t)}{}} (\vec{x}_t - \vec{x}^\star)                                                                                                                                                                                          \\
                                    & = \frac{1}{\gamma} \transpose{(\vec{x}_t - \vec{x}_{t+1})} (\vec{x}_t - \vec{x}^\star)                                                                                                                                                                      \\
                                    & = \frac{1}{2 \gamma} \lft( \|\vec{x}_t - \vec{x}_{t+1}\|^2 + \|\vec{x}_t - \vec{x}^\star\|^2 - \|\vec{x}_{t+1}-\vec{x}^\star\|^2 \rgt) \margintag{$2\transpose{\vec{v}} \vec{w} = \|\vec{v}\|^2 + \|\vec{w}\|^2 - \|\vec{v}-\vec{w}\|^2$ (cosine theorem).} \\
                                    & = \frac{\gamma}{2} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2\gamma}\lft( \|\vec{x}_t-\vec{x}^\star\|^2 - \|\vec{x}_{t+1}-\vec{x}^\star\|^2 \rgt). \margintag{$\vec{x}_t - \vec{x}_{t+1} = \gamma \grad{f(\vec{x}_t)}{}$.}
\end{align*}
Summing this up over the first $T$ iterations, we get an upper bound on the summed error,
\begin{align*}
    \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^\star)) & \leq \sum_{t=0}^{T-1} \transpose{\grad{f(\vec{x}_t)}{}}(\vec{x}_t - \vec{x}^\star)                                                                                                         \\
                                                       & = \frac{\gamma}{2} \sum_{t=0}^{T-1} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2\gamma} \lft( \|\vec{x}_0-\vec{x}^\star\|^2 - \|\vec{x}_T-\vec{x}^\star\|^2 \rgt) \margintag{Telescoping sum.} \\
                                                       & \leq \frac{\gamma}{2} \sum_{t=0}^{T-1} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2\gamma} \|\vec{x}_0-\vec{x}^\star\|^2. \margintag{$\|\vec{x}_T - \vec{x}^\star\|^2 \geq 0$.}
\end{align*}
We can also use this to get a bound on the average error, \[
    \frac{1}{T} \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^\star)) \leq \frac{1}{T} \lft( \frac{\gamma}{2} \sum_{t=0}^{T-1} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2\gamma} \|\vec{x}_0-\vec{x}^\star\|^2 \rgt).
\]

\subsection{Lipschitz convex functions}

\marginnote{A function $f: \R^m \to \R^n$ is Lipschitz continuous if there exists an $M$, such that
    $\forall \vec{x},\vec{y}\in\R:$ \[
        \|f(\vec{x})-f(\vec{y})\| \leq M \|\vec{x} - \vec{y}\|.
    \]
    This holds if and only if the gradient is bounded.}

\begin{theorem} \label{thm:lipschitz}
    Let $f: \R^d \to \R$ be convex and differentiable with a global minimum $\vec{x}^\star$.
    Furthermore, suppose that $\|\vec{x}_0-\vec{x}^\star\| \leq R$ and $\|\grad{f(\vec{x})}{}\| \leq B$
    for all $\vec{x}$. Choosing the stepsize, \[
        \gamma \doteq \frac{R}{B \sqrt{T}},
    \]
    gradient descent yields \[
        \frac{1}{T} \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^\star)) \leq \frac{RB}{\sqrt{T}}.
    \]
\end{theorem}

\begin{proof}
    We can plug our bounds into the vanilla analysis,
    \begin{align*}
        \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^\star)) & \leq \frac{\gamma}{2} \sum_{t=0}^{T-1} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2\gamma} \|\vec{x}_0-\vec{x}^\star\|^2 \\
                                                           & \leq \frac{\gamma}{2} B^2 T + \frac{1}{2\gamma} R^2                                                                  \\
                                                           & \doteq q(\gamma).
    \end{align*}
    Now, we want to choose $\gamma$ such that the bound $q(\gamma)$ is minimized. We can compute the
    minimum by computing the derivative, \[
        q'(\gamma) = \frac{1}{2} B^2 T - \frac{R^2}{2 \gamma^2},
    \]
    and solving for $q'(\gamma) = 0$, which yields \[
        \gamma = \frac{R}{B \sqrt{T}}.
    \]
    Then, we can compute the bound by \[
        q \lft( \frac{R}{B \sqrt{T}} \rgt) = RB \sqrt{T}.
    \]
    Dividing by $T$ yields the result.
\end{proof}

We want to find out how many iterations we would need to ensure that the average error is bounded
by $\epsilon$. We can use \Cref{thm:lipschitz} to compute a lower bound on the number of
iterations, \[
    \frac{RB}{\sqrt{T}} \leq \epsilon \implies T \geq \frac{R^2 B^2}{\epsilon^2}.
\]
So, the amount of iterations until convergence is of the order $\bigo{\nicefrac{1}{\epsilon^2}}$.
This means that we need at most $10000 \cdot R^2 B^2$ iterations to achieve an error of $0.01$.

\subsection{Smooth functions}

\marginnote{``Not too curved.``}

\begin{marginfigure}
    \begin{tikzpicture}
        \begin{axis}[
                scale only axis,
                width = \textwidth,
                axis lines = left,
                % Remove y axis
                y axis line style={opacity=0}, ytick=\empty,
                % Remove arrow from x axis and set to middle
                axis x line=middle, axis y line=middle, axis line style={-},
                % Remove x axis ticks
                xtick=\empty]

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
            ]{x};
            \addlegendentry{$x$}

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
                dotted,
            ]{(1/2) * x^2 - x};
            \addlegendentry{$g(x)$}

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
                dashed,
            ]
            {(-6) + (x - (-6)) + (1 / 2) * ((-6) - x)^2};

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
                dashed,
            ]
            {(4) + (x - (4)) + (1 / 2) * ((4) - x)^2};

            \addplot[black, mark=*, only marks] coordinates {(-6,-6) (4,4)};
        \end{axis}
    \end{tikzpicture}
    \caption{Plot of $f(x)=x$ with the tangent paraboloids at $x=-8,5$, showing smoothness with
        parameter $L=1$. Furthermore, it shows that $g(x) = \frac{1}{2}x^2 - f(x)$ is convex.}
    \label{fig:smooth}
\end{marginfigure}

\begin{definition}[Smoothness] \label{def:smoothness}
    Let $f: \dom{f} \to \R$ be differentiable, $\mathcal{X} \subseteq \dom{f}$ convex, $L\in \R_+$.
    $f$ is smooth (with parameter $L$) over $\mathcal{X}$ if
    $\forall \vec{x},\vec{y}\in \mathcal{X}:$ \[
        \underbrace{f(\vec{y}) \leq f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}} (\vec{y}-\vec{x})}_{\text{first-order convexity}} + \frac{L}{2} \|\vec{x}-\vec{y}\|^2.
    \]
\end{definition}

\begin{remark}
    This definition does not require convexity.
\end{remark}

Geometrically, this definition of smoothness means that the graph $f$ is below a not too steep
tangent paraboloid at $(\vec{x},f(\vec{x}))$; see \Cref{fig:smooth}. Furthermore, we can easily
check if $f$ is smooth with parameter $L$ if the following function is convex, \[
    g(\vec{x}) \doteq \frac{L}{2} \transpose{\vec{x}}\vec{x} - f(\vec{x}),
\]
over $\dom{g} \doteq \dom{f}$.

\begin{observation}[Operations that preserve smoothness]
    Let $f_1,\ldots,f_m$ be smooth with parameters $L_1,\ldots,L_m$ and let
    $\lambda_1,\ldots,\lambda_m\in \R_+$. Then, the function $f \doteq \sum_{i=1}^{m} \lambda_i f_i$
    is smooth with parameter $\sum_{i=1}^{m} \lambda_i L_i$.

    Let $f$ be smooth with parameter $L$ and let $g(\vec{x}) = \mat{A}\vec{x} + \vec{b}$. Then, the
    function $f\circ g$ is smooth with parameter $L\|\mat{A}\|^2$, where $\|\mat{A}\|$ is the spectral
    norm of $\mat{A}$.
\end{observation}

\begin{lemma}
    Let $f: \R^d \to \R$ be convex and differentiable, then $f$ is smooth with parameter $L$ if and only if $\forall \vec{x},\vec{y}\in \R^d:$ \[
        \|\grad{f(\vec{x})}{} - \grad{f(\vec{y})}{}\| \leq L\|\vec{x}-\vec{y}\|.
    \]
    \Ie, $\grad{f}{}$ is Lipschitz continuous.
\end{lemma}

\begin{lemma}[Sufficient decrease]
    Let $f: \R^d \to \R$ be differentiable and smooth with parameter $L$. With stepsize \[
        \gamma \doteq \frac{1}{L},
    \]
    gradient descent satisfies \[
        f(\vec{x}_{t+1}) \leq f(\vec{x}_t) - \frac{1}{2L} \|\grad{f(\vec{x}_t)}{}\|^2.
    \]
\end{lemma}

Sufficient decrease implies that, every step, we get closer to the minimum.

\begin{proof}
    \begin{align*}
        f(\vec{x}_{t+1}) & \leq f(\vec{x}_t) + \transpose{\grad{f(\vec{x}_t)}{}} (\vec{x}_{t+1} - \vec{x}_t) + \frac{L}{2} \|\vec{x}_t-\vec{x}_{t+1}\|^2 \margintag{Smoothness.}                                                                                           \\
                         & = f(\vec{x}_t) + \transpose{\grad{f(\vec{x}_t)}{}} \lft( - \frac{\grad{f(\vec{x}_t)}{}}{L} \rgt) + \frac{L}{2} \lft\| \frac{\grad{f(\vec{x}_t)}{}}{L} \rgt\|^2 \margintag{GD: $\vec{x}_{t+1} = \vec{x}_t - \frac{1}{L} \grad{f(\vec{x}_t)}{}$.} \\
                         & = f(\vec{x}_t) - \frac{1}{L} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2L} \| \grad{f(\vec{x}_t)}{} \|^2                                                                                                                                           \\
                         & = f(\vec{x}_t) - \frac{1}{2L} \| \grad{f(\vec{x}_t)}{} \|^2.
    \end{align*}
\end{proof}

\begin{theorem}
    Let $f: \R^d \to \R$ be convex and differentiable with a global minimum $\vec{x}^\star$.
    Furthermore, suppose that $f$ is smooth with parameter $L$. Choosing stepsize \[
        \gamma \doteq \frac{1}{L},
    \]
    gradient descent yields \[
        f(\vec{x}_T) - f(\vec{x}^\star) \leq \frac{L}{2T} \|\vec{x}_0 - \vec{x}^\star \|^2, \quad T > 0.
    \]
\end{theorem}

\begin{proof}
    Due to sufficient decrease, we can bound the sum of squared gradients (which will be useful when bounding
    the vanilla analysis),
    \begin{align*}
        \frac{1}{2L} \sum_{t=0}^{T-1} \| \grad{f(\vec{x}_t)}{} \|^2 & \leq \sum_{t=0}^{T-1} f(\vec{x}_t) - f(\vec{x}_{t+1}) \margintag{SD: $f(\vec{x}_{t+1}) \leq f(\vec{x}_t) - \frac{1}{2L} \|\grad{f(\vec{x}_t)}{}\|^2$.} \\
                                                                    & = f(\vec{x}_0) - f(\vec{x}_T). \margintag{Telescoping sum.}
    \end{align*}
    Using the vanilla analysis, we can derive a bound on the average error,
    \begin{align*}
        \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^\star))           & \leq \frac{1}{2L} \sum_{t=0}^{T-1} \| \grad{f(\vec{x}_t)}{} \|^2 + \frac{L}{2} \|\vec{x}_0-\vec{x}^\star\|^2 \margintag{$\gamma \doteq \frac{1}{L}$.} \\
                                                                     & \leq f(\vec{x}_0) - f(\vec{x}_T) + \frac{L}{2} \|\vec{x}_0 - \vec{x}^\star \|^2                                                                       \\
        \sum_{t=1}^{T} (f(\vec{x}_t) - f(\vec{x}^\star))             & \leq \frac{L}{2} \| \vec{x}_0 - \vec{x}^\star \|^2                                                                                                    \\
        \frac{1}{T} \sum_{t=1}^{T} (f(\vec{x}_t) - f(\vec{x}^\star)) & \leq \frac{L}{2T} \| \vec{x}_0 - \vec{x}^\star \|^2.
    \end{align*}
    From sufficient decrease, we know that the last iterate must be the best, thus \[
        f(\vec{x}_T) - f(\vec{x}^\star) \leq \frac{1}{T} \sum_{t=1}^{T} (f(\vec{x}_t) - f(\vec{x}^\star)) \leq \frac{L}{2T} \|\vec{x}_0 - \vec{x}^\star \|^2.
    \]
\end{proof}

Using this result, we can compute a bound on $T$ to achieve an error smaller than $\epsilon$, \[
    f(\vec{x}_T) - f(\vec{x}^\star) \leq \frac{L}{2T} \|\vec{x}_0 - \vec{x}^\star \|^2 \leq \frac{R^2L}{2T} \leq \epsilon.
\]
The error then becomes, \[
    T \geq \frac{R^2 L}{2 \epsilon},
\]
which is on the order of $\bigo{\nicefrac{1}{\epsilon}}$. This means that we need at most $50\cdot
    R^2 L$ iterations for an error of $0.01$ (as opposed to $10000 \cdot R^2 B^2$ in the Lipschitz
case).

\subsection{Smooth and strongly convex functions}

\marginnote{``Not too flat.``}

\begin{marginfigure}
    \begin{tikzpicture}
        \begin{axis}[
                scale only axis,
                width = \textwidth,
                axis lines = left,
                % Remove y axis
                y axis line style={opacity=0}, ytick=\empty,
                % Remove arrow from x axis and set to middle
                axis x line=middle, axis y line=middle, axis line style={-},
                % Remove x axis ticks
                xtick=\empty]

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
            ]{x^2};
            \addlegendentry{$x^2$}

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
                dotted,
            ]{x^2 - (1/2) * x^2};
            \addlegendentry{$g(x)$}

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
                dashed,
            ]
            {(-8)^2 + 2*(-8) * (x - (-8)) + (1 / 2) * ((-8) - x)^2};

            \addplot [
                domain=-10:10,
                samples=100,
                color=black,
                dashed,
            ]
            {(5)^2 + 2*(5) * (x - (5)) + (1 / 2) * ((5) - x)^2};

            \addplot[black, mark=*, only marks] coordinates {(-8,64) (5,25)};
        \end{axis}
    \end{tikzpicture}
    \caption{Plot of $f(x)=x^2$ with the tangent paraboloids at $x=-6,4$, showing strong convexity with
        parameter $\mu=1$. Furthermore, it shows $g(x) = f(x) - \frac{1}{2}x^2$, which is convex.}
    \label{fig:strong-convexity}
\end{marginfigure}

\begin{definition}[Strong convexity]
    Let $f: \dom{f} \to \R$ be a convex and differentiable function, $\mathcal{X}\in\dom{f}$ convex
    and $\mu \in \R_+, \mu > 0$. $f$ is called \textit{strongly convex} with parameter $\mu$ over
    $\mathcal{X}$ if $\forall \vec{x},\vec{y}\in \mathcal{X}:$ \[
        f(\vec{y}) \geq f(\vec{x}) + \transpose{\grad{f(\vec{x})}{}} (\vec{y} - \vec{x}) + \frac{\mu}{2} \|\vec{x}-\vec{y}\|^2.
    \]
\end{definition}

Geometrically, this means that, for any $\vec{x}$, the graph of $f$ is above a not too flat tangent
paraboloid at $(\vec{x},f(\vec{x}))$; see \Cref{fig:strong-convexity}. Furthermore, we can easily
check if $f$ is strongly convex if and only if the following function is convex, \[
    g(\vec{x}) \doteq f(\vec{x}) - \frac{\mu}{2}\transpose{\vec{x}}\vec{x}.
\]

By assuming that a function $f$ is smooth and strongly convex, we can use a stronger lower bound to
derive a bound on the error from the vanilla analysis.

\begin{theorem}
    Let $f: \R^d \to \R$ be convex and differentiable with a global minimum $\vec{x}^\star$.
    Furthermore, suppose that $f$ is smooth with parameter $L$ and strongly convex with parameter
    $\mu > 0$. Choosing $\gamma \doteq \nicefrac{1}{L}$, gradient descent with arbitrary $\vec{x}_0$
    satisfies the following two properties,
    \begin{enumerate}
        \item Squared distances to $\vec{x}^\star$ are geometrically decreasing, \[
                  \|\vec{x}_{t+1} - \vec{x}^\star\|^2 \leq \lft( 1-\frac{\mu}{L} \rgt) \|\vec{x}_t-\vec{x}^\star\|^2;
              \]
        \item The absolute error after $T$ iterations is exponentially small in $T$, \[
                  f(\vec{x}_T) - f(\vec{x}^\star) \leq \frac{L}{2} \lft( 1-\frac{\mu}{L} \rgt)^T \|\vec{x}_0 - \vec{x}^\star\|^2.
              \]
    \end{enumerate}
\end{theorem}

\begin{proof}[Proof of 1]
    Using the vanilla analysis, we know the following,
    \begin{align*}
        f(\vec{x}_t) - f(\vec{x}^\star) & \leq \transpose{\grad{f(\vec{x}_t)}{}} (\vec{x}_t - \vec{x}^\star)                                                                                       \\
                                        & = \frac{\gamma}{2} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2 \gamma} \lft( \| \vec{x}_t - \vec{x}^\star \|^2 - \| \vec{x}_{t+1}-\vec{x}^\star \|^2 \rgt).
    \end{align*}
    But, using strong convexity, we have a stronger lower bound on the left hand side,
    \begin{align*}
        f(\vec{x}_t) - f(\vec{x}^\star) & \leq \transpose{\grad{f(\vec{x}_t)}{}} (\vec{x}_t - \vec{x}^\star) - \frac{\mu}{2} \|\vec{x}_t-\vec{x}^\star\|^2 \margintag{Strong convexity.}          \\
                                        & = \frac{\gamma}{2} \|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{2 \gamma} \lft( \| \vec{x}_t - \vec{x}^\star \|^2 - \| \vec{x}_{t+1}-\vec{x}^\star \|^2 \rgt) \\
                                        & \quad - \frac{\mu}{2} \|\vec{x}_t-\vec{x}^\star\|^2.
    \end{align*}
    This can be rewritten to the following bound, \[
        \|\vec{x}_{t+1} - \vec{x}^\star\|^2 \leq \underbrace{2\gamma(f(\vec{x}^\star) - f(\vec{x}_t)) + \gamma^2\|\grad{f(\vec{x}_t)}{}\|^2}_{\text{``noise``}} + (1-\mu \gamma) \|\vec{x}_t - \vec{x}^\star\|^2.
    \]
    Now we need to show that the noise is non-positive,
    \begin{align*}
        2\gamma(f(\vec{x}^\star) - f(\vec{x}_t)) + \gamma^2\|\grad{f(\vec{x}_t)}{}\|^2 & = \frac{2}{L} (f(\vec{x}^\star) - f(\vec{x}_t)) + \frac{1}{L^2}\|\grad{f(\vec{x}_t)}{}\|^2 \margintag{$\gamma \doteq \frac{1}{L}$.}                                                         \\
                                                                                       & \leq \frac{2}{L} (f(\vec{x}_{t+1}) - f(\vec{x}_t)) + \frac{1}{L^2} \|\grad{f(\vec{x}_t)}{}\|^2 \margintag{Sufficient decrease.}                                                             \\
                                                                                       & \leq -\frac{1}{L^2}\|\grad{f(\vec{x}_t)}{}\|^2 + \frac{1}{L^2} \|\grad{f(\vec{x}_t)}{}\|^2 \margintag{SD: $f(\vec{x}_{t+1}) \leq f(\vec{x}_t) - \frac{1}{2L} \|\grad{f(\vec{x}_t)}{}\|^2$.} \\
                                                                                       & = 0.
    \end{align*}
    Hence, the noise is non-positive, and we get the following, \[
        \|\vec{x}_{t+1} - \vec{x}^\star\|^2 \leq \lft( 1-\frac{\mu}{L} \rgt) \|\vec{x}_t - \vec{x}^\star\|^2.
    \]
\end{proof}

\begin{proof}[Proof of 2]
    From (1), we know the following, \[
        \|\vec{x}_T - \vec{x}^\star\|^2 \leq \lft( 1-\frac{\mu}{L} \rgt)^T \|\vec{x}_0-\vec{x}^\star\|^2.
    \]
    Using smoothness, we can derive a bound on the final iterate error,
    \begin{align*}
        f(\vec{x}_T) - f(\vec{x}^\star) & \leq \transpose{\grad{f(\vec{x}^\star)}{}}(\vec{x}_T - \vec{x}^\star) + \frac{L}{2} \|\vec{x}_T - \vec{x}^\star\|^2          \\
                                        & = \frac{L}{2} \|\vec{x}_T - \vec{x}^\star\|^2 \margintag{$\grad{f(\vec{x}^\star)}{} = 0$, because it is a stationary point.} \\
                                        & \leq \frac{L}{2} \lft( 1-\frac{\mu}{L} \rgt)^T \|\vec{x}_0-\vec{x}^\star\|^2. \margintag{Using (1).}
    \end{align*}
\end{proof}

From there, we can derive a lower bound on the number of iterations $T$ to get an error of at most
$\epsilon$, \[
    T \geq \frac{L}{\mu} \log \lft( \frac{R^2L}{2 \epsilon} \rgt),
\]
This means that we need $\frac{L}{\mu} \log (50 \cdot R^2 L)$ iterations for an error of at most
$0.01$, as opposed to $50\cdot R^2 L$ in the smooth case. This bound only depends linearly on
$\frac{\mu}{L}$, which might be very high.
