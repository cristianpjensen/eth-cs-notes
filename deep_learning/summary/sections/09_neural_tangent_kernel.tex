\section{Neural tangent kernel}

\subsection{Linearized models}

The non-linearities of neural networks make them hard to analyze. We can linearize a model
$f[\vec{\theta}]$ by a first-order Taylor approximation over fixed parameters $\vec{\theta}_0$, \[
    f[\vec{\theta}] \approx f[\vec{\theta}_0] + \langle \grad{f[\vec{\theta}_0]}{}, \vec{\theta} - \vec{\theta}_0 \rangle, \quad \vec{\theta} \in \R^p.
\]
Here, $f$ is still a non-linear function, but it is linear \wrt $\vec{\theta}$. In this way, we can
define a linear model with parameters $\vec{\beta}$, \[
    h[\vec{\beta}](\vec{x}) \doteq f[\vec{\theta}_0](\vec{x}) + \transpose{\vec{\beta}} \grad{f[\vec{\theta}_0](\vec{x})}{}.
\]
Here, $\grad{f[\vec{\theta}]}{}: \R^d \to \R^p$ can be seen as constructing a $p$-dimensional
feature vector and $h[\vec{\beta}]$ a linear model over those features---we have a kernel method
with the following kernel, \[
    k(\vec{x}, \vec{x}') \doteq \transpose{\grad{f[\vec{\theta}_0](\vec{x})}{}} \grad{f[\vec{\theta}_0](\vec{x}')}{}.
\]
Assuming that we want to minimize the mean-squared error, \[
    \ell(\vec{\beta}) \doteq \frac{1}{2} \| \vec{f} + \mat{\Phi} \vec{\beta} - \vec{y} \|^2, \quad \mat{\Phi} \doteq [\grad{f[\vec{\theta}_0](\vec{x}_1)}{}, \ldots, \grad{f[\vec{\theta}_0](\vec{x}_n)}{}] \in \R^{n \times p},
\]
we get the following unique solution, \[
    \vec{\beta}^\star = \transpose{\mat{\Phi}} \inv{\mat{K}} (\vec{y} - \vec{f}), \quad \mat{K} \doteq \mat{\Phi} \transpose{\mat{\Phi}}.
\]
And, we make predictions by \[
    h^\star(\vec{x}) = \transpose{\vec{k}(\vec{x})} \inv{\mat{K}} (\vec{y} - \vec{f}), \quad \vec{k}(\vec{x}) \doteq [k(\vec{x}, \vec{x}_1), \ldots, k(\vec{x}, \vec{x}_n)] \in \R^n.
\]
We now have a linearized network---along with a way of evaluating it---which is simply an
approximation of a model with parameters $\vec{\theta}_0$. Note that computing $\mat{K}$ may be
intractable due to the number of samples and parameters. Furthermore, \citet{lee2019wide} showed
that linearized networks are non-competitive with full networks. However, the benefit of this
derivation is that we can look at neural networks through the lens of kernel methods, as long as
the parameters do not evolve far away from $\vec{\theta}_0$ such that the linear approximation
achieves a negligible loss.

\subsection{Training dynamics}

Consider the case where we wish to minimize the mean-squared error, \[
    \ell(\vec{\theta}) = \frac{1}{2} \| \vec{f}[\vec{\theta}] - \vec{y} \|^2, \quad \vec{f}[\vec{\theta}] \doteq [f[\vec{\theta}](\vec{x}_1), \ldots, f[\vec{\theta}](\vec{x}_n)].
\]
We optimize the model parameters by gradient descent, \[
    \vec{\theta}_{t+1} = \vec{\theta}_t - \eta \grad{\ell(\vec{\theta}_t)}{}.
\]
We can derive the gradient flow ODE of this process by rearranging the above,
\begin{align*}
    \frac{\vec{\theta}_{t+1} - \vec{\theta}_t}{\eta} & = - \grad{\ell(\vec{\theta}_t)}{}                                                                         \\
    \odv{\vec{\theta}_t}{t}                          & = - \grad{\ell(\vec{\theta}_t)}{} \margintag{$\eta$ is the stepsize and we turn it into continuous time.} \\
                                                     & = \sum_{i=1}^{n} (y_i - f[\vec{\theta}_t](\vec{x}_i)) \grad{f[\vec{\theta}_t](\vec{x}_i)}{}               \\
                                                     & \doteq \dot{\vec{\theta}}_t.
\end{align*}
We can also consider the functional gradient flow of the function $f_j[\vec{\theta}]$ for a data point
$\vec{x}_j$,
\begin{align*}
    \dot{f}_j[\vec{\theta}_t] & \doteq \odv{f[\vec{\theta}_t](\vec{x}_j)}{t}                                                                                                            \\
                              & = \transpose{\dot{\vec{\theta}}_t} \grad{f[\vec{\theta}_t](\vec{x}_j)}{} \margintag{Chain rule.}                                                        \\
                              & = \transpose{\lft( \sum_{i=1}^{n} (y_i - f[\vec{\theta}](\vec{x}_i)) \grad{f[\vec{\theta}_t](\vec{x}_i)}{} \rgt)} \grad{f[\vec{\theta}_t](\vec{x}_j)}{} \\
                              & = \sum_{i=1}^{n} (y_i - f[\vec{\theta}_t](\vec{x}_i)) \transpose{\grad{f[\vec{\theta}_t](\vec{x}_i)}{}} \grad{f[\vec{\theta}_t](\vec{x}_j)}{}           \\
                              & = \sum_{i=1}^{n} (y_i - f[\vec{\theta}_t](\vec{x}_i)) k[\vec{\theta}_t](\vec{x}_i, \vec{x}_j).
\end{align*}
This can also be written in matrix form as \[
    \dot{\vec{f}}[\vec{\theta}_t] = \mat{K}[\vec{\theta}_t] (\vec{y} - \vec{f}[\vec{\theta}_t]). \margintag{We can generalize this to any loss function by \[ \dot{\vec{f}}[\vec{\theta}_t] = -\mat{K}[\vec{\theta}_t] \grad{\ell(\vec{\theta}_t)}{\vec{f}[\vec{\theta}]}. \] But, for our purposes, we only consider the MSE loss.}
\]
This shows that the kernel governs the evolution of the joint sample predictions---the kernel is
called the NTK (\textit{\textbf{N}eural \textbf{T}angent \textbf{K}ernel}).

Now we can analyze how gradient descent behaves---it is entirely dependent on the kernel. However,
the NTK has a dependence on the parameters, so it is practically not very interesting. We will next
see why the NTK is interesting as a theoretical tool.

\subsection{Infinite width}

In practice, it has been found that as the width of a model is scaled, the parameters stay close to
their initialization during gradient descent. One can prove, under basic assumptions, that if the
model is scaled to infinite width, then the kernel converges in probability to a deterministic
limit, \[
    k[\vec{\theta}] \xrightarrow{p \to \infty} k_{\infty}, \quad \vec{\theta} \in \R^p.
\]
The limit $k_{\infty}$ only depends on the law of initialization, not the initial random parameters
themselves. Effectively, there is only one infinite-width network at initialization. Under these
training dynamics, minimizing the mean-squared error equates to solving a kernel regression problem
with $k_{\infty}$. This provides analytical insight into why overparametrization works so well in
practice and why such models generalize, despite having the obvious ability to overfit.

\subsection{NTK constancy}

Consider an MLP with $L$ layers and $m_l$ denoting the number of parameters in layer $l \in [L]$,
where we initialize the parameters by \[
    w^l_{ij} \sim \mathcal{N}\lft( 0, \frac{\sigma_w}{\sqrt{m_l}} \rgt), \quad b^l_{i} \sim \mathcal{N}\lft( 0, \frac{\sigma_b}{\sqrt{m_l}} \rgt). \margintag{LeCun initialization.}
\]
Now consider the case where $m_l \to \infty$ for all $l \in [L]$. Under suitable conditions, it can
be shown that with the above scaling, the initial NTK converges to a deterministic limit
$k_\infty$. The kernel limit depends only on the law of initialization and not the actual
initialized values. Effectively, there is only one possible initial infinite-width network. With a
few additional assumptions, it can also be shown that $\vec{\theta}_t$ converges uniformly to
$k_\infty$.

In other words, the NTK remains constant under gradient flow---NTK constancy, \[
    \pdv{k[\vec{\theta}_t]}{t} = 0.
\]
When NTK constancy holds, learning in the infinite width limit works the same as for the linearized
model---kernel methods. As a result, the solution to NTK learning can be expressed as \[
    f_\infty(\vec{x}) = \transpose{\vec{k}_\infty(\vec{x})} \inv{\mat{K}_\infty} (\vec{y} - \vec{f}).
\]

\citet{bietti2019inductive} showed that the NTK of a two-layer MLP with a ReLU activation function
can analytically be written as \[
    k_\infty(\vec{x}, \vec{x}') = \| \vec{x} \| \| \vec{x}' \| \kappa \lft( \frac{\transpose{\vec{x}}\vec{x}'}{\| \vec{x} \| \| \vec{x}' \|} \rgt),
\]
where \[
    \kappa(u) \doteq \frac{2u}{\pi} \lft( \pi - \arccos(u) \rgt) + \frac{\sqrt{1-u^2}}{\pi}.
\]
