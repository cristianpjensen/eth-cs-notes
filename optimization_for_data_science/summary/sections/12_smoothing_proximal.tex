\section{Smoothing and proximal algorithms}

Often, we want to optimize non-smooth functions. However, most of the time, we assume functions to
be smooth. The question is thus whether we can exploit additional structure of non-smooth
functions, instead of treating them as black boxes. The idea behind smoothing is to optimize a
smooth approximation $f_{\mu}$ of the non-smooth function $f$.

\subsection{Nesterov smoothing}

\begin{definition}[Conjugate function]
    The conjugate function of $f$ is \[
        f^*(\vec{y}) = \sup_{\vec{x} \in \dom{f}} \lft\{ \transpose{\vec{x}} \vec{y} - f(\vec{x}) \rgt\}.
    \]
    It is also called the Legendre-Fenchel transformation.
\end{definition}

\begin{lemma}[Convex conjugate properties]
    The following holds for conjugate functions,
    \begin{enumerate}
        \item (Duality) If $f$ is continuous and convex, then $f^{**} = f$;
        \item (Fenchel's inequality) \[
                  f(\vec{x}) + f^*(\vec{y}) \geq \transpose{\vec{x}} \vec{y}, \quad \forall \vec{x}, \vec{y};
              \]
        \item If $f$ and $g$ are continuous and convex, then \[
                  (f+g)^*(\vec{x}) = \inf_{\vec{y}} \lft\{ f^*(\vec{y}) + g^*(\vec{x} - \vec{y}) \rgt\};
              \]
        \item If $f$ is $\mu$-strongly convex, then $f^*$ is differentiable and $\nicefrac{1}{\mu}$-smooth.
    \end{enumerate}
\end{lemma}

Nesterov smoothing approximates a non-smooth function $f$ by \[
    f_{\mu}(\vec{x}) = \max_{\vec{y} \in \dom{f^*}} \lft\{ \transpose{\vec{x}}\vec{y} - f^*(\vec{y}) - \mu \cdot d(\vec{y}) \rgt\},
\]
where $d(\vec{y})$ is a proximity function. A proximity function is 1-strongly convex and
non-negative. The function $f_{\mu}$ is $\nicefrac{1}{\mu}$-smooth and approximates a convex $f$ by \[
    f(\vec{x}) - \mu D^2 \leq f_{\mu}(\vec{x}) \leq f(\vec{x}), \quad D^2 \doteq \max{\vec{y} \in \dom{f^*}} d(\vec{y}). \margintag{High $\mu$ results in a bad approximation.}
\]
Thus, we have a trade-off between approximation error and optimization efficiency. Specifically, \[
    f(\vec{x}) - f(\vec{x}^\star) \leq \underbrace{f(\vec{x}) - f_{\mu}(\vec{x})}_{\textit{approximation error}} + \underbrace{f_{\mu}(\vec{x}) - \min_{\vec{x}} f_{\mu}(\vec{x})}_{\textit{optimization error}}.
\]
The approximation error is on the order $\bigo{\mu}$, while the optimization error is on the order
$\bigo{\nicefrac{1}{\mu t}}$ using gradient descent.

If we apply accelerated gradient descent to solve the smoothed problem, we get an error of the
following order, \[
    f(\vec{x}_t) - f(\vec{x}^\star) \leq \bigo{\mu D^2 + \frac{R^2}{\mu t^2}}.
\]
Note that this is faster than applying subgradient methods.

\subsection{Moreau-Yosida smoothing}

Moreau-Yosida regularization smooths $f$ by \[
    f_{\mu}(\vec{x}) = \min_{\vec{y} \in \dom{f^*}} \lft\{ f(\vec{y}) + \frac{1}{2\mu} \| \vec{x} - \vec{y} \|_2^2 \rgt\}.
\]
This function is called the \textit{Moreau envelope} of $f(\vec{x})$. For example, the Huber
function is the Moreau envelope of $f(x) = |x|$, \[
    f_{\mu}(x) = \begin{cases}
        \frac{x^2}{2 \mu}, \quad |x| \leq \mu \\
        |x| - \frac{\mu}{2}, \quad |x| > \mu.
    \end{cases}
\]
As in Nesterov smoothing, $f_{\mu}$ is $\nicefrac{1}{\mu}$-smooth. However, the advantage is that
it minimizes exactly, \ie, $\min_{\vec{x}} f(\vec{x}) = \min_{\vec{x}} f_{\mu}(\vec{x})$.

\subsection{Proximal operators}

\begin{definition}[Proximal operator]
    The proximal operator of a convex function $f$ at $\vec{x}$ is defined as \[
        \mathrm{prox}_f(\vec{x}) \doteq \argmin_{\vec{y} \in \dom{f}} \lft\{ f(\vec{y}) + \frac{1}{2} \| \vec{x} - \vec{y} \|_2^2 \rgt\}.
    \]
\end{definition}
Note that this is a special case of Moreau-Yosida regularization with $\mu = 1$. Furthermore, for
many non-smooth functions, their proximal operator can be computed efficiently in a closed form.

\subsection{Proximal point algorithm}

The proximal point algorithm (PPA) repeatedly applies the proximal operator, \[
    \vec{x}_{t+1} = \mathrm{prox}_{\lambda_t f}(\vec{x}_t).
\]

\begin{theorem}[Convergence of PPA]
    If $f$ is convex, then for any $T \geq 1$, we have \[
        f(\vec{x}_{T+1}) - f(\vec{x}^\star) \leq \frac{\| \vec{x}_0 - \vec{x}^\star \|_2^2}{2 \sum_{t=1}^{T} \lambda_t}.
    \]
\end{theorem}
If we set $\lambda_t = \lambda$ to be constant, we get a $\bigo{\nicefrac{1}{t}}$ convergence rate.

\subsection{Proximal gradient method}

Assume the following convex composite optimization problem, \[
    \min_{\vec{x} \in \R^d} f(\vec{x}) + g(\vec{x}),
\]
where $f$ and $g$ are convex.\sidenote{Most supervised learning problems can be cast into this
    form, \[
        \min_{\vec{\theta}} \frac{1}{n} \sum_{i=1}^{n} \ell(h_{\vec{\theta}}(\vec{x}_i), y_i) + g(\vec{\theta}),
    \]
    where $h_{\vec{\theta}}$ is the predictor and $g$ is a regularization function.} The proximal
gradient method (PGM) has the following update rule, \[
    \vec{x}_{t+1} = \mathrm{prox}_{\gamma_t g}(\vec{x}_t - \gamma_t \grad{f(\vec{x}_t)}{}).
\]
Note that it alternates between a gradient update on $f$ and a proximal operator on $g$.

\begin{theorem}[Convergence of PGM]
    Let $F(\vec{x}) = f(\vec{x}) + g(\vec{x})$. Assume $f$ is convex and $L$-smooth, $g$ is convex
    and possibly non-smooth. Proximal gradient method with fixed stepsize $\gamma_t = \nicefrac{1}{L}$
    satisfies \[
        F(\vec{x}_t) - F(\vec{x}^\star) \leq \frac{L \| \vec{x}_0 - \vec{x}^\star \|_2^2}{2t}.
    \]
\end{theorem}
This is nearly the same convergence rate as gradient descent, despite $F$ being possibly non-smooth.
