\section{Reinforcement learning}

\begin{marginfigure}
    \centering
    \incfig{reinforcement-learning-algorithms}
    \caption{Overview of the types of reinforcement learning algorithms.}
    \label{fig:reinforcement-learning-algorithms}
\end{marginfigure}

Unlike in Markov decision processes, in reinforcement learning, we do not have access to the
transition model and reward function. Thus, we cannot use value iteration, policy iteration, or
linear programming to plan within this unknown environment. In some way, we must learn from the
environment by interacting with it.

Furthermore, even if we did have access to the underlying MDP, the state and action spaces might be
very large or even infinite. In these cases, we must make some form of approximation to be able to
act within the environment.

These two points make up the two fundamental challenges of reinforcement learning; learning and
representation.

We can distinguish between two kinds of RL algorithms; see
\Cref{fig:reinforcement-learning-algorithms},
\begin{itemize}
    \item \textit{Model-based}, where we learn the underlying MDP, and then do planning by one of the
          algorithms seen in the previous section;

    \item \textit{Model-free}, where we do not learn the underlying MDP. As we saw in the previous
          section, you only need the value function to act optimally within an environment. Thus, we
          either learn the value function or directly learn the policy, which maps states to actions.
\end{itemize}
Generally, model-free algorithms are less expensive to run, while model-based algorithms are more
sample efficient.

Furthermore, we can distinguish between RL algorithms by the following,
\begin{itemize}
    \item \textit{On-policy}, where the algorithm must learn from its own actions;
    \item \textit{Off-policy}, where the algorithm can learn from the trajectory data of any algorithm.
\end{itemize}
Generally, we prefer algorithms to be off-policy, since we can collect data more efficiently.

A way that RL differs from supervised learning is that the data depends on past actions; data is
not independently and identically distributed. RL algorithms learn from trajectory data, which
looks like the following, \[
    \tau = (x_0,a_0,r_0,x_1,a_1,r_1,x_2, \ldots).
\]
