\section{Preliminaries}

\subsection{Vector spaces}

The \textit{vector space} $\R^m$ consists of all column vectors with $m$ elements. For a set of
vectors $\mathcal{C} = \{ \vec{c}_1,\ldots,\vec{c}_n \mid \vec{c}_i \in \R^m \}$, we can define a
subspace spanned by this set, denoted by $\mathrm{span}(\mathcal{C})$. It is the set of all
possible linear combinations of elements of $\mathcal{C}$. If a set of vectors that span a subspace
are independent, they are called a \textit{basis}, $\vec{b}_1,\ldots,\vec{b}_k \in \R^m$. The
number of basis vectors defines the \textit{dimensionality} of the subspace.\sidenote{We know that
    the amount of basis vectors must be smaller than the amount of vectors that span the subspace,
    which must be smaller than the dimensionality of the space, \[
        k \leq m \leq n.
    \]}

\begin{observation}
    The following facts hold about subspaces,
    \begin{itemize}
        \item Every subspace contains the zero vector $\vec{0}$;
        \item If $\vec{x}$ and $\vec{y}$ are in the subspace, then $\vec{x}+\vec{y}$ is also in the subspace;
        \item If $\vec{x}$ is in the subspace and $a\in \R$, then $a\vec{x}$ is also in the subspace.
    \end{itemize}
\end{observation}

\begin{definition}[Orthogonal subspaces]
    Subspaces $\mathcal{V}$ and $\mathcal{W}$ are orthogonal when $\transpose{\vec{v}}\vec{w} = 0$ for
    all $\vec{v}\in \mathcal{V}, \vec{w}\in \mathcal{W}$.
\end{definition}

\subsection{Norms}

\begin{definition}[Inner product]
    An inner product $\ang{\cdot,\cdot}: \mathcal{V} \times \mathcal{V} \to \R$ is an operation
    defined on a vector space $\mathcal{V}$ that satisfies the following properties $\forall \vec{x},\vec{y},\vec{z},a,b\in \R$,
    \begin{itemize}
        \item Commutativity: $\ang{\vec{x},\vec{y}} = \ang{\vec{y},\vec{x}}$;
        \item Linearity: $\ang{\vec{x},a \vec{y} + b \vec{z}} = a \ang{\vec{x},\vec{y}} +
                  b\ang{\vec{x},\vec{z}}$;
        \item Positive definiteness,
              \begin{align*}
                  \vec{x} \neq \vec{0} & \implies \ang{\vec{x},\vec{x}} > 0 \\
                  \vec{x} = 0          & \iff \ang{\vec{x},\vec{x}} = 0;
              \end{align*}
        \item Bilinearity (follows from commutativity and linearity),
              \begin{align*}
                  \ang{\vec{x} + \vec{y}, \vec{z}} & = \ang{\vec{x},\vec{z}} + \ang{\vec{y},\vec{z}}  \\
                  \ang{\vec{x},\vec{y} + \vec{z}}  & = \ang{\vec{x},\vec{y}} + \ang{\vec{x},\vec{z}}.
              \end{align*}
    \end{itemize}
\end{definition}

\begin{corollary}
    $\ang{\vec{x}+\vec{y},\vec{x}+\vec{y}} = \ang{\vec{x},\vec{x}} + 2\ang{\vec{x},\vec{y}} + \ang{\vec{y},\vec{y}}$.
\end{corollary}

\begin{corollary}
    $\ang{\mat{A}\vec{x}, \vec{y}} = \ang{\vec{x}, \transpose{\mat{A}} \vec{y}}$.
\end{corollary}

The vector space $\mathcal{V}$, along with an inner product, defines an inner vector space. During
this course, we will assume that we always work with real vectors in $\R^n$. An example of an inner
product is the dot product,\sidenote{Usually, this operation is what is meant by the inner
    product.} \[
    \vec{x}\cdot \vec{y} = \transpose{\vec{x}} \vec{y} = \sum_{i=1}^{n} x_i y_i.
\]

\begin{definition}[Norm]
    A norm $\|\cdot\|: \R^n \to \R$ is a function that can be thought of as a way of measuring the distance from the origin.
    Norms satisfy the following properties,
    \begin{itemize}
        \item Positive definiteness, $\vec{x} \neq \vec{0} \implies \|\vec{x}\| > 0$;
        \item Triangle inequality, $\|\vec{x}+\vec{y}\| \leq \|\vec{x}\| + \|\vec{y}\|$;
        \item Cauchy-Schwarz inequality, $|\ang{\vec{x},\vec{y}}| \leq \|\vec{x}\| \|\vec{y}\|$.
    \end{itemize}
\end{definition}

\begin{corollary}
    For the Euclidean norm, the following holds, \[
        \cos \theta = \frac{\ang{\vec{x},\vec{y}}}{\|\vec{x}\| \|\vec{y}\|},
    \]
    where $\theta$ is the angle between $x$ and $y$.
\end{corollary}

\begin{corollary}[Cosine theorem]
    \[
        \| \vec{x} - \vec{y} \|^2 = \| \vec{x} \|^2 + \| \vec{y} \|^2 - 2 \langle \vec{x}, \vec{y} \rangle.
    \]
\end{corollary}

Each inner product defines a canonical norm $\|\vec{x}\| \doteq \sqrt{\ang{\vec{x},\vec{x}}}$. For
example, the Euclidean norm is defined by the dot product, \[
    \|\vec{x}\|_2 = \sqrt{\transpose{\vec{x}}\vec{x}} = \sqrt{\sum_{i=1}^{n} x_i^2}.
\]
The $p$-norm is a generalization of the Euclidean norm, \[
    \|\vec{x}\|_p = \sqrt{\sum_{i=1}^{n} |x_i|^p}.
\]

\subsection{Matrices}

The rank $r$ of a matrix $\mat{A}\in \R^{m\times n}$ is the dimensionality of its \textit{column
    space}. It is bounded by \[
    r \leq \min \{ m,n \}. \margintag{The matrix is full-rank if $r=\min \{ m,n \}$.}
\]

A matrix $\mat{A} \in \R^{m\times n}$ defines 4 fundamental subspaces,
\begin{itemize}
    \item Column space $\subseteq \R^m$ ($r$ dimensional), $\{ \vec{b} \mid \mat{A}\vec{x} = \vec{b} \}$;
    \item Null space $\subseteq \R^n$ ($n-r$ dimensional), $\{ \vec{x} \mid \mat{A}\vec{x} = \vec{0} \}$;
    \item Row space $\subseteq \R^n$ ($r$ dimensional), $\{ \vec{b} \mid \transpose{\mat{A}}\vec{x} = \vec{b}
              \}$;
    \item Left null space $\subseteq \R^m$ ($m-r$ dimensional), $\{ \vec{x} \mid \transpose{\mat{A}} \vec{x}
              = \vec{0} \}$.
\end{itemize}

The row space $\mathrm{row}(\mat{A})$ is the \textit{orthogonal complement} of the null space
$\mathrm{null}(\mat{A})$, thus $\mathrm{row}(\mat{A}) + \mathrm{null}(\mat{A}) = \R^n$. Similarly,
$\mathrm{col}(\mat{A}) + \mathrm{null}(\transpose{\mat{A}}) = \R^m$.

\begin{marginfigure}
    \centering
    \incfig{matrix-spaces}
    \caption{Illustration of the 4 spaces defined by a matrix $\mat{A}$. It shows the perpendicular spaces.
        Furthermore, it shows that $\mat{A}\vec{x}_r = \vec{b}$ for some $\vec{x}_r \in \mathrm{col}(\mat{A})$. Also,
        if you add a vector from the null space to the row vector, it still maps to the same $\vec{b}$,
        $\mat{A}(\vec{x}_r + \vec{x}_n) = \mat{A}\vec{x}_r + \mat{A}\vec{x}_n = \mat{A}\vec{x}_r = \vec{b}$.}
    \label{fig:matrix-spaces}
\end{marginfigure}

\begin{definition}[Orthonormal matrix]
    An orthonormal matrix is an invertible matrix whose columns $\vec{q}_1,\ldots,\vec{q}_n$ are all
    orthogonal to each other and of unit length, \ie,
    \begin{align*}
        \transpose{\vec{q}_i} \vec{q}_j & = 0, \quad \forall i \neq j \in [n] \\
        \transpose{\vec{q}_i} \vec{q}_i & = 1, \quad \forall i \in [n].
    \end{align*}
\end{definition}

\begin{properties}
    Let $\mat{Q}\in \R^{n\times n}$ be an orthogonal matrix,
    \begin{align*}
        \transpose{\mat{Q}}   & = \inv{\mat{Q}}                         \\
        \ang{\vec{x},\vec{y}} & = \ang{\mat{Q}\vec{x}, \mat{Q}\vec{y}}.
    \end{align*}
\end{properties}

\begin{definition}[Trace]
    The trace of a square matrix $\mat{A}\in \R^{n\times n}$ is the sum of its diagonal, \[
        \tr{\mat{A}} = \sum_{i=1}^{n} a_{ii}.
    \]
\end{definition}

\begin{properties}
    Let $\mat{A},\mat{B},\mat{C}\in \R^{n\times n}, \vec{x},\vec{y}\in \R^n, c,d \in \R$, then
    \begin{align*}
        \tr{c \mat{A} + d \mat{B}} & = c\cdot \tr{\mat{A}} + d\cdot \tr{\mat{B}} \margintag{Linearity.}                                                                \\
        \tr{\mat{A}}               & = \tr{\transpose{\mat{A}}}                                                                                                        \\
        \tr{\mat{A}\mat{B}\mat{C}} & = \tr{\mat{C}\mat{A}\mat{B}} = \tr{\mat{B}\mat{C}\mat{A}} \margintag{Cyclic property.}                                            \\
        \transpose{\vec{x}}\vec{y} & = \tr{\transpose{\vec{x}}\vec{y}} = \tr{\vec{x}\transpose{\vec{y}}}. \margintag{$\vec{x}\transpose{\vec{y}}$ is a rank-1 matrix.}
    \end{align*}
    Furthermore, $\tr{\mat{A}}$ is equal to the sum of the eigenvalues of $\mat{A}$.
\end{properties}

\subsection{Convexity}

\begin{definition}[Convexity]
    A function $f: \R^n \to \R$ is convex if \[
        f(\lambda \vec{x} + (1-\lambda) \vec{y}) \leq \lambda f(\vec{x}) + (1-\lambda) f(\vec{y}), \quad \vec{x}, \vec{y} \in \R^n, \lambda \in [0,1].
    \]
\end{definition}

\begin{lemma}[Jensen's inequality]
    Let $f$ be convex, then \[
        f(\E[\vec{x}]) \leq \E[f(\vec{x})].
    \]
\end{lemma}
