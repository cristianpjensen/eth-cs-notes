\section{Fully convolutional neural network}

\textit{Semantic segmentation} is a computer vision task that involves assigning a semantic class to
each pixel in an image. While in image classification, the model must output a single class for the
entire image, semantic segmentation requires classifying a class for each pixel individually.

A naive approach would be to apply a single convolutional layer to an image, and then running a
classifier on each individual pixel. However, this method is inefficient, because we have to run
the classifier $H\times W$ times. Instead, we use the output of convolutional neural networks. A
naive approach of using CNNs would be to simply apply $n$ convolutional layers with no
downsampling, and then considering the last output as the predicted segmentation map. However, this
method is expensive.

In practice, the most common approach is to downsample the features obtained using convolution and
pooling layers and then upsample them again. By downsampling, this method is more computationally
efficient, has larger receptive field, and suffers less from ``The curse of dimensionality.'' By
upsampling, the model produces an output of the same resolution as the input.

\subsection{Upsampling methods}

\paragraph{Nearest neighbor.}

Nearest neighbor upsampling copies the same value into all corresponding pixels at a higher
resolution; see \Cref{fig:nearest-neighbor-upsampling}.

\begin{marginfigure}
    \centering
    \incfig{nearest-neighbor-upsampling}
    \caption{Nearest neighbor upsampling.}
    \label{fig:nearest-neighbor-upsampling}
\end{marginfigure}

\paragraph{Bed of nails.}

Bed of nails upsampling only copies each value once into the output in the top left value, and pads
the rest with zero; see \Cref{fig:bed-of-nails-upsampling}.

\begin{marginfigure}
    \centering
    \incfig{bed-of-nails-upsampling}
    \caption{Bed of nails upsampling.}
    \label{fig:bed-of-nails-upsampling}
\end{marginfigure}

\paragraph{Max unpooling.}

Max unpooling also uses zero padding, like bed of nails. However, it also remembers the original
position of the maximum value before the corresponding max pooling in the downsampling phase. This
information is then used to place each element back in the correct position; see
\Cref{fig:max-pooling,fig:max-unpooling}.

\begin{marginfigure}
    \centering
    \incfig{max-unpooling}
    \caption{Max unpooling the output of \Cref{fig:max-pooling}.}
    \label{fig:max-unpooling}
\end{marginfigure}

\paragraph{Transposed convolutions.}

Transposed convolution \citep{shelhamer2016fully} is a learned upsampling technique. This layer
learns a kernel that is used to produce the terms whose sum will be the final output. Each term is
obtained by multiplying all the element of the kernel by the same value of one single input pixel
and then inserting the result in the correct position of a matrix of the same size as the output.

\subsection{U-net}

The \textit{U-net} \citep{ronneberger2015unet} is an FCNN architecture, whose main idea is to
combine global and local feature maps by copying corresponding tensors from earlier stages in each
upsampling stage; see \Cref{fig:u-net}. This allows the network to capture both local and global
context. In each upsampling, the corresponding output from the downsampling phase is appended to
the output. The copied tensor can be seen as the ``global'' information, while the input of the
upsampling layer is the ``local'' information. Combining these allows for more fine-grained
outputs.

\begin{marginfigure}
    \centering
    \incfig{u-net}
    \caption{U-net architecture. Down arrows are downsampling layers, up arrows are upsampling layers, and right arrows copy.}
    \label{fig:u-net}
\end{marginfigure}
