\section{Smoothing and proximal algorithms}

Often, we want to optimize non-smooth functions. However, most of the time, we assume functions to
be smooth. The question is thus whether we can exploit additional structure of non-smooth
functions, instead of treating them as black boxes. The idea behind smoothing is to optimize a
smooth approximation $f_{\mu}$ of the non-smooth function $f$.

\subsection{Convex conjugate theory}

\begin{definition}[Conjugate function]
    The conjugate function of $f$ is \[
        f^*(\vec{y}) \doteq \sup_{\vec{x} \in \dom{f}} \lft\{ \transpose{\vec{x}} \vec{y} - f(\vec{x}) \rgt\}.
    \]
    It is also called the Legendre-Fenchel transformation.
\end{definition}

\begin{lemma}[Convex conjugate properties]
    The following holds for conjugate functions,
    \begin{enumerate}
        \item (Duality) If $f$ convex, then $f^{**} = f$;
        \item (Fenchel's inequality) \[
                  f(\vec{x}) + f^*(\vec{y}) \geq \transpose{\vec{x}} \vec{y} \iff \vec{y} \in \partial f(\vec{x}) \iff \vec{x} \in \partial f^*(\vec{y}), \quad \forall \vec{x}, \vec{y};
              \]
        \item If $f$ and $g$ are convex, then \[
                  (f+g)^*(\vec{x}) = \inf_{\vec{y}} \lft\{ f^*(\vec{y}) + g^*(\vec{x} - \vec{y}) \rgt\};
              \]
        \item If $f$ is $\mu$-strongly convex, then $f^*$ is differentiable and $\nicefrac{1}{\mu}$-smooth.
    \end{enumerate}
\end{lemma}

\subsection{Nesterov smoothing}

Nesterov smoothing approximates a non-smooth function $f$ by \[
    f_{\mu}(\vec{x}) = \max_{\vec{y} \in \dom{f^*}} \lft\{ \transpose{\vec{x}}\vec{y} - f^*(\vec{y}) - \mu \cdot d(\vec{y}) \rgt\},
\]
where $d(\vec{y})$ is a proximity function. A proximity function is 1-strongly convex and
non-negative. The function $f_{\mu}$ is $\nicefrac{1}{\mu}$-smooth and approximates a convex $f$ by \[
    f(\vec{x}) - \mu D^2 \leq f_{\mu}(\vec{x}) \leq f(\vec{x}), \quad D^2 \doteq \max_{\vec{y} \in \dom{f^*}} d(\vec{y}). \margintag{High $\mu$ results in a bad approximation.}
\]
Thus, we have a trade-off between approximation error and optimization efficiency. Specifically, \[
    f(\vec{x}) - f(\vec{x}^\star) \leq \underbrace{f(\vec{x}) - f_{\mu}(\vec{x})}_{\textit{approximation error}} + \underbrace{f_{\mu}(\vec{x}) - \min_{\vec{x}} f_{\mu}(\vec{x})}_{\textit{optimization error}}.
\]
The approximation error is on the order $\bigo{\mu}$, while the optimization error is on the order
$\bigo{\nicefrac{1}{\mu t}}$ using gradient descent.

If we apply accelerated gradient descent to solve the smoothed problem, we get an error of the
following order, \[
    f(\vec{x}_t) - f(\vec{x}^\star) \leq \bigo{\mu D^2 + \frac{R^2}{\mu t^2}}.
\]
Note that this is faster than applying subgradient methods.

\subsection{Moreau-Yosida smoothing}

Moreau-Yosida regularization smooths $f$ by \[
    f_{\mu}(\vec{x}) = \min_{\vec{y} \in \dom{f^*}} \lft\{ f(\vec{y}) + \frac{1}{2\mu} \| \vec{x} - \vec{y} \|_2^2 \rgt\}.
\]
This function is called the \textit{Moreau envelope} of $f(\vec{x})$. For example, the Huber
function is the Moreau envelope of $f(x) = |x|$, \[
    f_{\mu}(x) = \begin{cases}
        \frac{x^2}{2 \mu}, \quad |x| \leq \mu \\
        |x| - \frac{\mu}{2}, \quad |x| > \mu.
    \end{cases}
\]
As in Nesterov smoothing, $f_{\mu}$ is $\nicefrac{1}{\mu}$-smooth. However, the advantage is that
it minimizes exactly, \ie, $\min_{\vec{x}} f(\vec{x}) = \min_{\vec{x}} f_{\mu}(\vec{x})$.

\subsection{Proximal point algorithm}

\begin{definition}[Proximal operator]
    The proximal operator of a convex function $f$ at $\vec{x}$ is defined as \[
        \mathrm{prox}_{\mu,f}(\vec{x}) \doteq \argmin_{\vec{y} \in \dom{f}} \lft\{ f(\vec{y}) + \frac{1}{2 \mu} \| \vec{x} - \vec{y} \|_2^2 \rgt\}.
    \]
\end{definition}
For many non-smooth functions, their proximal operator can be computed efficiently in a closed form.

The gradient of $f_{\mu}$ is \[
    \grad{f_{\mu}(\vec{x})}{} = \frac{1}{\mu} (\vec{x} - \mathrm{prox}_{\mu, f}(\vec{x})).
\]
Thus, applying gradient descent to $f_{\mu}$ --- which is $\nicefrac{1}{\mu}$-smooth --- reduces to \[
    \vec{x}_{t+1} = \vec{x}_t - \mu \grad{f_{\mu}(\vec{x}_t)}{} = \mathrm{prox}_{\mu, f}(\vec{x}_t),
\]
which is the proximal point algorithm (PPA). In general, we define a timestep-dependent stepsize
$\lambda_t$, \[
    \vec{x}_{t+1} = \mathrm{prox}_{\lambda_t, f}(\vec{x}_t).
\]

\begin{corollary}[PPA descent]
    \label{cor:ppa-descent}

    When applying PPA, we have \[
        f(\vec{x}_{t+1}) \leq f(\vec{x}_t).
    \]
\end{corollary}

\begin{theorem}[Convergence of PPA]
    If $f$ is convex, then for any $T \geq 0$, we have \[
        f(\vec{x}_T) - f(\vec{x}^\star) \leq \frac{\| \vec{x}_0 - \vec{x}^\star \|_2^2}{2 \sum_{t=0}^{T-1} \lambda_t}.
    \]
\end{theorem}

\begin{proof}
    Let $g_t(\vec{y}) \doteq f(\vec{y}) + \frac{1}{2 \lambda_t} \| \vec{x}_t - \vec{y} \|^2$, which is
    minimized by $\vec{x}_{t+1}$. This function has the following subdifferential, \[
        \partial g_t(\vec{y}) = \partial f(\vec{y}) + \frac{\vec{y} - \vec{x}_t}{\lambda_t}.
    \]
    By subgradient optimality, we have \[
        \vec{0} \in \partial g_t(\vec{x}_{t+1}) \iff - \frac{\vec{x}_{t+1} - \vec{x}_t}{\lambda_t} \in \partial f(\vec{x}_{t+1}).
    \]
    $f$ is convex, so the subgradient exists everywhere in its domain's interior. Hence, \[
        f(\vec{x}) \geq f(\vec{x}_{t+1}) - \frac{1}{\lambda_t} \langle \vec{x}_{t+1} - \vec{x}_t, \vec{x} - \vec{x}_{t+1} \rangle, \quad \forall \vec{x} \in \dom{f}.
    \]
    Rearranging yields
    \begin{align*}
        \lambda_t (f(\vec{x}_{t+1}) - f(\vec{x})) & \leq \langle \vec{x}_{t+1} - \vec{x}_t, \vec{x} - \vec{x}_{t+1} \rangle                                                                                 \\
                                                  & = \frac{1}{2} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x}_t \|^2 \rgt) \margintag{Cosine theorem.} \\
                                                  & \leq \frac{1}{2} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 \rgt).
    \end{align*}
    Summing over all timesteps,
    \begin{align*}
        \sum_{t=0}^{T-1} \lambda_t (f(\vec{x}_{t+1}) - f(\vec{x})) & \leq \frac{1}{2} \sum_{t=0}^{T-1} \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 \\
                                                                   & = \frac{1}{2} \lft( \| \vec{x}_0 - \vec{x} \|^2 - \| \vec{x}_T - \vec{x} \|^2 \rgt)             \\
                                                                   & \leq \frac{\| \vec{x}_0 - \vec{x} \|^2}{2}.
    \end{align*}
    Dividing both sides by $\sum_{t=0}^{T-1} \lambda_t$ and using \Cref{cor:ppa-descent}, we get
    \begin{align*}
        f(\vec{x}_T) - f(\vec{x}) & \leq \frac{\sum_{t=0}^{T-1} \lambda_t (f(\vec{x}_{t+1}) - f(\vec{x}))}{\sum_{t=0}^{T-1} \lambda_t} \\
                                  & \leq \frac{\| \vec{x}_0 - \vec{x} \|^2}{2 \sum_{t=0}^{T-1} \lambda_t}.
    \end{align*}
    We apply this to $\vec{x} = \vec{x}^\star$ to obtain the result.
\end{proof}

If we set $\lambda_t = \lambda$ to be constant, we get an $\bigo{\nicefrac{1}{t}}$ convergence
rate.

\subsection{Proximal gradient method}

Consider the following convex composite optimization problem, \[
    \min_{\vec{x} \in \R^d} F(\vec{x}) \doteq f(\vec{x}) + g(\vec{x}),
\]
where $f$ and $g$ are convex.\sidenote{Most supervised learning problems can be cast into this
    form, \[
        \min_{\vec{\theta}} \frac{1}{n} \sum_{i=1}^{n} \ell(h_{\vec{\theta}}(\vec{x}_i), y_i) + g(\vec{\theta}),
    \]
    where $h_{\vec{\theta}}$ is the predictor and $g$ is a regularization function.} The proximal
gradient method (PGM) has the following update rule, \[
    \vec{x}_{t+1} = \mathrm{prox}_{\gamma_t g}(\vec{x}_t - \gamma_t \grad{f(\vec{x}_t)}{}).
\]
Note that it alternates between a gradient update on $f$ and a proximal operator on $g$.

\begin{theorem}[Convergence of PGM]
    Let $F(\vec{x}) = f(\vec{x}) + g(\vec{x})$. Assume $f$ is convex and $L$-smooth, $g$ is convex
    and possibly non-smooth. Proximal gradient method with fixed stepsize $\gamma_t \doteq \nicefrac{1}{L}$
    satisfies \[
        F(\vec{x}_T) - F(\vec{x}^\star) \leq \frac{L \| \vec{x}_0 - \vec{x}^\star \|_2^2}{2T}.
    \]
\end{theorem}

\begin{proof}
    Let $h_t(\vec{y}) \doteq g(\vec{y}) + \frac{1}{2 \gamma_t} \| \vec{y} - (\vec{x}_t - \gamma_t
        \grad{f(\vec{x}_t)}{}) \|^2$, which is minimized by $\vec{x}_{t+1}$. This function has the
    following subdifferential, \[
        \partial h_t(\vec{y}) = \partial g(\vec{y}) + \frac{1}{\gamma_t} (\vec{y} - \vec{x}_t + \gamma_t \grad{f(\vec{x}_t)}{}).
    \]
    By subgradient optimality, we have \[
        \vec{0} \in \partial h_t(\vec{x}_{t+1}) \iff \frac{1}{\gamma_t} (\vec{x}_t - \vec{x}_{t+1} - \gamma_t \grad{f(\vec{x}_t)}{}) \in \partial g(\vec{x}_{t+1}).
    \]
    $g$ is convex, so the subgradient exists everywhere in its domain's interior. Hence, \[
        g(\vec{x}) \geq g(\vec{x}_{t+1}) + \lft\langle \frac{1}{\gamma_t} (\vec{x}_t - \vec{x}_{t+1}) - \grad{f(\vec{x}_t)}{}, \vec{x} - \vec{x}_{t+1} \rgt\rangle, \quad \forall \vec{x} \in \dom{g}.
    \]
    Rearranging yields
    \begin{align*}
        g(\vec{x}_{t+1}) - g(\vec{x}) & \leq \frac{1}{\gamma_t} \langle \vec{x}_t - \vec{x}_{t+1}, \vec{x}_{t+1} - \vec{x} \rangle - \langle \grad{f(\vec{x}_t)}{}, \vec{x}_{t+1} - \vec{x} \rangle                                     \\
                                      & = \frac{1}{2\gamma_t} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 - \| \vec{x}_t - \vec{x}_{t+1} \|^2 \rgt) \margintag{Cosine theorem.}                                 \\
                                      & \qquad - \langle \grad{f(\vec{x}_t)}{}, \vec{x}_{t+1} - \vec{x} \rangle                                                                                                                         \\
                                      & = \frac{1}{2\gamma_t} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 - \| \vec{x}_t - \vec{x}_{t+1} \|^2 \rgt)                                                             \\
                                      & \qquad - \langle \grad{f(\vec{x}_t)}{}, \vec{x}_{t+1} - \vec{x}_t \rangle - \langle \grad{f(\vec{x}_t)}{}, \vec{x}_t - \vec{x} \rangle                                                          \\
                                      & \leq \frac{L}{2} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 - \| \vec{x}_t - \vec{x}_{t+1} \|^2 \rgt) \margintag{Smoothness, convexity, and definition of $\gamma_t$.} \\
                                      & \qquad + f(\vec{x}_t) - f(\vec{x}_{t+1}) + \frac{L}{2} \| \vec{x}_t - \vec{x}_{t+1} \|^2                                                                                                        \\
                                      & \qquad + f(\vec{x}) - f(\vec{x}_t)                                                                                                                                                              \\
                                      & = \frac{L}{2} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 \rgt) + f(\vec{x}) - f(\vec{x}_{t+1}).
    \end{align*}
    Moving the calls to $f$ to the left side gives \[
        F(\vec{x}_{t+1}) - F(\vec{x}) \leq \frac{L}{2} \lft( \| \vec{x}_t - \vec{x} \|^2 - \| \vec{x}_{t+1} - \vec{x} \|^2 \rgt).
    \]
    Summing over all timesteps and using $F(\vec{x}_{t+1}) \leq F(\vec{x}_t)$, \[
        F(\vec{x}_T) - F(\vec{x}) \leq \frac{L \| \vec{x}_0 - \vec{x} \|^2}{2 T}.
    \]
    This concludes the proof.
\end{proof}

This is nearly the same convergence rate as gradient descent, despite $F$ being possibly
non-smooth.
