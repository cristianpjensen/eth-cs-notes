\section{Segmentation}

\textit{Segmentation} involves the grouping of pixels, where the groupings
indicate a semantic relationship. For example, segmenting pixels into
background and foreground. The purpose of segmentation is that the found
groupings can be used in further algorithms.

\subsection{$k$-means}

The most simple way of performing segmentation is using the $k$-means
algorithm. First, we encode all pixels into a feature space, after which we
apply $k$-means to the features. Then, if two pixels are part of the same
cluster, they are part of the same grouping in the segmentation.

Examples of features are RGB values or filter bank responses.\sidenote{Apply
24 common filters by convolution.} However, these do not give us spatial
coherence, because spatial information is not taken into account. If there is
a lot of noise, the groupings may not be very good. Thus, we want pixels that
are spatially close to have a greater chance of being in the same grouping.
The way to do this with $k$-means is to add two dimensions to the features,
$x$ and $y$ position. This enforces spatial coherence.

The advantage of $k$-means is that it is simple and fast. However, it has
many problems,
\begin{itemize}
  \item There is no way of knowing what the value of $k$ should be;
  \item $k$-means is sensitive to initial centers;
  \item $k$-means is sensitive to outliers;
  \item It only detects spherical clusters, because points are assigned to
    clusters by their squared distance.
\end{itemize}

\subsection{Mixture of Gaussians}

\textit{Mixture of Gaussians} (MoG) solves the problem of $k$-means'
sensitivity to outliers. Instead of treating the pixels as a bunch of points,
we will assume that they are all generated by sampling a continuous function.
Then, we will be able to remove outliers, because their likelihood will be very
low.

MoGs assume that the data is generated by $k$ weighted $d$-dimensional
Gaussians with means $\vec{\mu}_b\in\R^d$, covariance matrices
$\mat{\Sigma}_b\in\R^{d\times d}$, and weights $\alpha_b$. The likelihood of
observing $\vec{x}$ is the following, \[
  p(\vec{x}\mid\bm{\theta}) = \sum_{b=1}^k \alpha_b \mathcal{N}(\vec{x}; \vec{\mu}_b,\mat{\Sigma}_b)
.\]
We then want to optimize the model \wrt \[
  \bm{\theta} = \begin{bmatrix} \vec{\mu}_1 & \cdots & \vec{\mu}_k & \mat{\Sigma}_1 & \cdots & \mat{\Sigma}_k & \alpha_1 & \cdots & \alpha_k \end{bmatrix}
,\]
which is done with the \textit{expectation-maximization} (EM) algorithm. EM
works by estimating the ``ownership`` each Gaussian has over the points, given
the current estimate of $\bm{\theta}$ (E-step). Then, we update $\bm{\theta}$
given the ``ownership`` (M-step). This is done iteratively.

In more details, given the current estimate $\bm{\theta}$, the E-step
computes the probability that point $\bm{x}$ is in blob $b$, which we call
the ``ownership`` of $b$ over $\bm{x}$, \[
  p(b\mid \vec{x},\vec{\mu}_b,\mat{\Sigma}_b) = \frac{\alpha_b p(\vec{x}\mid \vec{\mu}_b,\mat{\Sigma}_b)}{\sum_{b'=1}^k \alpha_{b'}p(\vec{x}\mid\vec{\mu}_{b'},\mat{\Sigma}_{b'})}
.\]

Given the ``ownership`` information, the M-step computes the new means,
covariance matrices, and weights,
\begin{align*}
  \alpha_b' &= \frac{1}{n} \sum_{i=1}^n p(b\mid \vec{x}_i,\vec{\mu}_b,\mat{\Sigma}_b) \\
  w^{(b)}_i &= \frac{p(b\mid \vec{x}_i,\vec{\mu}_b,\mat{\Sigma}_b)}{\sum_{j=1}^n p(b\mid \vec{x}_j,\vec{\mu}_b,\mat{\Sigma}_b)} \\
  \vec{\mu}_b' &= \sum_{i=1}^n w^{(b)}_i\vec{x}_i & \text{\footnotesize weighted mean} \\
  \mat{\Sigma}_b' &= \sum_{i=1}^n w^{(b)}_i (\vec{x}_i - \vec{\mu}_b) (\vec{x}_i - \vec{\mu}_b)^\top & \text{\footnotesize weighted variance}
.\end{align*}

Then, after running the EM algorithm, we assign each pixel to the blob with
the highest ``ownership`` over the pixel.

The advantage of this approach is that it is probabilistic and can predict new
data points, because it is generative. This also has the advantage that we can
detect outliers. Furthermore, it can be stored in $\bigo{kd^2}$, which is quite
compact. The problems are that we still need to know the number of components
$k$, and we need a good initializiation.\sidenote{It is often a good idea to
start from the output of $k$-means.}

\subsection{Mean-shift}

\textit{Mean-shift} is an algorithm that does not require a predetermined
amount of clusters, like $k$-means and MoG. Instead, it localizes the histogram
modes of the data. Then, all identified modes are considered a cluster. It
works by starting from random points and iteratively moving toward the center
of mass of the window around the point. This will cause the algorithm to move
toward high density areas. Ultimately, all points that move toward the same
area, \ie same mode, are considered a cluster.

The iterative update rule is the following, \[
  f(\vec{x}) = \frac{1}{\sum_{i=1}^n k(\vec{x},\vec{x}_i)} \sum_{i=1}^n \vec{x} k(\vec{x},\vec{x}_i)
,\]
where $\vec{x}$ is the current point, $k$ is a distance function, and
$f(\vec{x})$ is the new point. Intuitively, it is a weighted average of its
distance to all other points, which makes it move toward high density areas.
The attraction basin is the region for which all trajectories lead to the same
mode, and all data points in the same attraction basin of a mode are considered
a cluster.

The advantage of this method is that it does not require a predetermined
amount of clusters. Another advantage is that it does not assume any prior
shape on data clusters. Furthermore, it has just a single parameter $h$,
which is the window size, so it has a physical meaning. However, the
selection of $h$ is not trivial, and the method is computationally quite
expensive.

\begin{marginfigure}
    \centering
    \incfig{mean-shift}
    \caption{Iteration of mean-shift where points move toward a mode.}
    \label{fig:mean-shift}
\end{marginfigure}

\subsection{Hough transform}

\begin{remark}
This is not an algorithm for segmentation, but rather for finding features.
\end{remark}

The \textit{Hough transform} uses the structure of shapes to recognize them in
an image. First, we have to find the edges of the objects in the image, so we
can fit shapes to them. Now, let's say that we want to detect straight lines
for example.\sidenote{These are important, because of their omnipresence in
man-made scenes.} We use the following equation for a line,\sidenote{We use
this instead of $y=ax+b$, because this has a bounded parameter domain.} \[
  x\cos\theta + y\sin\theta = \rho
.\]
Then, we represent all lines through all the points on the edges in a
parameter space. \Eg, the point $[0,1]$ lies on all lines with $\rho=1$, thus
all lines in this point are repsented by the line $\rho=1$ in parameter
space. Then, we need to identify peaks in the parameter space. These peaks
make up the geometry of the objects in the image.

The advantage of this approach is that it can be done for any shape, such as
a circle, but the problem is that as the parameters grow, it gets
exponentially more expensive.

\subsection{Interactive segmentation}

In \textit{interactive segmentation}, we assume that we also have information
from the user about the segmentation. \Ie, the user has identified some pixels
as being part of some grouping. We can use this information to provide a better
segmentation.

\begin{marginfigure}
    \centering
    \incfig{markov-random-field}
    \caption{Markov random field.}
    \label{fig:mrf}
\end{marginfigure}

In this approach, we will assume that the pixels of the image form a
\textit{Markov random field} (MRF). We assume that there is a defined
relationship $\psi(\bm{x}_i,\bm{x}_j)$ between adjacent pixels and a defined
relationship $\phi(\bm{x}_i,y_i)$ between pixels and their hidden state
(label). The joint probability of the field is then defined as the following,
\[
  p(\mat{X},\vec{y}) = \prod_i \phi(\vec{x}_i,y_i) \prod_{i,j} \psi(\vec{x}_i,\vec{x}_j)
.\]
We then want to minimize the negative log-likelihood, \[
  E(\mat{X},\vec{y}) = -\sum_i \phi(\vec{x}_i,y_i) -\sum_{i,j} \psi(\vec{x}_i,\vec{x}_j)
.\]
We call this the energy function. We can then use an inference algorithm such
as Gibbs sampling to minimize the energy function. However, the most popular
one is GraphCut.

The idea of \textit{GraphCut} is to turn the MRF into a source-sink graph,
where the edge weights are defined by $\phi$ and $\psi$. Let's say we want to
segment the image into foreground/background\sidenote{If there are more than 2
possible labels, there will not be a unique global solution.} and we have input
from the user about some pixels that are background and some that are
foreground. We now want to know how to assign the surrounding pixels. First, we
convert all the known background pixels into sources and foreground pixels into
sinks in the source-sink graph. Then, we find the minimum cost cut that
separates the background from the foreground and label the pixels
appropriately. This enforces spatial coherence.

We can also minimize the energy function further by iteratively alternating
between using GraphCut and fitting an MoG. Starting from the user-provided
bound, each iteration, we get a new bound from GraphCut, which is used to
refine the MoG, which is used to refine the bounds with GraphCut, etc.

\subsection{Learning-based approaches}

In fully supervised learning, we assume we have access to a training dataset
of images with full segmentation mask as labels. Thus, whatever type of
segmentation that will be done by a model will depend on the training data.
If the training data consists of background/foreground segmentation, the
model will do background/foreground segmentation.

\begin{marginfigure}
    \centering
    \incfig{intersection-over-union-metric}
    \caption{Illustration of the intersection over union metric.}
    \label{fig:intersection-over-union-metric}
\end{marginfigure}

Models are evaluated with the intersection over union (IoU) metric (see
\Cref{fig:intersection-over-union-metric}), \[
  \frac{1}{C} \sum_{i=1}^C \frac{n_{ii}}{n_i + \sum_{j=1}^C n_{ji} - n_{ii}}
,\]
where $C$ is the amount of labels, $n_i$ is the amount of pixels of label
$i$, and $n_{ij}$ is the amount of pixels of class $i$ predicted to have
class $j$.

Like in unsupervised approaches, we first need to extract features of the
pixels, which can be learned. These features are then given as input to the
models, which are optimized to predict the training labels.

\paragraph{$k$-nearest neighbors.}

The simplest learning-based approach is $k$-nearest neighbors (kNN). In kNN,
every point is labeled to be the mode of its $k$ nearest neighbors in the
training data. The ``nearest`` neighbors refer to the points in the feature
space with the smallest distance to the current feature vector, not the
nearest point in the image.

\paragraph{Transfer learning.}

Transfer learning is a method that does not require a large
dataset.\sidenote{Which is good, because, in segmentation, training data is
very expensive.} In transfer learning, the weights are first learned on a
related task, such as classification, and dataset (pretraining). Then, we
initialize our segmentation model with these weights as much as possible and
train on the actual dataset (finetuning). This makes the assumption that
pretrained weights are easier to train for segmentation than a random
initalization.

\paragraph{Hypercolumns.}

It is common to pretrain on a classification task (one label per image) and
then augmenting the model for segmentation (one label per pixel). Hypercolumns
do this by taking the pixel activations for each pixel at each layer of the
classification network, concatenating them, and calling that the pixel's
embedding (see \Cref{fig:hypercolumns}).\sidenote{For convolutional layers,
this means taking the output of the convolution at that pixel, and, for
downsampling layers, multiple pixels get mapped to the same value.} This
embedding is then used as input to a pixel classification network that actually
predicts the grouping of the pixel.

This implicitly enforces spatial coherence, because convolutional layers work
locally. Thus, close activations are closely related. The results of
hypercolumns are pretty good, but the boundaries are very coarse, due to the
downsampling layers of the classification model, which cause many features to
be the same value for close pixels.

\begin{marginfigure}
    \centering
    \incfig{hypercolumns}
    \caption{Hypercolumns.}
    \label{fig:hypercolumns}
\end{marginfigure}

\paragraph{Fully convolutional networks.}

In fully convolutional networks (FCN), we change the usually used linear layers
of the classification network to $1\times 1$ convolutional layers. This allows
for pretraining on a classification task, because linear layers and $1\times 1$
convolutional layers have the same parameters. Then, at the end of the model,
the images are not turned into vectors, but rather feature maps. Then dependent
on how much the images were downsampled by the model, we upsample the feature
map from different layers of the nerwork to be of same size as the input image
with a transposed convolution. Now, we have features per pixel that can be used
to predict the label per pixel with $1\times 1$ convolution.

\paragraph{Refinement with conditional random fields.}

The outputs of FCNs are not sharp. But, we can use an additional refinement
step with a conditional random field (generalization of Markov random field).
We can use the FCN output as the unary term $\phi$ and an edge-aware pairwise
term $\psi$ to refine the segmentation.

\paragraph{U-nets.}

This model is similar to FCNs, but rather than doing the upsampling in a single
transposed convolutional layer, we use the same amount of upsampling layers as
downsamping layers. Additionally, U-nets also use skip-connections between
downsampling and upsampling layers of the same level.

\paragraph{Dilated convolutions.}

\begin{marginfigure}
    \centering
    \incfig{dilated-convolution}
    \caption{Dilated convolution}
    \label{fig:dilated-convolution}
\end{marginfigure}

In computer vision, we need convolutional layers to process an input image for
a task, and downsampling layers to make the receptive field large. However,
because the convolutional layers work very locally (usually $3\times 3$ or
$5\times 5$), we miss out on contextual information, and because the
downsampling layers remove data, we lose information. We want a layer that does
not downsample, but still has a large receptive field. The dilated convolution
is a generalization of the convolutional layer that skips values for an
exponentially large receptive field without a change in the amount of weights
(see \Cref{fig:dilated-convolution}). Thus, we can still pretrain a
classification network and reuse the weights in the dilated layers. Another
advantage of not needing downsampling layers is that we get an output for every
pixel without needing to upsample, which is what we want in segmentation.

However, simply only using dilated layers causes artifacts, because the learned
filters are often discontinuous. To mitigate this, we can add additional
convolutional layers to the beginning and end of the model to smooth out the
artifacts. This results in much better segmentations.
