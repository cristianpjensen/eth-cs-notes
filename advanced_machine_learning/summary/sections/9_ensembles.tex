\section{Ensembles}

\margintag{Model averaging is very common in machine learning and data science. For example, the Bayesian approach for inference weights parameters according to their posterior, \[
        \int p(\vec{\theta} \mid \mat{X}, \vec{y}) p(\vec{x}^\star \mid \vec{\theta}) \mathrm{d}\vec{\theta}.
    \]}

The idea of ensembling is simple: create a highly accurate estimator by combining many relatively
weak and inaccurate estimators. The advantage over large models is that weak models are easy to
train---they only need to be better than random. In general, we have a set of $B$ estimators, \[
    \hat{f}_1, \hat{f}_2, \ldots, \hat{f}_B.
\]
We could take the final estimator to be a convex combination of the estimators, \[
    \hat{f}(\vec{x}) = \sum_{b=1}^{B} p_b \hat{f}_b(\vec{x}), \quad \sum_{b=1}^{B} p_b = 1, p_b \geq 0.
\]
The bias of this estimator is the same convex combination of the biases of the set of estimators,
\begin{align*}
    \mathrm{bias}(\hat{f}) & \doteq \E [ \hat{f}(X) ] - \E[Y \mid X]                             \\
                           & = \sum_{b=1}^{B} p_b \E [ \hat{f}_b(X) ] - \E[Y \mid X]             \\
                           & = \sum_{b=1}^{B} p_b \lft( \E [ \hat{f}_b(X) ] - \E[Y \mid X] \rgt) \\
                           & = \sum_{b=1}^{B} p_b \mathrm{bias}(\hat{f}_b).
\end{align*}
Thus, unbiased estimators remain unbiased after averaging. We can also look at the variance of this estimator,
\begin{align*}
    \Var[\hat{f}] & \doteq \E \lft[ \lft( \hat{f}(X) - \E [\hat{f}(X)] \rgt)^2 \rgt]                                                                                                                     \\
                  & = \E \lft[ \lft( \sum_{b=1}^{B} p_b \lft( \hat{f}_i(X) - \E [\hat{f}_i(X)] \rgt) \rgt)^2 \rgt]                                                                                       \\
                  & = \frac{1}{B^2} \sum_{b=1}^{B} \Var [\hat{f}_b] + \frac{1}{B^2} \sum_{b=1}^{B} \sum_{b' \neq b}^B \mathrm{Cov}[\hat{f}_b, \hat{f}_{b'}]. \margintag{Assume $p_b = \nicefrac{1}{B}$.}
\end{align*}
Often, the covariances are small ($\mathrm{Cov}[\hat{f}_b, \hat{f}_{b'}] \approx 0$) and the
variances similar ($\Var[\hat{f}_b] \approx \sigma^2$), so we gain from averaging because the variance is reduced, \[
    \Var[\hat{f}] \approx \frac{\sigma^2}{B}.
\]

We will consider techniques for ensembling classifiers, $c_1, c_2, \ldots, c_B$ with weights
$\alpha_1, \alpha_2 \ldots, \alpha_B$. These estimators are composed as follows, \[
    \hat{c}(\vec{x}) = \mathrm{sgn} \lft( \sum_{b=1}^{B} \alpha_b c_b(\vec{x}) \rgt).
\]
It is important to note that this procedure can only succeed if the classifiers are diverse.
Otherwise, their covariances are large and we do not gain anything.

\subsection{Bagging}

Bagging generates diversity in the classifier pool by training on different bootstrapped
subsets---see \Cref{alg:bootstrap}. Bootstrapped subsets are generated by sampling \iid samples
from the dataset with replacement.

This works well, because the covariances are small due to using different subsets for training.
Furthermore, the variances are similar, because each subsample behaves similarly on average.
Finally, the bias is not affected, since the final bias is an average of its estimators' biases.

\begin{algorithm}[t]
    \begin{algorithmic}[1]
        \For{$b \in [B]$}
        \State $\mathcal{Z}^{*b} = \mathrm{BootstrapSample}(\mathcal{Z})$
        \State $\hat{c}_b = \mathrm{TrainClassifier}(\mathcal{Z}^{*b})$
        \EndFor
        \State \Return $\hat{c}(\vec{x}) = \mathrm{sgn}\lft( \sum_{b=1}^{B} \hat{c}_b(\vec{x}) \rgt)$
    \end{algorithmic}
    \label{alg:bootstrap}
    \caption{Bagging with bootstrapped subsets.}
\end{algorithm}

\subsection{Random forests}

A random forest is a collection of decision trees, which are used for classification. The idea of
the random forest algorithm is to grow a sufficiently deep decision tree to reduce bias, which
gives a noisy classifier with high variance---see \Cref{alg:random-forest}. The decision trees are
grown by repeatedly splitting nodes based on feature values. An example splitting criterium is
splitting to reduce entropy. We can then average to enhance the robustness, since the variance is
reduced. Classification is finally done by a majority vote of all decision trees.

\begin{algorithm}[t]
    \begin{algorithmic}[1]
        \For{$b \in [B]$}
        \State $\mathcal{Z}^{*b} = \mathrm{BootstrapSample}(\mathcal{Z})$
        \Repeat
        \State Select $m \approx \sqrt{p}$ features at random
        \State Pick the best feature to split
        \State Split the node into two daughter nodes
        \Until{node size $n_{\min}$ is reached}
        \EndFor
        \State \Return Set of decision trees $\{ \hat{c}_b \}_{b=1}^B$
    \end{algorithmic}
    \label{alg:random-forest}
    \caption{Random forest algorithm over a dataset $\mathcal{Z}$, where the feature space is $p$-dimensional.}
\end{algorithm}

\subsection{AdaBoost}

AdaBoost \citep{freund1995desicion} does not use bootstrapped samples, but re-weights the data at
each iteration according to the error of previous weak learners. As a result, the weak estimators
should be diverse and hence reduce variance. The diversity is due to later classifiers being
trained to classify points that previous classifiers found hard with more weight. The weighting of
the data is defined recursively,
\begin{align*}
    w_i^{(b+1)} & = w_i^{(b)} \exp \lft( \alpha_b \mathbb{1} \{ c_b(\vec{x}_i) \neq y_i \} \rgt), \quad w_i^{(1)} = \frac{1}{n}                                                                                  \\
    \alpha_b    & \doteq \log \lft( \frac{1-\epsilon_b}{\epsilon_b} \rgt) \margintag{Log-odds of no error \vs error.}                                                                                            \\
    \epsilon_b  & \doteq \sum_{i=1}^{n} \frac{w_i^{(b)}}{\sum_{j=1}^{n} w_j^{(b)}} \mathbb{1} \{ c_b(\vec{x}_i) \neq y_i \}. \margintag{Probability of an error, where the normalized weights serve as a prior.}
\end{align*}
We then iteratively fit $B$ classifiers on the full training data using the recursive weights placed on the data points. At inference time, the classifiers are weighted according to their log-odds of error, \[
    \hat{c}(\vec{x}) = \mathrm{sgn} \lft( \sum_{b=1}^{B} \alpha_b c_b(\vec{x}) \rgt).
\]
Note that AdaBoost is a deterministic algorithm.

\begin{marginfigure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[width=\textwidth, xlabel={$\hat{f}(\vec{x})$}, domain=-4:4, xmin=-4, xmax=4, ymax=20, ymin=0]
            \addplot[blue, no markers, samples=50, semithick] {exp(-x)};
            \addplot[red, no markers, samples=50, semithick] {exp(x)};
            \legend{$y=1$, $y=-1$};
        \end{axis}
    \end{tikzpicture}
    \caption{Exponential loss function.}
\end{marginfigure}

\citet{friedman2000additive} showed that the minimizer of the exponential loss function\sidenote{The
    exponential loss function is $\exp(-y \hat{f}(\vec{x}))$.} \wrt the true distribution is the
log-odds of the class probabilities. As such, AdaBoost effectively fits an additive model in base
learners, optimizing the exponential function.

\begin{lemma} \label{lem:adaboost1}
    The minimizer of $\E[\exp(-y f(\vec{x}))]$ is \[
        f^\star(\vec{x}) = \frac{1}{2} \log \frac{\mathbb{P}(y = 1 \mid \vec{x})}{\mathbb{P}(y = -1 \mid \vec{x})}.
    \]
    Thus, we have the following posteriors,
    \begin{align*}
        \mathbb{P}(y=1 \mid \vec{x})  & = \frac{\exp(f^\star(\vec{x}))}{\exp(-f^\star(\vec{x})) + \exp(f^\star(\vec{x}))}  \\
        \mathbb{P}(y=-1 \mid \vec{x}) & = \frac{\exp(-f^\star(\vec{x}))}{\exp(-f^\star(\vec{x})) + \exp(f^\star(\vec{x}))}
    \end{align*}
\end{lemma}

\begin{proof}
    The expected exponential loss function is the following, \[
        \E[\exp(-y f(\vec{x})) \mid \vec{x}] = \mathbb{P}(y=1 \mid \vec{x}) \exp(-f(\vec{x})) + \mathbb{P}(y=-1 \mid \vec{x}) \exp(f(\vec{x}))                   \\
    \]
    The exponential loss is convex, so the minimizer has vanishing gradient, \[
        \pdv{\E[\exp(-y f(\vec{x})) \mid \vec{x}]}{f(\vec{x})} = -\mathbb{P}(y=1 \mid \vec{x}) \exp(-f(\vec{x})) + \mathbb{P}(y=-1 \mid \vec{x}) \exp(f(\vec{x}))
    \]
    Hence, the minimizer $f^\star$ can be derived to be the following (by setting the gradient to
    zero),
    \begin{align*}
         &      & \mathbb{P}(y=1 \mid \vec{x}) \exp(-f^\star(\vec{x})) & = \mathbb{P}(y=-1 \mid \vec{x}) \exp(f^\star(\vec{x}))                                 \\
         & \iff & \exp(2 \cdot f^\star(\vec{x}))                       & = \frac{\mathbb{P}(y=1 \mid \vec{x})}{\mathbb{P}(y=-1 \mid \vec{x})}                   \\
         & \iff & f^\star(\vec{x})                                     & = \frac{1}{2} \log \frac{\mathbb{P}(y=1 \mid \vec{x})}{\mathbb{P}(y=-1 \mid \vec{x})}.
    \end{align*}
    This concludes the proof.
\end{proof}

\begin{theorem}
    The discrete AdaBoost algorithm builds an additive logistic regression model via Newton-like
    updates for minimizing the exponential loss function, $\E[\exp(-y f(\vec{x}))]$.
\end{theorem}

\begin{proof}
    Let $J(f) = \E[\exp(-y f(\vec{x}))]$. Suppose that $f(\vec{x})$ is the current estimate and
    $f(\vec{x}) + \alpha c(\vec{x})$ is an improvement with $c: \mathcal{X} \to \{ -1, +1 \}$ deciding
    the direction for a small step $\alpha$. Then, for a fixed $\alpha$ and $\vec{x}$, the second-order
    Taylor expansion around $c(\vec{x}) = 0$ yields
    \begin{align*}
        J(f + \alpha c) & = \E [\exp(-y (f(\vec{x}) + \alpha c(\vec{x})))]                                                                                                                                           \\
                        & \approx \E \lft[ \exp(-y f(\vec{x})) \lft(1 - y \alpha c(\vec{x}) + \frac{1}{2} \alpha^2 y^2 c(\vec{x})^2 \rgt) \rgt] \margintag{Second-order Taylor approximation around $c(\vec{x})=0$.} \\
                        & = \E \lft[ \exp(-y f(\vec{x})) \lft( 1 - y \alpha c(\vec{x}) + \frac{\alpha^2}{2} \rgt) \rgt]. \margintag{$y^2 = 1$ and $c(\vec{x})^2 = 1$.}
    \end{align*}
    Minimizing pointwise \wrt $c(\vec{x}) \in \{ -1, 1 \}$, we write
    \begin{align*}
        c(\vec{x}) & = \argmin_{c} \E_w \lft[ 1 - y \alpha c(\vec{x}) + \frac{\alpha^2}{2} \;\middle|\; \vec{x} \rgt] \\
                   & = \argmax_{c} \E_w [y c(\vec{x})].
    \end{align*}
    where $\E_w[\cdot \mid \vec{x}]$ is a weighted conditional expectation, where $w(\vec{x}, y) =
        \exp(-y f(\vec{x}))$, and \[
        \E_w[g(\vec{x}, y) \mid \vec{x}] \doteq \frac{\E[w(\vec{x}, y) g(\vec{x}, y) \mid \vec{x}]}{\E[w(\vec{x}, y)]}.
    \]
    The solution is \[
        c^\star(\vec{x}) = \begin{cases}
            1  & \E_w[y \mid \vec{x}] = \mathbb{P}_w(y=1 \mid \vec{x}) - \mathbb{P}_w(y = -1 \mid \vec{x}) > 0 \\
            -1 & \text{otherwise}.
        \end{cases}
    \]
    We can directly minimize $J(f + \alpha c)$ without making a Taylor approximation to determine the
    optimal $\alpha^\star$,
    \begin{align*}
        \alpha^\star & = \argmin_{\alpha} \E_w \lft[ -y \alpha c(\vec{x}) \rgt]                                                                                                 \\
                     & = \frac{1}{2} \log \lft( \frac{\mathbb{P}_w(y = 1 \mid \vec{x})}{\mathbb{P}_w(y=-1 \mid \vec{x})} \rgt) \margintag{Application of \Cref{lem:adaboost1}.} \\
                     & = \frac{1}{2} \log \lft( \frac{1 - \epsilon_w}{\epsilon_w} \rgt),
    \end{align*}
    where $\epsilon_w = \E_w[\mathbb{1} \{ y \neq f(\vec{x}) \}]$.

    Combining these steps yields the AdaBoost update,
    \begin{align*}
        f(\vec{x})    & \gets f(\vec{x}) + \alpha^\star c(\vec{x})                                                                                            \\
        w(\vec{x}, y) & \gets w(\vec{x}, y) \exp(-y \alpha^\star c(\vec{x})) = w(\vec{x}, y) \exp \lft( \alpha^\star \mathbb{1}\{ c(\vec{x}) \neq y \} \rgt).
    \end{align*}
    This concludes the proof that the AdaBoost algorithm minimizes the exponential loss function by
    iteratively adding weighted models via second-order optimization.
\end{proof}
