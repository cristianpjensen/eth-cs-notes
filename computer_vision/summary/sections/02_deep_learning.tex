\section{Deep learning}

In this course, we will see many computer vision tasks that pre-date the deep
learning era. However, these tasks have seen much improvement by applying
machine learning to them. Thus, we will first be introduced to the tasks,
followed by how the problem was dealt with initially, and then how machine
learning is applied to it.

\subsection{Convolutional neural networks}

TODO

\subsection{Sequence models}

\marginnote{An application of RNNs in computer vision is processing video,
    where each frame is processed by a CNN. Then, each frame's vector
    representation is used to update the hidden state of the RNN to obtain a
    representation of the video.}

A \textit{recurrent neural network} (RNN) is a neural network that maps from an
input space of sequences to an output space of sequences in a stateful way
\citep{rumelhart1985learning}. That is, the prediction of the output
$\vec{y}_t$ depends on all inputs $\vec{x}$ that came before $t$. This is done
by keeping track of a hidden state $\vec{h}_t$, which is a function of all
previous inputs and the current input, \[
    \vec{h}_t = \sigma(\mat{W}_h\vec{h}_{t-1}+\mat{W}_x\vec{x}_t).
\]

\begin{figure}
    \centering
    \incfig{rnn}
    \caption{Computation graph of an RNN.}
    \label{fig:rnn}
\end{figure}

Putting everything together, backpropagation computes the following gradient
\wrt $\mat{W}_h$,
\begin{align*}
    \pdv{\ell}{\mat{W}_h} & = \sum_{j=0}^T \pdv{\ell_j}{\mat{W}_h}                                                                                                                    \\
                          & = \sum_{j=0}^T \sum_{k=1}^j \pdv{\ell_j}{\vec{h}_k} \pdv{\vec{h}_k}{\mat{W}_h}                                                                            \\
                          & = \sum_{j=0}^T \sum_{k=1}^j \pdv{\ell_j}{y_j} \pdv{y_j}{\vec{h}_j} \pdv{\vec{h}_j}{\vec{h}_k} \pdv{\vec{h}_k}{\mat{W}_h}                                  \\
                          & = \sum_{j=0}^T \sum_{k=1}^j \pdv{\ell_j}{y_j} \pdv{y_j}{\vec{h}_j} \lft( \prod_{m=k+1}^j \pdv{\vec{h}_m}{\vec{h}_{m-1}} \rgt) \pdv{\vec{h}_k}{\mat{W}_h},
\end{align*}
where $T$ is the amount of timesteps in the sequence. If we use the following
approximation $\pdv{\vec{h}_m}{\vec{h}_{m-1}} \approx \mat{W}_h$, then we see
that there are a large amount of matrix multiplications of $\mat{W}_h$ with
itself. This will lead to either vanishing or exploding gradients, depending on
whether $\det{\mat{W}_h}$ is greater or less than $1$. Possible solutions for
this are gradient clipping and gated recurrent units
\citep{hochreiter1997long,cho2014learning}.

\subsection{Transformers}

\begin{marginfigure}
    \centering
    \incfig{attention}
    \caption{Self-attention mechanism.}
    \label{fig:attention}
\end{marginfigure}

\textit{Attention} is a mechanism in neural networks that a model can learn to
make predictions by selectively attending to a given set of data by using query
$\bm{q}$, key $\bm{k}$, and value $\bm{v}$ vector representations. The query
and key vectors are used to determine how much weight should be given to the
value vector.\sidenote{Note the parallel with dictionaries/hashmaps in
    programming languages, but, in the attention mechanism, we do a
    ``soft-lookup``.} The weights are computed by
$\mat{A}_i=\mathrm{softmax}(\transpose{\vec{q}_i} \vec{k}_i)$, so the values
after the attention block can be computed as follows, \[
    \mathrm{att}(\vec{x}_i) = \sum_{j} \mat{A}_{ij} \vec{v}_i.
\]

\textit{Self-attention} blocks learn the query, key, and value representations
from data. More specifically, it learns matrices $\mat{W}_Q$, $\mat{W}_K$, and
$\mat{W}_V$ and computes the vectors from these matrices,
\begin{align*}
    \mat{Q} & = \transpose{\mat{W}_Q} \mat{X}  \\
    \mat{K} & = \transpose{\mat{W}_K} \mat{X}  \\
    \mat{V} & = \transpose{\mat{W}_V} \mat{X}.
\end{align*}
Then, we can use these to compute the output of the self-attention block, \[
    \mathrm{selfatt}(\mat{X}) = \mathrm{softmax}\lft( \frac{\transpose{\mat{Q}} \mat{K}}{\sqrt{d_q}} \rgt) \mat{V},
\]
where $d_q$ is the dimensionality of the query and key vectors. Furthermore, we
need to add a positional encoding to provide ordering information to the model,
because the self-attention operation is permutation equivariant. This is done
by a sinusoidal positional encoding and are simply combined with $\mat{X}$ by
addition.

\begin{figure}[ht]
    \centering
    \incfig{transformer}
    \caption{Transformer encoder architecture.}
    \label{fig:transformer}
\end{figure}

Transformers use multi-headed self-attention, which is a module where
self-attention is applied $M$ times independently to the data. Thus, this
module learns $M$ different ways of looking at the same dataset. The outputs
of each self-attention block is concatenated and linearly transformed to the
expected dimensionality. Transformers follow this by normalization and MLP
layers, as can be seen in \Cref{fig:transformer}.

\subsection{Latent variable models}

Latent variable models map between an observation space $\vec{x}\in\R^D$ and
latent space $\vec{z}\in\R^Q$ with $Q<D$. The models then learn two functions:
encoder $f_{\vec{\theta}}: \R^D \to \R^Q$ and decoder $g_{\vec{\theta}}: \R^Q
    \to \R^D$. These models are called \textit{autoencoders}, because they predict
their input as output.

Generative latent variable models are Bayesian probabilistic functions, \[
    p(\vec{x}) = \int_{\vec{z}} p(\vec{x}\mid\vec{z}) p(\vec{z}) d\vec{z}.
\]
So, the probability of a sample is the ``weighted average`` of this sample
given a latent variable weighted by the probability of this latent variable.
Generally, the goal is to be able to sample from this probability distribution.

\textit{Variational autoencoders} (VAE) sample $\vec{z}$ from a simple
distributions such as the Gaussian and learn decoders that map from this
distribution to data points $\vec{x}$ \citep{kingma2013auto}. The aim of
training is to maximize the probability of each $\vec{x}$ in the training set,
\ie, maximize the likelihood of seeing our data. Since the likelihood (also
known as evidence) is an intractable integral, we need to derive a tractable
lower bound,
\begin{align*}
    \log p(\vec{x}) & = \log \int p(\vec{x}\mid\vec{z}) p(\vec{z}) d\vec{z}                                                                            \\
                    & = \log \int p(\vec{x}\mid\vec{z}) \frac{p(\vec{z})}{q(\vec{z})}q(\vec{z}) d\vec{z}                                               \\
                    & \geq \int q(\vec{z})\log \lft( p(\vec{x}\mid\vec{z})\frac{p(\vec{z})}{q(\vec{z})} \rgt) d\vec{z} \margintag{Jensen's inequality} \\
                    & = \E_q \lft[ \log \frac{p(\vec{x}\mid\vec{z})p(\vec{z})}{q(\vec{z})} \rgt],
\end{align*}
which is called the \textit{evidence lower bound} (ELBO). The probability
distribution $q_{\vec{\theta}}$ is the encoder and can be defined to be
anything, \eg, a Gaussian distribution $\mathcal{N}(\vec{0},\mat{I})$ or a
probabilistic deep learning model $q_{\vec{\theta}}$ dependent on $\vec{x}$.

An objective function can be derived from the ELBO, which maximizes the lower
bound of the likelihood,
\begin{align*}
    \E_q \lft[ \log \frac{p(\vec{x}\mid\vec{z})p(\vec{z})}{q(\vec{z})} \rgt] & = \E_q \lft[ \log p(\vec{x}\mid\vec{z}) \rgt] - \E_q \lft[ -\log \frac{p(\vec{z})}{q(\vec{z})} \rgt] \\
                                                                             & = \E_q \lft[ \log p(\vec{x}\mid\vec{z}) \rgt] - D_{\mathrm{KL}} (q \mathrel{\Vert} p).
\end{align*}
The negative log-likelihood is thus upper-bounded by \[
    D_{\mathrm{KL}}(q \mathrel{\Vert} p) + \E_q [-\log p(\vec{x}\mid\vec{z})].
\]
VAEs minimize this bound, \[
    \hat{\vec{\theta}} = \argmin_{\vec{\theta}\in\Theta} \sum_{\vec{x}\in\mathcal{D}} \underbrace{D_{\mathrm{KL}}(q_{\vec{\theta}}(\vec{z}\mid\vec{x})\mathrel{\Vert}p(\vec{z}))}_{\text{encoder loss}} + \underbrace{\E_{q_{\vec{\theta}}(\vec{z}\mid\vec{x})} [-\log p_{\vec{\theta}}(\vec{x}\mid\vec{z})]}_{\text{decoder loss}},
\]
where we now learn a latent representation $\vec{z}$ given the datapoint
$\vec{x}$.

\begin{figure}[ht]
    \centering
    \incfig{variational-autoencoder}
    \caption{Variational autoencoder architecture.}
    \label{fig:variational-autoencoder}
\end{figure}

Because of problems with the gradient, the encoder does not directly predict
$\vec{z}$ from $\vec{x}$, but rather predicts a Gaussian distribution
$\mathcal{N}(\vec{\mu},\mat{\Sigma})$ from $\vec{x}$, which is used to sample
$\vec{z}$. This splits the model into an encoder and a decoder network that are
updated separately, as can be seen in \Cref{fig:variational-autoencoder}.

\textit{Generative adversarial networks} (GAN) do not explicitly model the
likelihood. Instead, they use an adversarial process in which two models are
trained simultaneously: a generator $G: \R^Q\to\R^D$ and a discriminator
$D:\R^D \to [0,1]$ \citep{goodfellow2014generative}. The generator captures the
data distribution, while the discriminator estimates whether a sample comes
from the data distribution or the generator. At a high level, we train $D$ to
assign probability 1 to samples from the training data and 0 to samples from
the generator, while training $G$ to fool $D$ such that it assigns probability
1 to its samples, \[
    \argmin_{\vec{\theta}_G}\argmax_{\vec{\theta}_D} \E_{p_{\text{data}}(\bm{x})}[\log D_{\vec{\theta}_D}(\vec{x})] + \E_{p(\vec{z})} \lft[ \log (1-D_{\vec{\theta}_D}(G_{\vec{\theta}_G}(\vec{z}))) \rgt].
\]
The loss function of $G$ solely depends on $D$, which is learned. So, GANs
are a way of learning the loss function.

\begin{figure}[ht]
    \centering
    \incfig{gan}
    \caption{Generative adversarial network architecture. The discriminator must
        predict 0 for data points generated by the generator, and 1 for data points
        from the dataset. The generator wants the discriminator to predict 1 for its
        generations.}
    \label{fig:gan}
\end{figure}
