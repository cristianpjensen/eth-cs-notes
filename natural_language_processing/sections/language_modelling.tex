\newcommand{\const}{\mathrm{const.}}

\section{Language modelling} \label{sec:lm}

In language modelling, $\Sigma$ is a finite, non-empty set of symbols. In the
context of natural language, this is usually set to be the vocabulary of words
or tokens. A string over an alphabet $\Sigma$ is any finite sequence of
alphabet symbols. The output space $\mathcal{Y}$ is set to the set of all
possible strings $\Sigma^*$, which is infinite, thus there is no way to sum
over every possible structure, nor is there an easy way to output the maximum
scoring structure.

A \textit{language model} (LM) is a probability distribution over $\Sigma^*$,
\ie, LMs assign probabilities to strings $\vec{y}\in\Sigma^*$. We can discriminate
between two types of LMs,
\begin{itemize}
  \item \textit{Globally normalized} LMs, which define a single scoring
    function $\score_{\vec{\theta}}: \Sigma^* \to \R$ and normalizes the scores
    across all $\vec{y}\in\Sigma^*$,\sidenote{Global LMs are not used much,
    because their normalizer requires a sum over an infinite set. Thus, we will
    focus on local LMs.} \[
      p(\vec{y}) \doteq \frac{1}{Z_{\vec{\theta}}} \exp \score_{\vec{\theta}}(\vec{y})
    ;\]
  \item \textit{Locally normalized} LMs, which decompose string probabilities
    into conditional probabilities $p(y_i\mid \bm{y}_{<i})$ over symbols $y_i$
    given the previous symbols $\bm{y}_{<i}$,
    \begin{align*}
      p(\vec{y}) &\doteq p(\eos \mid \vec{y}) \cdot \prod_{i=1}^N p(y_i \mid \vec{y}_{<i}) \\
      p(y_i \mid \vec{y}_{<i}) &\doteq \frac{1}{Z_{\vec{\theta}}(\vec{y}_{<i})} \exp \score_{\vec{\theta}}(y_i, \vec{y}_{<i})
    .\end{align*}
\end{itemize}

Local LMs are collections of conditional probability distributions $p(y_i\mid
\vec{y}_{<i})$, which intuitively tells us how probable symbol $y_i$ is to
follow the already seen string $\vec{y}_{<i}$. In practice, we also need
beginning-of-sentence ($\bos$) and end-of-sentence ($\eos$) symbols. We
condition on $\bos$ for the first token to model that the sequence starts with
$y_1$ and we condition on the entire string with $\eos$ to model the
probability that no more symbols follow.

\marginnote{We can also use local LMs to generate strings by continuously
sampling from the probability distribution $p(y_i\mid \bm{y}_{<i})$ until
we sample \eos, \eg, \[
  \text{\smallcaps{He} \smallcaps{walks} \smallcaps{the} } \begin{cases}
    p(\smallcaps{dog} \mid \smallcaps{He} \cdots)=0.5 \\
    p(\smallcaps{cat} \mid \smallcaps{He} \cdots)=0.24 \\
    p(\eos \mid \smallcaps{He} \cdots)=0.01. \end{cases}
\]}

A well-defined LM always assigns probability to \eos\;given any history
$\vec{y}_{<i}$, because otherwise we could get in the situation where we have a
history that never ends in \eos, which will not result in a string. Models that
have this problem are called non-tight. The probability of all sentences in a
non-tight model do not sum to $1$. To mitigate this problem, we ensure a model
is tight by forcing $p(\eos \mid \vec{y}_{<i}) > \xi > 0$ for every history
$\vec{y}_{<i}$.

The problem with local LMs is that there are infinitely many distributions
$p(\cdot \mid \vec{y}_{<i})$ with $\vec{y}_{<i}\in\Sigma^*$. Thus, we need some
way of being able to compute all these distributions.

\subsection{$n$-grams}

The $n$-gram solution is to limit histories $\vec{y}_{<i}$ to a length $n-1$.
This assumption leads to the following probability distribution, \[
  p(y_i\mid \bm{y}_{<i}) = p(y_i\mid y_{i-n+1},\ldots,y_{i-1})
.\]
This ensures a finite number $|\Sigma|^{n-1}$ of histories.

The naive implementation of $n$-gram would be to define a separate
conditional probability distribution for every possible context of size
$n-1$. Thus, we can assign each history a probability distribution based on
the counts we observe in training data, \[
  p(y_i\mid y_{i-n+1},\ldots,y_{i-1}) = \frac{\mathcount(y_{i-n+1},\ldots,y_{i-1},y_i)}{\mathcount(y_{i-n+1},\ldots,y_{i-1})}
.\]
However, this does not allow us to share parameters between histories, which
might be very similar and give much insight.\sidenote{\Eg, the distributions
over \[ p(\cdot \mid \text{\smallcaps{She} \smallcaps{walks}}) \hspace{1em} p(\cdot \mid \text{\smallcaps{He} \smallcaps{walks}}) \] are
parametrized independently, even though their distributions should be
extremely similar.} Furthermore, it is very memory inefficient, since we need
to store $\left(|\bar{\Sigma}|-1\right) |\Sigma|^{n-1}$ parameters to specify
the $|\Sigma|^{n-1}$ conditional distributions for each history, where
$\bar{\Sigma} = \Sigma \cup \{ \eos \}$.

The neural $n$-gram \citep{bengio2000neural} is a more efficient approach that does allow parameter
sharing. It uses word embeddings to encode the words and
histories, \[
  p(y_i\mid \bm{y}_{<i}) = \frac{\exp \lft( \transpose{\vec{e}(y_i)} \vec{h}_i \rgt)}{\sum_{y'\in\bar{\Sigma}} \exp \lft( \transpose{\vec{e}(y')} \vec{h}_i \rgt)}
,\]
where $\bm{h}_i = \mathrm{enc}(\vec{y}_{<i}) =
\text{enc}(y_{i-n+1},\ldots,y_{i-1})$.\sidenote{\citet{bengio2000neural} used a
neural network to encode the history. In this architecture, the words in the
history $\vec{y}_{i-n+1:i-1}$ are concatenated (preserve word-order) and used
as input to a neural network that outputs a $d$-dimensional representation
$\vec{h}_i$.} The crucial idea of this approach is that if the word embedding
is similar to the encoding of the history, it is more likely. This approach has
$\bigo{d|V|}$ space complexity for all $n$.

\subsection{Recurrent neural networks}

\begin{figure}[ht]
    \centering
    \incfig{rnn}
    \caption{Diagram of the RNN architecture. Each hidden state $\vec{h}_i$ has
    ``seen`` all previous tokens $\vec{x}_{1:{i-1}}$.}
    \label{fig:rnn}
\end{figure}

The fixed history size of the $n$-gram model is not realistic. We want to be
able to encode the entire history, which is possible with \textit{recurrent
neural networks} (RNN). RNNs work by having two inputs at every timestep: the
time-dependent hidden state $\vec{h}_{i-1}$, representing all words before the
current one, and embedding of the current token $\vec{e}(y_{i-1})$. These are
combined to represent the entire history. There are many variations, but the
simplest one is the Elman RNN \citep{elman1990finding}, \[
  \vec{h}_i = \sigma \lft( \mat{W}_h \vec{h}_{i-1} + \mat{W}_x \vec{e}(y_{i-1}) + \vec{b} \rgt)
,\] 
where $\mat{W}_h \in \R^{d\times d}$, $\mat{W}_x \in \R^{d\times d}$, and
$\vec{b} \in \R^d$ are learned parameters. At their core, RNNs are just a
non-linear combination of the recurrent state and the inputs.

RNNs are trained by unrolling the timesteps and applying backpropagation. The
problem with this is that RNNs become prone to the vanishing/exploding gradient
problem, because computing the gradient of the parameters involves a lot of
matrix operations with itself. The LSTM \citep{hochreiter1997long} and GRU
\citep{cho2014learning} architectures seek to solve this problem.

\begin{align*}
  \pdv{\ell}{\mat{W}_h} &= \pdv{\ell}{y} \pdv{y}{\vec{h}_N} \pdv{\vec{h}_N}{\mat{W}_h} \\
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_N} \sum_{i=1}^N \pdv{\vec{h}_N}{\vec{h}_i} \pdv{\vec{h}_i}{\mat{W}_h} \\
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_N} \sum_{i=1}^N \lft( \prod_{m=i+1}^N \pdv{\vec{h}_m}{\vec{h}_{m-1}} \rgt) \pdv{\vec{h}_i}{\mat{W}_h} \\
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_N} \sum_{i=1}^N \lft( \prod_{m=i+1}^N
  \pdv*{\sigma\lft( \mat{W}_h\vec{h}_{i-1} + \mat{W}_x\vec{e}(y_{i-1}) + \vec{b} \rgt)}{\vec{h}_{m-1}} \rgt)
  \pdv*{\sigma \lft( \mat{W}_h\vec{h}_{i-1} + \mat{W}_x\vec{e}(y_{i-1}) + \vec{b} \rgt)}{\mat{W}_h} \\
  &= \pdv{\ell}{y} \pdv{y}{\vec{h}_N} \sum_{i=1}^N \lft( \prod_{m=i+1}^N \sigma' \lft( \mat{W}_h\vec{h}_{i-1} + \mat{W}_x\vec{e}(y_{i-1}) + \vec{b} \rgt) \mat{W}_h \rgt) \sigma'\lft( \mat{W}_h\vec{h}_{i-1} + \mat{W}_x\vec{e}(y_{i-1}) + \vec{b}\rgt) \vec{h}_{i-1} \\
  \intertext{If we use the $\tanh$ activation function with a derivative that
  never exceeds 1, we get the following,}
  &\approx \pdv{\ell}{y} \pdv{y}{\vec{h}_N} \sum_{i=1}^N \vec{h}_{i-1} \prod_{m=i+1}^N \mat{W}_h \\
\end{align*}
This contains many multiplications of $\mat{W}_h$ with itself, which causes
exploding or vanishing gradient (dependent on whether the determinant of
$\mat{W}_h$ is greater than or less than $1$).
