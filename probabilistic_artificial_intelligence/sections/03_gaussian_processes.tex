\section{Gaussian processes}

BLR can only make linear predictions, because it is linear in the parameters.
However, we might want to make non-linear predictions. We could apply BLR on
non-linearly transformed data,\sidenote{This would mean redefining $f(\vec{x})$
to \[ f(\vec{x}) = \transpose{\vec{\phi}(\vec{x})} \vec{w} ,\] with \eg \[
\vec{\phi}(x) = \transpose{\begin{bmatrix} 1 & x & x^2 \end{bmatrix}}.
\]} but
the computational cost would increase with the dimensionality of the feature
space.

An alternative way of looking at it is considering inference directly in
function space. We use \textit{Gaussian processes} (GP) to describe
distributions over functions. They are formally defined as an infinite
collection of random variables, of which any finite number have a joint Gaussian
distribution. GPs are specified by a mean function $\mu:
\mathcal{X}\to\mathbb{R}$ and a kernel function
$k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$, and written as the
following,\sidenote{In other words, we sample functions from a GP defined by a
mean and kernel function.} \[
  f \sim \mathcal{GP}(\mu, k).
\]

Now, we would want a finite Gaussian distribution over a marginal
$\{\vec{x}_1,\ldots,\vec{x}_m\} \subseteq \X$ (stored by matrix $\mat{X}$) of
the infinite collection of random variables $\X$. If we assume a normal prior on
the weights, \[
  \vec{w} \sim \mathcal{N}(\vec{0},\mat{I}),
\]
then the distribution
over $\vec{f} = \transpose{\vec{w}} \transpose{\mat{X}} = \mat{X} \vec{w}$
becomes the following, \[
  \vec{f} \sim \mathcal{N} \lft( \transpose{\mat{X}} \vec{0},\transpose{\mat{X}}
  \mat{I} \mat{X} \rgt) = \mathcal{N} \lft( \vec{0},\transpose{\mat{X}} \mat{X} \rgt).
\]
Notice that the data points enter as inner products, thus we do not necessarily
need to let them linearly depend. We could also use any kernel
function,\sidenote[][-3.5cm]{Formally, $k(\vec{x},\vec{x}')=\transpose{\vec{\phi}(\vec{x})}
  \vec{\phi}(\vec{x}')$ for some feature function $\vec{\phi}$. But, using a
kernel function $k$ instead makes it more computationally efficient, because the
dimensionality of the Gaussian does not scale with the output dimensionality of
the feature function.}
\[
  \vec{f} \sim \mathcal{N}(\vec{0}, k(\mat{X}, \mat{X}))
\]
with \[
  k(\mat{X}, \mat{X}) = \begin{bmatrix}
    k(\vec{x}_1,\vec{x}_1) & \cdots & k(\vec{x}_1,\vec{x}_m) \\
    \vdots & \ddots & \vdots \\
    k(\vec{x}_m,\vec{x}_1) & \cdots & k(\vec{x}_m,\vec{x}_m)
  \end{bmatrix}.
\]

\begin{marginfigure}
    \begin{tikzpicture}
      \begin{axis}[
        width=\textwidth,
        ymin=-3,
        ymax=3,
        xmin=-3,
        xmax=3,
        grid=major,
        no markers,
        yticklabel=\empty,
        scale only axis,
        every axis plot post/.append style={black},
      ]
        \addplot [dashed] table [x=x, y=mean, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};
        \addplot [thick] table [x=x, y=periodic 0, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};
        \addplot [thick] table [x=x, y=periodic 1, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};

        \addplot [name path=upper1,draw=none] table [x=x,y expr=\thisrow{mean}+\thisrow{std}, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};
        \addplot [name path=lower1,draw=none] table [x=x,y expr=\thisrow{mean}-\thisrow{std}, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};
        \addplot [fill=black, fill opacity=0.1] fill between [of=upper1 and lower1];

        \addplot [name path=upper2,draw=none] table [x=x,y expr=\thisrow{mean}+2*\thisrow{std}, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};
        \addplot [name path=lower2,draw=none] table [x=x,y expr=\thisrow{mean}-2*\thisrow{std}, col sep=comma] {data/gaussian_processes/a_priori_samples.csv};
        \addplot [fill=black, fill opacity=0.1] fill between [of=upper2 and lower2];
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
      \begin{axis}[
        width=\textwidth,
        ymin=0,
        ymax=1.5,
        xmin=-3,
        xmax=3,
        scale only axis,
        yticklabel=\empty,
        grid=major,
        no markers,
        every axis plot post/.append style={black},
      ]
        \addplot [thick] table [x=x, y=k, col sep=comma] {data/gaussian_processes/periodic_kernel.csv};
      \end{axis}
    \end{tikzpicture}
    \caption{A priori samples of a Gaussian process with the periodic kernel.
    The second plot shows the kernel function \wrt $x=0$.}
    \label{fig:gp-prior}
\end{marginfigure}

We can sample function realizations from a Gaussian process, by taking $n$
equidistant points from $\mathcal{X}$ as matrix $\mat{X}\in\R^{n\times d}$.
Then, we assume $\mu(\vec{x}) = 0$ and compute $k(\mat{X},\mat{X})$ to make a
multivariate Gaussian distribution over $\vec{f}\in\R^n$. Lastly, we sample a
vector from this multivariate Gaussian distribution and interpolate between the
points to form the function realization. Notice that GPs parametrize a
probability distribution over functions.

\Cref{fig:gp-prior} shows samples of a prior Gaussian process with the periodic
kernel, \[
  k(\vec{x},\vec{x}') = \sigma^2 \exp \lft( -\frac{2}{\ell^2}\sin^2\lft(\pi
  \frac{|\vec{x}-\vec{x}'|}{p} \rgt) \rgt),
\]
where $\sigma^2$ is the overall variance, $\ell$ is the lengthscale, and $p$ is
the period. It also shows the periodic kernel function \wrt $x=0$. As can be
seen, pairs of points $(x,x')$ with high covariance $k(x,x')$ have close values
in all sampled function realizations. This makes sense, since $k$ parametrizes
the covariance.

\subsection{Learning and inference}

Adding aleatoric noise, we define the observed data $\vec{y}$ to have the
following distribution, \[
  \vec{y} \sim \mathcal{N}(\vec{0}, k(\mat{X}, \mat{X}) + \sigma_n^2\mat{I}).
\]

Now, suppose that we observe data $\vec{y}$ for datapoints $\mat{X}$, and want
to predict the probability distribution of $y^\star$ for $\vec{x}^\star$ given
the observed data. We can define the following a priori joint
distribution,\sidenote[][0.25cm]{Notice that this is a priori because we have
  not observed (conditioned on) any data points yet. We are only specifying the
joint distribution over $\vec{y}$ and $f^\star$.} \[
  \begin{bmatrix}
    \vec{y} \\
    f^\star
  \end{bmatrix} \mid \vec{x}^\star, \vec{X}
  \sim
  \mathcal{N} \lft(
  \vec{0},
  \begin{bmatrix}
    k(\mat{X},\mat{X}) + \sigma_n^2\mat{I} & k(\mat{X},\vec{x}^\star) \\
    k(\vec{x}^\star,\mat{X}) & k(\vec{x}^\star, \vec{x}^\star)
  \end{bmatrix} \rgt),
\]
where \[
  k(\mat{X},\vec{x}) = \begin{bmatrix} k(\vec{x}_1,\vec{x}) \\ \vdots \\ k(\vec{x}_m,\vec{x}) \end{bmatrix}.
\]

Then, we can derive the conditional distribution, using the conditional
property of multivariate Gaussian distributions, as \[
  f^\star \mid \vec{x}^\star, \mat{X}, \vec{y} \sim \mathcal{N}(\mu^\star,k^\star),
\]
where
\begin{align*}
  \mu^\star &= k(\vec{x}^\star, \mat{X}) \inv{(k(\mat{X}, \mat{X})+\sigma_n^2\mat{I})} \vec{y} \\
  k^\star &= k(\vec{x}^\star,\vec{x}^\star) - k(\vec{x}^\star,\mat{X}) \inv{(k(\mat{X},\mat{X})+\sigma_n^2\mat{I})} k(\mat{X},\vec{x}^\star).
\end{align*}
Adding aleatoric noise, we get the probability distribution over $y^\star$: \[
  y^\star\mid\vec{x}^\star,\mat{X},\bm{y}\sim\mathcal{N}(\mu^\star,k^\star+\sigma_n^2).
\]


\subsection{Kernel functions}

Suppose we have two covariance functions, \[
  k_1 : \mathcal{X} \times \mathcal{X} \to \mathbb{R}, \hspace{1em} k_2 : \mathcal{X} \times \mathcal{X} \to \mathbb{R},
\]
$c>0$, and $f$ is a polynomial with positive coefficients or the exponential
function. Then, the following functions are valid covariance functions,
\begin{align*}
  k(\vec{x},\vec{x}') &= k_1(\vec{x},\vec{x}')+k_2(\vec{x},\vec{x}') \\
  k(\vec{x},\vec{x}') &= k_1(\vec{x},\vec{x}')\cdot k_2(\vec{x},\vec{x}') \\
  k(\vec{x},\vec{x}') &= c\cdot k_1(\vec{x},\vec{x}') \\
  k(\vec{x},\vec{x}') &= f(k_1(\vec{x},\vec{x}')).
\end{align*}

\begin{marginfigure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=-3,
      ymax=3,
      xmin=-3,
      xmax=3,
      grid=major,
      scale only axis,
      yticklabel=\empty,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [dashed] table [x=x, y=mean periodic, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=periodic 0, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=periodic 1, col sep=comma] {data/gaussian_processes/posterior_samples.csv};

      \addplot [name path=upper1,draw=none] table [x=x,y expr=\thisrow{mean periodic}+\thisrow{std periodic}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower1,draw=none] table [x=x,y expr=\thisrow{mean periodic}-\thisrow{std periodic}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper1 and lower1];

      \addplot [name path=upper2,draw=none] table [x=x,y expr=\thisrow{mean periodic}+2*\thisrow{std periodic}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower2,draw=none] table [x=x,y expr=\thisrow{mean periodic}-2*\thisrow{std periodic}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper2 and lower2];

      \addplot [only marks, mark=+, mark size=2pt, color=black] coordinates {
        (-3, 0)
        (-2, -2)
        (-1, -0.2)
        (0, 0.9)
        (2, -0.1)
      };
    \end{axis}
  \end{tikzpicture}
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=0,
      ymax=0.1,
      xmin=-3,
      xmax=3,
      grid=major,
      scale only axis,
      yticklabel=\empty,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [thick] table [x=x, y=k at 0 periodic, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{The periodic kernel function hyperparameters used are $\sigma^2=1$,
  $\ell=1$, and $p=3$. The second plot shows the covariance \wrt $x=0$.}
  \label{fig:gp-posterior-periodic}
\end{marginfigure}

\begin{marginfigure}[.5cm]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=-3,
      ymax=3,
      xmin=-3,
      xmax=3,
      grid=major,
      scale only axis,
      yticklabels=\empty,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [dashed] table [x=x, y=mean linear, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=linear 0, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=linear 1, col sep=comma] {data/gaussian_processes/posterior_samples.csv};

      \addplot [name path=upper1,draw=none] table [x=x,y expr=\thisrow{mean linear}+\thisrow{std linear}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower1,draw=none] table [x=x,y expr=\thisrow{mean linear}-\thisrow{std linear}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper1 and lower1];

      \addplot [name path=upper2,draw=none] table [x=x,y expr=\thisrow{mean linear}+2*\thisrow{std linear}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower2,draw=none] table [x=x,y expr=\thisrow{mean linear}-2*\thisrow{std linear}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper2 and lower2];

      \addplot [only marks, mark=+, mark size=2pt, color=black] coordinates {
        (-3, 0)
        (-2, -2)
        (-1, -0.2)
        (0, 0.9)
        (2, -0.1)
      };
    \end{axis}
  \end{tikzpicture}
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=0,
      ymax=0.2,
      xmin=-3,
      xmax=3,
      grid=major,
      scale only axis,
      yticklabels=\empty,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [thick] table [x=x, y=k at 0 linear, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick, dashed] table [x=x, y=k at 1.5 linear, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick, dotted] table [x=x, y=k at -1 linear, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \legend{$x=0$,$x=\nicefrac{3}{2}$,$x=-1$}
    \end{axis}
  \end{tikzpicture}
  \caption{Posterior linear kernel. The second plot shows the covariance \wrt several
  points.}
  \label{fig:gp-posterior-linear}
\end{marginfigure}

\begin{definition}[Stationary and isotropic kernels]
  A kernel function $k$ is \textit{stationary} if its function only depends on
  the difference between its arguments, \ie,
  $k(\vec{x},\vec{x}')=k(\vec{x}-\vec{x}')$. It is \textit{isotropic} if it only
  depends on the $\ell_2$-distance between its arguments, \ie,
  $k(\vec{x},\vec{x}')=k(\lVert \vec{x}-\vec{x}' \rVert_2)$
\end{definition}

The following is a list of popular kernels,
\begin{itemize}
  \item \textit{Linear kernel}, \[
      k(\vec{x},\vec{x}') = \transpose{\vec{x}}\vec{x}' + \sigma_0^2.
    \]
    Its posterior can be seen in Figure \ref{fig:gp-posterior-linear};

  \item \textit{Gaussian kernel} (a.k.a. \textit{Squared Exponential} or
    \textit{RBF}), \[
      k(\vec{x},\vec{x}') = \exp \lft( -\frac{\lVert \vec{x}-\vec{x}'
      \rVert^2_2}{2\ell^2} \rgt).
    \]
    Points that are close together have a high covariance, while points further
    away have a lower one. This is what makes it smooth, and continuous. Its
    posterior can be seen in \Cref{fig:gp-posterior-gaussian};

  \item \textit{Exponential kernel}, \[
      k(\vec{x},\vec{x}') = \exp \lft( -\frac{\lVert \vec{x}-\vec{x}' \rVert}{\ell} \rgt).
    \]
    Points that are close together have a high covariance. However, because it
    uses the $\ell_1$-distance, the covariance function is not smooth, but
    ``linear on two sides``.  This is why it has so many peaks. Its posterior
    can be seen in \Cref{fig:gp-posterior-exponential}.
\end{itemize}

\subsection{Model selection}


As can be seen in
\Cref{fig:gp-posterior-linear,fig:gp-posterior-gaussian,fig:gp-posterior-periodic,fig:gp-posterior-exponential},
the hyperparameters matter a lot for whether a GP can model the datapoints
correctly. These hyperparameters can be learned by maximizing its predictive
performance on the data. (Hyperparameter fitting on the training data does not
cause overfitting. This will become clear.)

Suppose we have data $\{(\vec{x}_i^\star,y_i^\star)\}_{i=1}^n$, then we would
like to choose hyperparameters, such that the performance on this data is
maximized. There are several choices for measuring predictive performance.
The most naive option is mean squared error, \[
  \text{MSE}_{\theta}(y^\star,\vec{x}^\star) = (y^\star - \mu_{\theta}^\star(\vec{x}^\star))^2.
\]
This metric ignores aleatoric ($\sigma_n^2$) and epistemic
($k(\vec{x}^\star,\vec{x}^\star)$) uncertainty, thus it will not work well for
measuring the performance of GPs. Another option is to just add the variance
to the loss, so it also will be minimized, \[
  \tilde{\text{MSE}}_{\theta}(y^\star,\vec{x}^\star) = \text{MSE}_{\theta}(y^\star,\vec{x}^\star) + k_{\theta}^\star(\vec{x}^\star,\vec{x}^\star).
\]
The problem with this is that it encourages low epistemic uncertainty which
will cause the loss function to favor models that have low epistemic
uncertainty without it necessarily being true. Furthermore, it still ignores
the aleatoric uncertainty.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=-3,
      ymax=3,
      xmin=-3,
      xmax=3,
      scale only axis,
      yticklabels=\empty,
      grid=major,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [dashed] table [x=x, y=mean gaussian, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=gaussian 0, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=gaussian 1, col sep=comma] {data/gaussian_processes/posterior_samples.csv};

      \addplot [name path=upper1,draw=none] table [x=x,y expr=\thisrow{mean gaussian}+\thisrow{std gaussian}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower1,draw=none] table [x=x,y expr=\thisrow{mean gaussian}-\thisrow{std gaussian}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper1 and lower1];

      \addplot [name path=upper2,draw=none] table [x=x,y expr=\thisrow{mean gaussian}+2*\thisrow{std gaussian}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower2,draw=none] table [x=x,y expr=\thisrow{mean gaussian}-2*\thisrow{std gaussian}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper2 and lower2];

      \addplot [only marks, mark=+, mark size=2pt, color=black] coordinates {
        (-3, 0)
        (-2, -2)
        (-1, -0.2)
        (0, 0.9)
        (2, -0.1)
      };
    \end{axis}
  \end{tikzpicture}
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=0,
      ymax=0.2,
      xmin=-3,
      xmax=3,
      scale only axis,
      yticklabels=\empty,
      grid=major,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [thick] table [x=x, y=k at 0 gaussian, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick, dashed] table [x=x, y=k at 1.5 gaussian, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick, dotted] table [x=x, y=k at -1 gaussian, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \legend{$x=0$,$x=\nicefrac{3}{2}$,$x=-1$}
    \end{axis}
  \end{tikzpicture}

  \caption{Posterior Gaussian kernel. The second plot shows the covariance \wrt several
  points.}
  \label{fig:gp-posterior-gaussian}
\end{marginfigure}

\begin{marginfigure}[.5cm]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=-3,
      ymax=3,
      xmin=-3,
      xmax=3,
      scale only axis,
      yticklabels=\empty,
      grid=major,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [dashed] table [x=x, y=mean exponential, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=exponential 0, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick] table [x=x, y=exponential 1, col sep=comma] {data/gaussian_processes/posterior_samples.csv};

      \addplot [name path=upper1,draw=none] table [x=x,y expr=\thisrow{mean exponential}+\thisrow{std exponential}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower1,draw=none] table [x=x,y expr=\thisrow{mean exponential}-\thisrow{std exponential}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper1 and lower1];

      \addplot [name path=upper2,draw=none] table [x=x,y expr=\thisrow{mean exponential}+2*\thisrow{std exponential}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [name path=lower2,draw=none] table [x=x,y expr=\thisrow{mean exponential}-2*\thisrow{std exponential}, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [fill=black, fill opacity=0.1] fill between [of=upper2 and lower2];

      \addplot [only marks, mark=+, mark size=2pt, color=black] coordinates {
        (-3, 0)
        (-2, -2)
        (-1, -0.2)
        (0, 0.9)
        (2, -0.1)
      };
    \end{axis}
  \end{tikzpicture}
  \begin{tikzpicture}
    \begin{axis}[
      width=\textwidth,
      ymin=0,
      ymax=0.2,
      xmin=-3,
      xmax=3,
      scale only axis,
      yticklabels=\empty,
      grid=major,
      no markers,
      every axis plot post/.append style={black},
    ]
      \addplot [thick] table [x=x, y=k at 0 exponential, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick, dashed] table [x=x, y=k at 1.5 exponential, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \addplot [thick, dotted] table [x=x, y=k at -1 exponential, col sep=comma] {data/gaussian_processes/posterior_samples.csv};
      \legend{$x=0$,$x=\nicefrac{3}{2}$,$x=-1$}
    \end{axis}
  \end{tikzpicture}
  \caption{Posterior exponential kernel. The second plot shows the covariance \wrt several
  points.}
  \label{fig:gp-posterior-exponential}
\end{marginfigure}

The Bayesian perspective provides an alternative approach: maximizing the
likelihood of the data,
\begin{align*}
  \ell \ell_{\theta}(y^\star,\vec{x}^\star) &= \mathcal{N}(y^\star; f_{\theta}^\star(\vec{x}^\star), \sigma_n^2) \\
  &= \mathcal{N}(y^\star; \mu_{\theta}^\star, k_{\theta}^\star(\vec{x}^\star,\vec{x}^\star) + \sigma_n^2).
\end{align*}

We can optimize $\theta$ as follows,
\begin{align*}
  \hat{\theta} &= \argmax_\theta p(\bm{y}^\star\mid \mat{X}^\star, \theta) \\
  &= \argmax_\theta \int p(\bm{y}^\star\mid f,\mat{X}^\star, \theta) p(f\mid \theta) df \\
  &= \argmax_\theta \mathcal{N}(\bm{y}^\star; \bm{0}, k(\mat{X}^\star,\mat{X}^\star) + \sigma_n^2\bm{I}) \\
  &= \argmin_\theta -\log \mathcal{N}(\bm{y}^\star; \bm{0}, k(\mat{X}^\star,\mat{X}^\star) + \sigma_n^2\bm{I}) \\
  &= \argmin_\theta \frac{n}{2} \log 2\pi + \frac{1}{2}\log \det{k(\mat{X}^\star,\mat{X}^\star)+\sigma_n^2\bm{I}}+ \frac{1}{2}\bm{y}^{\star\top} (k(\mat{X}^\star,\mat{X}^\star) + \sigma_n^2\bm{I})^{-1}\bm{y}^\star \\
  &= \argmin_\theta \underbrace{\frac{1}{2}\log \det{k(\mat{X}^\star,\mat{X}^\star)+\sigma_n^2\bm{I}}}_{\text{complexity penalty}} + \underbrace{\frac{1}{2}\bm{y}^{\star\top} (k(\mat{X}^\star,\mat{X}^\star) + \sigma_n^2\bm{I})^{-1}\bm{y}^\star}_{\text{``goodness`` of fit}}.
\end{align*}
As can be seen, the loss function seeks a balance between the ``goodness`` of
the fit and complexity. If we increase the aleatoric uncertainty
$\sigma_n^2$, we increase the goodness of the fit, but increase the
complexity. This is how it prevents over- and underfitting and is the reason
why we do not need a validation dataset.


\subsection{Efficiency}

The time complexity of computing the posterior is $\Theta(n^3)$\sidenote{For
BLR, this is $\bigo{dn^2}$.} and the space complexity of storing
$k(\mat{X},\mat{X})$ is $\Theta(n^2)$. The main approaches for accelerating GP
posterior computation are exploiting parallelism (GPU),\sidenote{This yields a
significant speedup, but does not address the cubic scaling in $n$.} local GP
methods, kernel function approximations, and inducing point methods.

\paragraph{Local GP methods.}

The basic idea is that, for covariance functions that decay with
distance,\sidenote{Think of stationary kernels such as the Gaussian and
exponential kernels.} we only need to condition on close points to $\vec{x}$.
\Ie, to make a prediction at point $\vec{x}$, we only need to condition on
points $\vec{x}'$ where $|k(\vec{x},\vec{x}')|\geq \tau$ for some threshold
$\tau$. The problem with this method is that it is still expensive if there
are many close points.

\paragraph{Kernel function approximation.}

The key idea of approximating kernel functions is that we can construct an
$m$-dimensional feature map with $m \ll n$ that approximates the true kernel
function, \[
  k(\vec{x},\vec{x}')\approx \transpose{\vec{\phi}(\vec{x})} \vec{\phi}(\vec{x}') \hspace{1em} \vec{\phi}(\vec{x})\in\R^m.
\]
Then, apply BLR. The computational cost becomes $\bigo{nm^2 + m^3}$ instead of
$\bigo{n^3}$.

An example is Random Fourier Features (RFF) that reduces stationary kernels to
their Fourier transform. Then, samples from this $m$ times and defines it as
the feature map $\vec{\phi}$. The problem with RFFs is that they approximate
the kernel function globally, however this might not be necessary, since we
only need accurate representation for the training and test points.

\paragraph{Inducing point methods.}

The idea behind inducing point methods is that we won't need all data. In areas
where there are a lot of data points, we can safely throw some away. This
method needs to figure out which points are safe to throw away, \ie, find
inducing points $\mathcal{U}$ that we need to keep to approximate well. This
can be done by choosing them randomly or we could treat $\mathcal{U}$ as
hyperparameters and maximize the marginal likelihood of the data.
