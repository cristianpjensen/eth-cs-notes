\section{Newton's method}

\marginnote{The \textit{Babylonian method} to compute square roots is an application of the
    Newton-Raphson method. It finds zeros of $f(x) = x^2 - R$, which is equal to zero at $\sqrt{R}$ and
    $-\sqrt{R}$. It takes $\bigo{\log R}$ steps to get within $\nicefrac{1}{2}$ of a square root.
    Then, to get within $\epsilon$, it takes $\log \log(\nicefrac{1}{\epsilon})$ steps. Thus, once we
    are close, we get very close very quickly.}

The \textit{Newton-Raphson method} is an iterative method for finding a zero of a differentiable
univariate function $f: \R \to \R$. Starting from some $x_0$, it iteratively computes \[
    x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}.
\]
In formulas, $x_{t+1}$ is the solution to the following linear equation, \[
    f(x_t) + f'(x_t) (x - x_t) = 0,
\]
which yields the above update formula. The Newton step fails if $f'(x_t) = 0$ or gets out of
control if $|f'(x_t)|$ is very small. Thus, we need to keep this in mind when making a theoretical
analysis.

\begin{marginfigure}
    \centering
    \incfig{newton-raphson}
    \caption{A step of the Newton-Raphson method.}
    \label{fig:newton-raphson}
\end{marginfigure}

We can use this method for optimization as well, called \textit{Newton's method}, where we can find
critical points $f'(x) = 0$ by applying the method to the derivative of $f$, \[
    x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}.
\]
We can further generalize this update step to finding critical points $\grad{f(\vec{x})}{} =
    \vec{0}$ in any dimensionality, \[
    \vec{x}_{t+1} = \vec{x}_t - \inv{\hess{f(\vec{x}_t)}{}} \grad{f(\vec{x}_t)}{}.
\]
As before, we need to keep in mind that the Hessian must be invertible and may get out of control
if the Hessian has a small Spectral norm.

A second interpretation of Newton's method is that it is a special case of the general update
scheme, \[
    \vec{x}_{t+1} = \vec{x}_t - H(\vec{x}_t) \grad{f(\vec{x}_t)}{},
\]
where $H(\vec{x}_t) \in \R^{d \times d}$ is some matrix---like gradient descent with $H(\vec{x}_t)
    = \gamma_t \mat{I}$. Hence, we can think of Newton's method as ``adaptive'' gradient descent that
adapts to the local curvature of the function at $\vec{x}_t$.\sidenote{This is very apparent in the
    case of optimizing a quadratic function of the form $f(\vec{x}) = \frac{1}{2} \transpose{\vec{x}}
        \mat{M} \vec{x} - \transpose{\vec{q}} \vec{x} + c$, which has the same curvature
    $\hess{f(\vec{x})}{} = \mat{M}$ everywhere. In this case, Newton's method yields the optimum in a
    single step, $\vec{x}_1 = \vec{x}^\star$. \textit{Proof}:
    \begin{align*}
        \vec{x}_1 & = \vec{x}_0 - \hess{f(\vec{x}_0)}{}^{-1} \grad{f(\vec{x}_0)}{} \\
                  & = \vec{x}_0 - \mat{M}^{-1} (\mat{M} \vec{x}_0 - \vec{q})       \\
                  & = \mat{M}^{-1} \vec{q}                                         \\
                  & = \vec{x}^\star.
    \end{align*}}

Furthermore, we can interpret Newton's method as minimizing the local second-order Taylor
approximation around $\vec{x}_t$, \[
    \vec{x}_{t+1} \in \argmin_{\vec{x} \in \R^d} f(\vec{x}_t) + \transpose{\grad{f(\vec{x}_t)}{}}(\vec{x} - \vec{x}_t) + \frac{1}{2} \transpose{(\vec{x} - \vec{x}_t)} \hess{f(\vec{x}_t)}{}(\vec{x} - \vec{x}_t).
\]

We will not prove any general convergence guarantees for Newton's method. We will prove that, under
suitable conditions, and starting close to a critical point, we will reach distance at most
$\epsilon$ to this critical point in $\bigo{\log \log(\nicefrac{1}{\epsilon})}$ steps. This also
holds for non-convex functions. However, this is quite weak, since we assume that we are already
close to the critical point. The proof will rely on the assumption that the local curvature in the
small space around the critical point is near constant.

\begin{theorem} \label{thm:newton}
    Let $f: \dom{f} \to \R$ be twice differentiable with a critical point $\tilde{\vec{x}}$. Suppose
    there is a ball $\mathcal{X} \subseteq \dom{f}$ with center $\tilde{\vec{x}}$ such that the inverse
    Hessians are bounded. \Ie, for some $\mu > 0$, we have \[
        \lft\| \inv{\hess{f(\vec{x})}{}} \rgt\|_2 \leq \frac{1}{\mu}, \quad \forall \vec{x} \in \mathcal{X}.
    \]
    Moreover, assume that the Hessian is Lipschitz continuous. \Ie, there exists $B > 0$, such that \[
        \lft\| \hess{f(\vec{x})}{} - \hess{f(\vec{y})}{} \rgt\|_2 \leq B \| \vec{x} - \vec{y} \|, \quad \forall \vec{x},\vec{y}\in \mathcal{X}.
    \]
    Then, for $\vec{x}_t \in \mathcal{X}$ and $\vec{x}_{t+1}$, resulting from the Newton step, we have \[
        \| \vec{x}_{t+1} - \tilde{\vec{x}} \| \leq \frac{B}{2 \mu} \| \vec{x}_t - \tilde{\vec{x}} \|^2.
    \]
\end{theorem}

\begin{proof}
    Let $H(\vec{x}) = \hess{f(\vec{x})}{}$, $\vec{x} = \vec{x}_t$, $\vec{x}' = \vec{x}_{t+1}$. Then,
    subtracting $\tilde{\vec{x}}$ from both sides of the Newton step yields
    \begin{align*}
        \vec{x}' - \tilde{\vec{x}} & = \vec{x} - \tilde{\vec{x}} - \inv{H(\vec{x})} \grad{f(\vec{x})}{}                                                                                                                                                                     \\
                                   & = \vec{x} - \tilde{\vec{x}} + \inv{H(\vec{x})} (\grad{f(\tilde{\vec{x}})}{} - \grad{f(\vec{x})}{}). \margintag{$\grad{f(\tilde{\vec{x}})}{} = \vec{0}$.}                                                                               \\
        \intertext{Let $h(t) \doteq \grad{f(\vec{x} + t(\tilde{\vec{x}} - \vec{x}))}{}$. Then, using the fundamental theorem of calculus, we get}
                                   & = \vec{x} - \tilde{\vec{x}} + \inv{H(\vec{x})} \int_0^1 H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) (\tilde{\vec{x}} - \vec{x}) \mathrm{d}t \margintag{$h'(t) = H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) (\tilde{\vec{x}} - \vec{x})$.} \\
                                   & = H(\vec{x})^{-1}H(\vec{x})(\vec{x} - \tilde{\vec{x}}) + \inv{H(\vec{x})} \int_0^1 H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) (\tilde{\vec{x}} - \vec{x}) \mathrm{d}t                                                                   \\
                                   & = \inv{H(\vec{x})} \int_0^1 \lft( H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) - H(\vec{x}) \rgt) (\tilde{\vec{x}} - \vec{x}) \mathrm{d}t                                                                                                 \\
        \intertext{Taking norm of both sides yields}
        \| \vec{x}' - \vec{x} \|   & = \lft\| \inv{H(\vec{x})} \int_0^1 \lft( H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) - H(\vec{x}) \rgt) (\tilde{\vec{x}} - \vec{x}) \mathrm{d}t \rgt\|                                                                                   \\
                                   & \leq \lft\| \inv{H(\vec{x})} \rgt\|_2 \cdot \lft\|  \int_0^1 \lft( H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) - H(\vec{x}) \rgt) (\tilde{\vec{x}} - \vec{x}) \mathrm{d}t \rgt\|                                                         \\
                                   & \leq \frac{1}{\mu} \cdot \int_0^1 \lft\| \lft( H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) - H(\vec{x}) \rgt) (\tilde{\vec{x}} - \vec{x}) \rgt\| \mathrm{d}t \margintag{Bounded inverse Hessians.}                                       \\
                                   & \leq \frac{1}{\mu} \cdot \int_0^1 \lft\|  H(\vec{x} + t(\tilde{\vec{x}} - \vec{x})) - H(\vec{x}) \rgt\|_2 \cdot \| \tilde{\vec{x}} - \vec{x} \| \mathrm{d}t                                                                            \\
                                   & \leq \frac{1}{\mu} \| \tilde{\vec{x}} - \vec{x} \| \int_0^1 B \| t(\tilde{\vec{x}} - \vec{x}) \| \mathrm{d}t \margintag{Hessian is $B$-Lipschitz continuous.}                                                                          \\
                                   & \leq \frac{B}{\mu} \| \tilde{\vec{x}} - \vec{x} \|^2 \int_0^1 t \mathrm{d}t                                                                                                                                                            \\
                                   & \leq \frac{B}{2 \mu} \| \tilde{\vec{x}} - \vec{x} \|^2.
    \end{align*}
    This concludes the proof.
\end{proof}

An easy way to ensure bounded inverse Hessians is by requiring strong convexity over $\mathcal{X}$.

\begin{lemma}
    Let $f: \dom{f} \to \R$ be twice differentiable and strongly convex with $\mu$ over an open
    convex subset $\mathcal{X} \subseteq \dom{f}$. Then, $\hess{f(\vec{x})}{}$ is invertible and \[
        \| \hess{f(\vec{x})}{}^{-1} \|_2 \leq \frac{1}{\mu}, \quad \forall \vec{x} \in \mathcal{X}.
    \]
\end{lemma}

\begin{corollary}
    With the assumptions of \Cref{thm:newton}, if $\vec{x}_0 \in \mathcal{X}$ satisfies \[
        \| \vec{x}_0 - \tilde{\vec{x}} \| \leq \frac{\mu}{B},
    \]
    then Newton's method yields \[
        \| \vec{x}_T - \tilde{\vec{x}} \| \leq \frac{\mu}{B} \lft( \frac{1}{2} \rgt)^{2^T - 1}.
    \]
\end{corollary}

Hence, we get the $\bigo{\log \log(\nicefrac{1}{\epsilon})}$ bound, but only if we are
$\nicefrac{\mu}{B}$-close to $\tilde{\vec{x}}$. Thus, we only converge fast to $\tilde{\vec{x}}$ if
we are already close to it. For this to hold, it is of course necessary that $\tilde{\vec{x}}$ is
the \textit{only} close critical point to $\vec{x}_0$. However, this necessarily follows from the
assumptions, since the Hessians are almost constant this close to $\tilde{\vec{x}}$ under the
Lipschitz continuity and inverse Hessian bound. Thus, locally, the function behaves like a
quadratic function, which converges to its unique critical point in one step.
