\documentclass{article}

\usepackage[a4paper, margin=0.25cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multicol}
\usepackage{amsmath,amsfonts,amsthm,amssymb,mathrsfs,bbm,mathtools,nicefrac,bm,centernot,colonequals,dsfont}
\usepackage{derivative}
\usepackage[skip=.5\baselineskip-0.5pt]{parskip}
\usepackage[extreme, mathspacing=normal, leadingfraction=0.85]{savetrees}
\usepackage[document]{ragged2e}

\usepackage{color,soul}
\usepackage{xcolor}

\DeclareMathOperator*{\argmax}{amax}
\DeclareMathOperator*{\argmin}{amin}

\newcommand{\lft}{\mathopen{}\mathclose\bgroup\left}
\newcommand{\rgt}{\aftergroup\egroup\right}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\tens}[1]{\bm{\mathsf{#1}}}

\renewcommand{\familydefault}{\sfdefault}

\setlength{\columnseprule}{0.4pt}

\title{Machine Perception Cheatsheet}

\newenvironment{topic}[1]
{\textbf{\sffamily \colorbox{black}{\rlap{\textbf{\textcolor{white}{#1}}}\hspace{\linewidth}\hspace{-2\fboxsep}}} \\ \vspace{0.2cm}}
{}

\begin{document}

\setlength{\columnsep}{0.15cm}

\begin{multicols*}{2}

    \begin{itemize}
        \item $\sigma(x) = \nicefrac{1}{1 + e^{-x}}$, $\tanh(x) = \nicefrac{e^x - e^{-x}}{e^x + e^{-x}}$, $\mathrm{ReLU}(x) = \max\{ 0,x \}$.
        \item \textbf{Derivatives}:
              \begin{align*}
                  \vec{y} = \sigma(\vec{x})           & \Rightarrow \nicefrac{\partial \vec{y}}{\partial \vec{x}} = \mathrm{diag}(\vec{y} \odot (1 - \vec{y}))    \\
                  \vec{y} = \tanh(\vec{x})            & \Rightarrow \nicefrac{\partial \vec{y}}{\partial \vec{x}} = \mathrm{diag}(1 - \vec{y}^2)                  \\
                  \vec{y} = \mathrm{ReLU}(\vec{x})    & \Rightarrow \nicefrac{\partial \vec{y}}{\partial \vec{x}} = \mathrm{diag}(\mathds{1}\{ \vec{x} \geq 0 \}) \\
                  \vec{y} = \mathrm{softmax}(\vec{y}) & \Rightarrow \nicefrac{\partial \vec{y}}{\partial \vec{x}} = \mathrm{diag}(\vec{y}) - \vec{y} \vec{y}^\top \\
                  |x|'                                & = \nicefrac{x}{|x|}.
              \end{align*}
        \item When using $\mat{I}$, specify dimensionality $\mat{I}_d$.
        \item \textbf{Chain rule}: $\vec{y} = g(\vec{x}), \vec{z} = f(\vec{y}) \Rightarrow \pdv{\vec{z}}{\vec{x}} = \pdv{\vec{z}}{\vec{y}} \pdv{\vec{y}}{\vec{x}}$.
        \item \textbf{Sparsity}: $\mathrm{diag}(\vec{x}) \mathrm{vec}_i(y) = \mathrm{vec}_i(x_i y)$.

        \item If $\vec{x} \in \R^{m}$ and $\vec{y} \in \R^{n}$, then $\nicefrac{\partial \vec{x}}{\partial
                      \vec{y}} \in \R^{m\times n}$.
        \item Gradient computation: Look at effect of changing param. on output.
        \item Always \textbf{type-check} gradients.
        \item \textbf{Quotient rule}: $(\nicefrac{f}{g})'(x) = \nicefrac{f'(x)\cdot g(x) - f(x)\cdot g'(x)}{g^2(x)}$.
        \item $\pdv{\mat{A} \vec{x}}{\vec{x}} = \mat{A}$, $\pdv{\vec{x} \odot \vec{y}}{\vec{x}} = \mathrm{diag}(\vec{y})$, $\pdv{\vec{x} \odot \vec{y}}{\vec{y}} = \mathrm{diag}(\vec{x})$.

    \end{itemize}

    % Multi-variate Gaussian: \[
    %     \mathcal{N}(\vec{x} ; \vec{\mu}, \mat{\Sigma}) = \frac{1}{2\pi \sqrt{\det(\mat{\Sigma})^{-1}}} \exp \lft( -\frac{1}{2} (\vec{x}-\vec{\mu})^\top \mat{\Sigma}^{-1} (\vec{x} - \vec{\mu}) \rgt).
    % \]

    \begin{topic}{Neural networks}

        \textbf{Perceptron}: The original perceptron was a single-layer perceptron with non-linearity
        $\mathds{1}\{ x > 0 \}$. Classification is done by $\hat{y} = \mathds{1}\{ \vec{w}^\top
            \vec{x} + b > 0 \}$. Learning: iteratively apply $\vec{\theta} \gets
            \vec{\theta} + \eta(y_i - \hat{y}_i) \vec{x}_i$. GD with Hinge loss.
        If the data is linearly separable, the perceptron converges in finite time.

        \textbf{MLP}: $\hat{y} = \sigma (\mat{W}_k \sigma( \mat{W}_{k-1} \cdots \sigma( \mat{W}_1 \vec{x} + \vec{b}_1 ) \cdots + \vec{b}_{k-1} ) + \vec{b}_k )$.

        Do not forget biases!

        \textbf{Loss functions}: MLE (=NLL): $\argmin_{\vec{\theta}} - \sum_{i=1}^{n} \log p(y_i \mid \vec{\theta})$.
        BCE (MLE with Bern.): $\argmin_{\vec{\theta}} - \sum_{i=1}^{n} y_i \log \hat{y}_i + (1-y_i) \log (1 - \hat{y}_i)$.
        MAP: $\argmin_{\vec{\theta}} - \log p(\vec{\theta}) - \sum_{i=1}^{n} \log p(y_i \mid \vec{\theta})$.

        \textbf{Backpropagation}: Linear-time algorithm to compute gradients using chain rule.
        Intuitively, gradients work well for updating weights, because it measures the direction in
        which the loss decreases at a point. \textbf{Gradient descent}: If we have to increase the
        output value, we can (1) increase the weight connected to a neuron with strong activation, or
        (2) increase the activation that is connected to a strong weight by recursion. The gradient
        tells us how to do this. Update: $\vec{\theta} \gets \vec{\theta} - \eta \nabla_{\vec{\theta}} \mathcal{L}(\hat{\vec{y}}, \vec{y}; \vec{\theta})$.

        \textbf{Universal approximation theorem}: Let $g$ be any function on unit hypercube. Let
        $\epsilon > 0$, then there exists a NN $f_{\vec{\theta}}$ with a single hidden layer that can
        approximate this function with $\epsilon$ precision. \textit{In words}: An MLP with a single
        hidden layer and continuous non-linear activation function can approximate any continuous
        function with arbitrary precision.

        \textbf{Early stopping}: Stop training if the validation error has increased for the last
        $p$ checks. We check every $n$ epochs. \textbf{Batch norm}: Solves internal covariate shift
        problem: The distribution of the input changes during training, because the weights change.
        Problem because the gradient tells us how to change parameters given that the other
        layers do not change. Learnable parameters are running averages of mean and variance.
        \textbf{Data augmentation}: Randomly transform data to enforce robustness.
        \textbf{Pre-training}: First train on task with large dataset to initialize model for task
        with small dataset. \textbf{Regularization}: Any technique that aims to improve
        generalization, e.g., $\ell_1$ parameter norm penalty: $\lambda \| \vec{w} \|_1$.
        \textbf{Residual layer}: Use cases: Prevent vanishing gradient, Propagate high-freq. information.

    \end{topic}

    \begin{topic}{Convolutional neural networks}

        \textbf{Convolution}: $(\mat{K} * \mat{I})[i,j] = \sum_{m=-k}^{k} \sum_{n=-k}^{k} \mat{K}[m,n] \mat{I}[i-m, j-n]$.
        Cross-correlation uses $+$ instead of $-$. Any linear, shift-equivariant transform can be
        written as a convolution. Matrix operation: \[
            \mat{K} * \mat{I} = \begin{bmatrix}
                k_1    & 0      & \cdots & 0      \\
                k_2    & k_1    & \cdots & 0      \\
                \vdots & \vdots & \ddots & \vdots \\
                0      & 0      & \cdots & k_m
            \end{bmatrix}
            \begin{bmatrix}
                I_1 \\ I_2 \\ \vdots \\ I_n
            \end{bmatrix}.
        \]
        Convolution = Cross-correlation iff $\mat{K}[i,j] = \mat{K}[-i,-j]$.

        \textbf{CNN}: Sequence of alternating convolutional and pooling layers. Convolutional layer from $C_{\mathrm{in}}$ to $C_{\mathrm{out}}$ channels: \[
            \mat{Z}_j^{(\ell)} = \sum\nolimits_{k=1}^{C_{\mathrm{in}}} \mat{W}_{kj}^{(\ell)} * \mat{Z}_k^{(\ell)} + b_j, \quad j \in [C_{\mathrm{out}}].
        \]
        With $C_{\mathrm{in}} = C_{\mathrm{out}} = 1$, we have derivative:
        \begin{gather*}
            \delta^{(\ell-1)}[i,j] \doteq \pdv{\mathcal{L}}{z^{(\ell-1)}[i{,}j]} = \sum\nolimits_{i'} \sum\nolimits_{j'} \delta^{(\ell)}[i',j'] w^{(\ell)}[i'-i,j'-j] \\
            \mat{\Delta}^{(\ell-1)} = \mat{\Delta}^{(\ell)} * \mathrm{Rot}_{180}(\mat{W}^{(\ell)}), \quad \pdv{\mathcal{L}}{\mat{W}^{(\ell)}} = \mat{\Delta}^{(\ell)} * \mat{Z}^{(\ell - 1)}.
        \end{gather*}
        $D_{\mathrm{out}} = \lft\lfloor \frac{D_{\mathrm{in}} + 2 \times p - d \times (k - 1) - 1}{s} + 1 \rgt\rfloor$.
        Params: $(C_{\mathrm{in}} \times K \times K + 1) \times C_{\mathrm{out}}$.

        \textbf{Max-pooling layer}: Increase RF exponentially:
        \begin{align*}
            z^{(\ell)}[i,j]                                & = \max \lft\{ z^{(\ell-1)}[i',j'] \mid i' \in [si : si + k], j' \in [sj : sj+k] \rgt\} \\
            \pdv{z^{(\ell)}[i{,}j]}{z^{(\ell-1)}[i'{,}j']} & = \mathds{1}\{ [i',j'] = [i^\star,j^\star] \}.
        \end{align*}
        No learnable parameters, only propagation of the error.
    \end{topic}

    \begin{topic}{Fully convolutional neural networks}
        \textit{Applications}: Semantic segmentation, Image-to-image translation, Human pose
        estimation. Naive: Classifier on every pixel. Naive: No downsampling (expensive).
        \textbf{FCNN}: Downsample, then upsample.

        \textbf{Upsampling methods}: \textit{Nearest neighbor}: Put value into all corresponding cells.
        \textit{Bed of nails}: Only put value into the top-left cell.
        \textit{Max unpooling}: Remember original position from max-pooling.
        \textit{Transposed convolution}: Insert $s-1$ zeros between pixels and $k-p-1$ zeros as padding. Then, convolve with kernel.

        \textbf{U-net}: Add skip-connection between corresponding down- and upsampling layers. This facilitates the
        combination of global from skip-connection with local information from previous layer.

    \end{topic}

    \begin{topic}{Recurrent neural networks}

        Processes sequential data and is able to take variable-length input. At each timestep, use the same
        network (behaves like a dynamical system): \[
            \vec{h}_t = f_{\vec{\theta}} \lft( \vec{h}_{t-1}, \vec{x}_t \rgt).
        \]
        $\vec{h}_t$ represents the sequence until timestep $t$, which we can use as input into an
        MLP for further processing.

        \textbf{Use-cases}: $1\to 1$: POS tagging. $1 \to N$: Image captioning. $N \to 1$: Sentiment
        classification. $N \to N$: Machine translation.

        \textbf{Elman RNN}: $f_{\vec{\theta}}(\vec{h}_{t-1}, \vec{x}_t) = \tanh \lft( \mat{W}_h \vec{h}_{t-1} + \mat{W}_x \vec{x}_t \rgt)$.

        \textbf{Backpropagation through time} (BPTT) to optimize by unrolling the RNN and applying backpropagation
        on the computational graph: \[
            \pdv{\ell_t}{\mat{W}_h} = \sum\nolimits_{k=1}^{t} \pdv{\ell_t}{\hat{\vec{y}}_t} \pdv{\hat{\vec{y}}_t}{\vec{h}_t} \pdv{\vec{h}_t}{\vec{h}_k} \pdv{^+ \vec{h}_k}{\mat{W}_h}.
        \]
        We have \[
            \pdv{\vec{h}_t}{\vec{h}_k} = \prod\nolimits_{i=k+1}^t \pdv{\vec{h}_i}{\vec{h}_{i-1}} \overset{\text{Elman RNN}}{=} \prod\nolimits_{i=k+1}^{t} \mathrm{diag}(1 - \vec{h}_i^2) \mat{W}_h.
        \]
        Suffers from \textbf{exploding or vanishing gradient} because of the many multiplications of
        $\mat{W}_h$ with itself in the gradient. If the largest eigenvalue of this matrix is greater than
        the upper bound of the non-linearity, the gradient will explode. If it is smaller, it will vanish.
        Problems: (1) Instability due to NaN/$\infty$, (2) Hard to capture long-term dependencies, (3)
        Jumps over local minima.

        \textbf{Leaky unit}: Constant error flow to solve vanishing gradient:
        \begin{align*}
            \hat{\vec{h}}_t & = f_{\vec{\theta}}\lft( \vec{h}_{t-1}, \vec{x}_t \rgt) \\
            \vec{h}_t       & = \alpha \vec{h}_{t-1} + (1-\alpha) \hat{\vec{h}}_t.
        \end{align*}

        \textbf{LSTM}: Take idea of leaky unit to the next level by introducing gates, which protect the memory cell to make sure there is always error flow:
        \begin{align*}
            \vec{f}_t & = \sigma \lft( \mat{W}_{f} [\vec{h}_{t-1}, \vec{x}_t] \rgt) \\
            \vec{i}_t & = \sigma \lft( \mat{W}_{i} [\vec{h}_{t-1}, \vec{x}_t] \rgt) \\
            \vec{o}_t & = \sigma \lft( \mat{W}_{o} [\vec{h}_{t-1}, \vec{x}_t] \rgt) \\
            \vec{g}_t & = \tanh \lft( \mat{W}_{g} [\vec{h}_{t-1}, \vec{x}_t] \rgt).
        \end{align*}
        Then, the memory cell and hidden state are computed by
        \begin{align*}
            \vec{c}_t & = \vec{f}_t \odot \vec{c}_{t-1} + \vec{i}_t \odot \vec{g}_t \\
            \vec{h}_t & = \vec{o}_t \odot \tanh \lft( \vec{c}_t \rgt).
        \end{align*}
        The \textit{forget gate} $\vec{f}$ decides what information to keep from previous cell state. The \textit{gate
            gate} $\vec{g}$ decides what to write to the cell state. The \textit{input gate} $\vec{i}$ decides what
        values of the cell state should be updated. The \textit{output gate} $\vec{o}$ decides what values of
        the cell state to put into the hidden state.

        The gates form an ``information highway'' that can easily propagate errors through the cell state
        due to the minimal modifications made to it.

        \textbf{Gradient clipping}: Solve exploding gradient by limiting gradient norm: \[
            \vec{\theta} \gets \begin{cases}
                \vec{\theta} - \eta \vec{g},                         & \| \vec{g} \| \leq k \\
                \vec{\theta} - \eta \frac{k}{\| \vec{g} \|} \vec{g}, & \text{otherwise}.
            \end{cases}
        \]

    \end{topic}

    \begin{topic}{Autoencoders}

        Autoencoders are generative models, meaning that they model the underlying distribution of the
        data, which makes it possible to sample from it. It works by an encoder-decoder structure, where
        $f$ maps data points $\vec{x} \in \R^n$ to latent variables $\vec{z} \in \R^d$, i.e., encodes the
        information compactly in $d \ll n$ dimensions. The decoder $g$ maps latent variables $\vec{z}$ back
        to the input space for a reconstruction $\hat{\vec{x}}$. Thus, $g \circ f$ aims to approximate the
        identity function. The assumption is that if the decoder is able to reconstruct the original input
        from the latent representation, this representation must be meaningful.

        \textbf{Linear autoencoder}: PCA ($+$ Closed form solution. $-$ Not powerful).

        \textbf{Non-linear}: Parametrize $f$ and $g$ as NNs to gain performance.

        \textbf{VAE}: Autoencoders have bad sampling quality, because the latent space is not
        well-structured, meaning that there is no continuity or interpolation. The reason for this is
        that there are large regions in the latent space where there are no observations.

        The solution is to make the generator output a distribution over latents. Specifically, it outputs
        a Gaussian distribution $\mathcal{N}(\vec{\mu}_{\vec{\phi}}(\vec{x}),
            \mathrm{diag}(\vec{\sigma}_{\vec{\phi}}^2(\vec{x})))$ over latent vectors. However, naively using
        this method will result in very different $\vec{\mu}$ with very low $\vec{\sigma}^2$ for the
        different data points, which is essentially the same as outputting points.

        To solve this, we must minimize the KL-divergence between the output distribution and the standard
        Gaussian. This encourages the encoder to distribute the encodings evenly around the center of the
        latent space.

        We want to maximize the likelihood $p(\vec{x}) = \int p_{\vec{\theta}}(\vec{x} \mid \vec{z})
            p(\vec{z}) \mathrm{d}\vec{z}$. However, this is intractable. The best we can do is optimize the
        \textbf{ELBO}: \[
            \log p(\vec{x}) \geq \E_{q_{\vec{\phi}}(\vec{z} \mid \vec{x})} [\log p_{\vec{\theta}}(\vec{x} \mid \vec{z})] - \mathrm{KL}(q_{\vec{\phi}}(\vec{z} \mid \vec{x}) \lVert p(\vec{z})).
        \]
        We need to use the reparametrization trick to take the gradient of the expectation, which means
        that instead of sampling $\vec{z} \sim \mathcal{N}(\vec{\mu}, \mathrm{diag}(\vec{\sigma}^2))$, we
        sample $\vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I})$ and compute $\vec{z} = \vec{\mu} +
            \vec{\sigma} \odot \vec{\epsilon}$. After training, we can sample from the distribution by sampling
        $\vec{z} \sim \mathcal{N}(\vec{0}, \mat{I})$ and giving this to the decoder.

        \textbf{$\beta$-VAE}: The VAE still has problems with its latent space: It is \textit{entangled}. A
        latent space is disentangled if each dimensions changes a single feature of the output. We
        solve this by introducing a hyperparameter $\beta$ which gives more weight to the KL term. The
        intuition behind this is that if factors are in practice independent from each other, the
        model should benefit from disentangling them. This can be derived as a
        Lagrangian: $\max_{\vec{\phi},\vec{\theta}} \E_{p_{\mathrm{d}}}[\E_{\vec{z}\mid \vec{x}}]$ s.t. $\mathrm{KL} < \delta$ $\Rightarrow$
        $\mathcal{L} = \E_{\vec{z}\mid \vec{x}} - \beta(\mathrm{KL} - \delta) \geq \E_{\vec{z}\mid \vec{x}} - \beta \mathrm{KL}$.

    \end{topic}

    \begin{topic}{Autoregressive models}

        Autoregressive models can compute the likelihood $p(\vec{x})$ in a tractable way by the chain rule: \[
            p(\vec{x}) = \prod\nolimits_{i=1}^n p(x_i \mid \vec{x}_{1:i-1}).
        \]
        The hard part of this approach is that we must parametrize all possible conditional distributions
        $p(x_{k+1} | \vec{x}_{1:k})$.

        \textbf{FVSBN}: Fully visible sigmoid belief networks parametrize each timestep by its own
        function ($\mathcal{O}(n^2)$ parameters): \[
            f_i(\vec{x}_{1:i-1}) = \sigma \lft( \alpha_0^{(i)} + \alpha_1^{(i)} x_1 + \cdots + \alpha_{i-1}^{(i)} x_{i-1} \rgt).
        \]

        \textbf{NADE}: Problem with FVSBN is that they only have a single hidden layer, making them
        not expressive. NADE uses MLPs: \[
            \vec{h}_i = \sigma(\mat{W}_{:,1:i-1} \vec{x}_{1:i-1} + \vec{b}), \quad \hat{x}_i = \sigma(\mat{V}_{i,:} \vec{h}_i + c_i).
        \]
        This model shares the parameters of $\mat{W}$ between timesteps, which means that it has
        $\mathcal{O}(nd)$ parameters and evaluated in $\mathcal{O}(nd)$ by recursion. \textbf{Teacher
            forcing}: Condition on ground truth when training.

        \textbf{MADE}: Constructs an autoencoder which fulfills the autoregressive property. For this,
        we must ensure that there is no computational path between output unit $\hat{x}_{k+1}$ and
        any of $x_{k+1}, \ldots, x_n$, relative to an
        arbitrary ordering. This is done by uniformly assigning integers $1$ to $n$ to each input
        unit and integer $1$ to $n-1$ to each hidden unit. Then, we only allow values to propagate
        from units in layer $\ell$ to units in layer $\ell+1$ with equal or higher value. Finally,
        we allow connections between the last hidden layer and the output only to units with value
        that is strictly greater.

        Problem: Requires very large networks. And, while it is possible to train efficiently, sampling
        still requires $n$ passes through the network.

        \textbf{Pixel-RNN}: The idea is to generate image pixels starting from the corner and
        modeling the dependency on previous pixels using an RNN. This is slow, because of its
        sequential nature.

        \textbf{Pixel-CNN}: We can solve the efficiency issue of Pixel-RNN by assuming that pixels
        only depend on a context region around them. This allows for parallelization during training.
        During training we need to make sure only previous pixels are used, thus we use a masked
        convolution. Blind spot $\Rightarrow$ Horizontal and vertical stacks of
        convolutions. To enforce the autoregressive property, we need to go over the color channels
        autoregressively. Problem: Still slow during inference.

        \textbf{WaveNet}: Pixel-CNN (audio) with dilated convs for an exponential RF.

        \textbf{VRNN}: RNNs are deterministic $\Rightarrow$ Add stoch. by sampling $\vec{h}_t$ from VAE:
        $\vec{z}_t \sim p_{\vec{\theta}}(\cdot \mid \vec{h}_{t-1}), \vec{x}_t \sim q_{\vec{\phi}}(\cdot \mid \vec{z}_t, \vec{h}_{t-1}), \vec{h}_t \sim p_{\vec{\theta}}(\cdot \mid \vec{h}_{t-1}, \vec{z}_t, \vec{x}_t)$. This is for generation. For encoding, $\vec{z}_t$ depends on $\vec{x}_t$.

        \textbf{Transformers}: Stack normalization, MLPs, and self-attention: \[
            \mat{Y} = \mathrm{softmax}\lft( \nicefrac{\mat{X} \mat{W}_Q \mat{W}_K^\top \mat{X}^\top}{\sqrt{d}} + \mat{M} \rgt) \mat{X} \mat{W}_V,
        \]
        where $\mat{M}$ is a mask that masks out future timesteps with $-\infty$. Intuitively, the softmax
        computes how much attention should be given to certain values. Computational complexity is
        $\mathcal{O}(n^2d)$ with a maximum path length between input and output of $\mathcal{O}(1)$. This
        allows for easy error propagation during training.
    \end{topic}

    \begin{topic}{Normalizing flow}
        \begin{align*}
            \log |\det(\mat{A}^{-1})|                          & = \log |\det(\mat{A})|^{-1} = -\log |\det(\mat{A})|                \\
            \det \lft( \mat{I} + \vec{u} h' \vec{w}^\top \rgt) & = 1 + h' \vec{u}^\top \vec{w}                                      \\
            \vec{x} \odot \vec{y}                              & = \mathrm{diag}(\vec{x}) \vec{y} = \mathrm{diag}(\vec{y}) \vec{x}.
        \end{align*}
        The determinant of a triangular matrix is the product of its diagonal.

        \textbf{Change of variables}: Best of both worlds: latent space and a tractable likelihood by
        leveraging change of variables: \[
            p_X(\vec{x}) = p_Z(f^{-1}(\vec{x})) \lft| \det\lft( \pdv{f^{-1}(\vec{x})}{\vec{x}} \rgt) \rgt| = p_Z(\vec{z}) \lft| \det\lft( \pdv{f(\vec{z})}{\vec{z}} \rgt) \rgt|^{-1}.
        \]
        Downside is that $f$ must be invertible, which means that we must preserve dimensionality between
        latent space and data space. Furthermore the determinant of the Jacobian must be efficiently
        computed, thus we must design $f$ such that its Jacobian is triangular.

        \textbf{Volume-preserving} means $\det\lft(\pdv{f^{-1}(\vec{x})}{\vec{x}}\rgt) = \det\lft(\pdv{f(\vec{x})}{\vec{x}}\rgt)^{-1} = 1$.

        \textbf{Coupling layer}: \[
            f: \begin{bmatrix}
                \vec{x}_A \\ \vec{x}_B
            \end{bmatrix}
            \mapsto
            \begin{bmatrix}
                h(\vec{x}_A, \beta(\vec{x}_B)) \\
                \vec{x}_B
            \end{bmatrix},
        \]
        where $\beta$ can be any NN and $h$ is invertible w.r.t. its first argument, given the second. The
        inverse is: \[
            f^{-1}: \begin{bmatrix}
                \vec{y}_A \\
                \vec{y}_B
            \end{bmatrix}
            \mapsto
            \begin{bmatrix}
                h^{-1}(\vec{y}_A, \beta(\vec{y}_B)) \\
                \vec{y}_B
            \end{bmatrix}.
        \]
        \[
            \pdv{f(\vec{x})}{\vec{x}} = \begin{bmatrix}
                \pdv{\vec{y}_A}{\vec{x}_A} & \pdv{\vec{y}_A}{\vec{x}_B} \\
                \pdv{\vec{y}_B}{\vec{x}_A} & \pdv{\vec{y}_B}{\vec{x}_B}
            \end{bmatrix}
            = \begin{bmatrix}
                h'(\vec{x}_A, \beta(\vec{x}_B)) & h'(\vec{x}_A, \beta(\vec{x}_B)) \beta'(\vec{x}_B) \\
                \mat{0}                         & \mat{I}
            \end{bmatrix}.
        \]
        To compute the determinant, we need only $\pdv{\vec{y}_A}{\vec{x}_A}$ and
        $\pdv{\vec{y}_B}{\vec{x}_B}$.

        This layer leaves part of its input unchanged, thus we must make sure to alternate what parts of
        the input get transformed.

        \textbf{Composing transformations}: Chaining many layers: \[
            \vec{x} = f(\vec{z}) = (f_m \circ \cdots \circ f_1)(\vec{z}).
        \]
        Using change of variables: \[
            p_X(\vec{x}) = p_Z(f^{-1}(\vec{x})) \prod\nolimits_{k=1}^m \lft| \det\lft( \pdv{f_k(\vec{x})}{\vec{x}} \rgt) \rgt|^{-1}.
        \]

        \textbf{Training}: Maximize the log-likelihood: \[
            \log p_{X}(\mat{X}) = \sum\nolimits_{i=1}^{n} \log p_Z(f^{-1}(\vec{x}_i)) - \sum\nolimits_{k=1}^{m} \log \lft| \det\lft( \pdv{f_k(\vec{x}_i)}{\vec{x}_i} \rgt) \rgt|.
        \]

        \textbf{NICE}: Split data by partitioning into two subsets and randomly alternating which is given to the NN. Additive coupling network: \[
            \begin{bmatrix} \vec{y}_A \\ \vec{y}_B \end{bmatrix}
            =
            \begin{bmatrix} \vec{x}_A + \beta(\vec{x}_B) \\ \vec{x}_B \end{bmatrix}.
        \]

        \textbf{RealNVP}: Splits data by partitioning using a checkerboard and a channel-wise
        masking. The channel-wise masking is used after a squeezing operation to go from $C\times H
            \times W$ to $4C \times \nicefrac{H}{2} \times \nicefrac{W}{2}$. This ensures all data can interact with each other. Affine mapping: \[
            \begin{bmatrix} \vec{y}_A \\ \vec{y}_B \end{bmatrix} = \begin{bmatrix} \vec{x}_A \odot \exp(\vec{s}(\vec{x}_B)) + \vec{t}(\vec{x}_B) \\ \vec{x}_B \end{bmatrix},
        \]
        where $\vec{s}$ and $\vec{t}$ can be arbitrarily complex.

        \textbf{GLOW}: Uses invertible $1\times 1$ convolutions to split the data, meaning that it
        learns how to split. It consists of $L$ levels, consisting of $K$ steps of flow, which apply
        activation norm, invertible $1 \times 1$ convolution, and a coupling layer as in RealNVP, in
        that order.

    \end{topic}

    \begin{topic}{Generative adversarial network}

        \begin{itemize}
            \item KL div: $\mathrm{KL}(p \lVert q) = \E_{\vec{x} \sim p} \lft[ \log \nicefrac{p(\vec{x})}{q(\vec{x})}
                          \rgt] = - \E_{\vec{x} \sim p} \lft[ \log \nicefrac{q(\vec{x})}{p(\vec{x})} \rgt]$.
            \item JS div: $\mathrm{JS}(p \lVert q) = \nicefrac{1}{2} \cdot \mathrm{KL}\lft(p \middle\lVert
                      \nicefrac{(p + q)}{2} \rgt) + \nicefrac{1}{2} \cdot \mathrm{KL}\lft( q \middle\lVert \nicefrac{(p +
                              q)}{2} \rgt)$.
        \end{itemize}

        \textbf{Problem with optimizing likelihood}: Optimizing likelihood does not necessarily give good results. Two possible cases:
        \begin{itemize}
            \item \textit{Good likelihood with bad sample quality}: Let $p$ be a good model and $q$ a model that only outputs
                  noise. $0.01p + 0.99q$ has log-likelihood: \[
                      \log(0.01p(\vec{x}) + 0.99q(\vec{x})) \geq \log(p(\vec{x})) - \log 100.
                  \]
                  The $\log p(\vec{x})$ is proportional to the dimensionality of the input. Thus, will be high for
                  high-dimensional data.

            \item \textit{Bad likelihood with high sample quality}: Occurs when he model overfits on the training data.
                  Results in bad likelihood on test set.

        \end{itemize}

        \textbf{GAN}: Solve the above problem by introducing a discriminator. The objective of the
        generator is then to maximize the discriminator's classification loss by generating images
        similar to the training set, implicitly inducing $p_{\mathrm{model}}$. Value function (derived from BCE): \[
            \argmin\nolimits_{G} \argmax\nolimits_{D} V(D, G) := \E_{p_{\mathrm{data}}}[\log D(\vec{x})] + \E_{p_{\mathrm{prior}}} [\log (1 - D(G(\vec{z})))].
        \]

        \textbf{Optimal discriminator}: $D^\star(\vec{x}) = \nicefrac{p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})}$. \\
        $\blacksquare:$ $\argmax_{D} V(D,G)$ $\Rightarrow$ $\E \to \int$ $\Rightarrow$ $p_{\mathrm{prior}} \to p_{\mathrm{model}}$ $\Rightarrow$ Combine $\int$ $\Rightarrow$ $\max_y a \log(y) + b \log(1-y) = \nicefrac{a}{a+b}$ for $a,b > 0$.

        \textbf{Global optimality}: The generator is optimal if $p_{\mathrm{model}} = p_{\mathrm{data}}$ and at optimum, we have $V(D^\star, G^\star) = -\log 4$. $G$ implicitly optimizes JS div. \\
        $\blacksquare:$ $V(D^\star,G)$ $\Rightarrow$ $\times \nicefrac{2}{2}$ in $\E$s $\Rightarrow$ Take
        out $-\log 2$ of $\E$s $\Rightarrow$ $2 \mathrm{JS}(p \| q) - \log 4$.

        \textbf{Convergence guarantee}: Assuming that $D$ and $G$ have sufficient capacity, at each update step $D \to D^\star$, and $p_{\mathrm{model}}$ is updated to improve
        \begin{align*}
            V(D^\star, p_{\mathrm{model}}) & = \E_{\vec{x} \sim p_d} [\log D^\star(\vec{x})] + \E_{\vec{x} \sim p_m} [\log (1 - D^\star(\vec{x}))] \\
                                           & \propto \sup_D \int p_{\mathrm{model}}(\vec{x}) \log (1 - D(\vec{x})) \mathrm{d}\vec{x}.
        \end{align*}
        Then, $p_{\mathrm{model}}$ converges to $p_{\mathrm{data}}$, because $V(D^\star,
            p_{\mathrm{model}})$ is convex in $p_{\mathrm{model}}$ and supremum preserves convexity.

        \textit{Weak result}: $G$ and $D$ have finite capacity, $D$ does not necessarily converge to $D^\star$, and
        due to $G$ being NN, the obj is no longer convex.

        \textbf{Saturation}: Early in training, $G$ is poor ($D(G(\vec{z})) \approx 0$), which
        results in $\log (1 - D(G(\vec{z})))$ saturating (small gradient) $\Rightarrow$ $\argmax_G \log D(G(\vec{z}))$.

        \textbf{Mode collapse}: The generator learns to produce high-quality samples with low
        variability. Solution: Unrolled GAN, which optimizes the generator w.r.t. the last $k$
        discriminators.

        \textbf{Training instability}: Optimizing two-player games lead to training instabilities,
        since making progress for one player may mean that the other player is worse off. Finding
        Nash-Equilibria is hard.

        \textbf{Optimizing JS divergence}: If the supports of
        $p_{\mathrm{model}}$ and $p_{\mathrm{data}}$ are disjoint, it is always possible to find a
        perfect discriminator. This results in the loss function equaling zero, meaning that there
        will be no gradient to update the generator's parameters. The solution is the Wasserstein
        GAN, which optimizes the Wasserstein distance. In this case, the loss does not fall to zero
        for disjoint supports, because it measures divergence by how different they are horizontally,
        rather than vertically. Intuitively, it measures how much “work” it takes to turn one
        distribution into the other.

        \textbf{Gradient penalty}: To stabilize training, add a gradient penalty: \[
            \E_{\vec{x} \sim p_d} \lft[ \log D(\vec{x}) + \lambda \| \nabla D(\vec{x}) \|^2 \rgt] + \E_{\vec{x} \sim p_m} [\log (1 - D(\vec{x}))].
        \]

    \end{topic}

    \begin{topic}{Diffusion models}

        Compared to GANs, DMs offer high quality generations with better diversity and a more stable
        training process.

        \textbf{Diffusion}: Governed by a noise schedule $\{ \beta_t \}_{t=1}^T$: $q(\vec{x}_t \mid \vec{x}_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t} \vec{x}_{t-1}, \beta_t \mat{I})$.
        Closed-form solution: $\vec{x}_t = \sqrt{\bar{\alpha}_t} \vec{x}_0 + \sqrt{1 - \bar{\alpha}_t} \vec{\epsilon}, \vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I})$,
        where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$. \\
        \textbf{Denoising}: $q(\vec{x}_{t-1} \mid \vec{x}_t)$ intractable $\Rightarrow$ Learn $p_{\vec{\theta}}(\vec{x}_{t-1} \mid \vec{x}_t) \approx
            q(\vec{x}_{t-1} \mid \vec{x}_t, \vec{x}_0)$. For small steps, $q(\vec{x}_{t-1} \mid \vec{x}_t)$
        is Gaussian: \[
            p_{\vec{\theta}}(\vec{x}_{t-1} \mid \vec{x}_t) = \mathcal{N}(\vec{x}_{t-1} ; \vec{\mu}_{\vec{\theta}}(\vec{x}_t, t), \sigma_t^2 \mat{I}).
        \]
        In practice, parametrize network to predict noise $\vec{\epsilon}_{\vec{\theta}}(\vec{x}_t, t)$.
        Iteratively:
        \begin{align*}
            \vec{z}       & \sim \mathcal{N}(\vec{0}, \mat{I}) \text{ if $t > 1$, else $\vec{z} = \vec{0}$}                                                                                    \\
            \vec{x}_{t-1} & = \frac{1}{\sqrt{\alpha_t}} \lft( \vec{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \vec{\epsilon}_{\vec{\theta}}(\vec{x}_t, t) \rgt) + \sigma_t \vec{z}.
        \end{align*}

        \textbf{Training}: \textbf{ELBO}: $\E_{q(\vec{x}_1\mid \vec{x}_0)}[\log p_{\vec{\theta}}(\vec{x}_0 \mid \vec{x}_1)] - \mathrm{KL}(q(\vec{x}_T \mid \vec{x}_0) \| p(\vec{x}_T)) - \sum_{t=2}^{T} \E_{q(\vec{x}_t \mid \vec{x}_0)} [\mathrm{KL}(q(\vec{x}_{t-1} \mid \vec{x}_t, \vec{x}_0) \| p_{\vec{\theta}}(\vec{x}_{t-1} \mid \vec{x}_t))]$ (reconstruction term, prior matching term, denoising matching term). \\
        $\blacksquare$: $\log p(\vec{x}_0)$ $\Rightarrow$ $ \int \mathrm{d}\vec{x}_{1:T}$ $\Rightarrow$ $\times \nicefrac{q(\vec{x}_{1:T} \mid \vec{x}_0)}{q(\vec{x}_{1:T} \mid \vec{x}_0)}$ $\Rightarrow$ $\E_{q(\vec{x}_{1:T} \mid \vec{x}_0)}$ $\Rightarrow$ Jensen $\Rightarrow$ Prob. CR: $\sum_{t=2}^{T} \log \nicefrac{p(\vec{x}_{t-1} \mid \vec{x}_t)}{q(\vec{x}_t \mid \vec{x}_{t-1}, \vec{x}_0)}$ $\Rightarrow$ Bayes in $\sum$: $\log \nicefrac{q(\vec{x}_1 \mid \vec{x}_0)}{q(\vec{x}_T \mid \vec{x}_0)}$ out $\Rightarrow$ Lin. and marginalize $\E$ $\Rightarrow$ In $\sum$: 2 nested $\E$.

        \textbf{Closed-form denoising matching term}: $\argmin\nolimits_{\vec{\theta}} \mathrm{KL}(q(\vec{x}_{t-1} \mid \vec{x}_t, \vec{x}_0) \| p_{\vec{\theta}}(\vec{x}_{t-1} \mid \vec{x}_t)) = \argmin\nolimits_{\vec{\theta}} \nicefrac{1}{2 \sigma_q^2(t)} \| \vec{\mu}_{\vec{\theta}} - \vec{\mu}_q \|_2^2$,
        with
        \begin{align*}
            \vec{\mu}_q(\vec{x}_t, \vec{x}_0)      & = \frac{1}{\sqrt{\alpha_t}} \vec{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t} \sqrt{\alpha_t}} \vec{\epsilon}                                 \\
            \vec{\mu}_{\vec{\theta}}(\vec{x}_t, t) & = \frac{1}{\sqrt{\alpha_t}} \vec{x}_t - \frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t} \sqrt{\alpha_t}} \vec{\epsilon}_{\vec{\theta}}(\vec{x}_t, t).
        \end{align*}
        \textbf{Cosine theorem}: $\| \vec{x} - \vec{y} \|^2 = \| \vec{x} \|^2 + \| \vec{y} \|^2 -2 \langle \vec{x}, \vec{y} \rangle$. \\
        \textbf{Loss function}: $\lft\| \vec{\epsilon} - \vec{\epsilon}_{\vec{\theta}}\lft(
            \sqrt{\bar{\alpha}_t} \vec{x}_0 + \sqrt{1 - \bar{\alpha}_t} \vec{\epsilon}, t \rgt) \rgt\|^2,
            \vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I})$.

        \textbf{CLIP}: Image-language model that has been trained on image-caption pairs. By using a
        contrastive loss, CLIP is encouraged to encode the image and caption into similar embeddings.

        \textbf{Classifier guidance}: Pretrain a classifier and guide the denoising in a direction
        favoring images that are more reliably classified by the classifier. This is done by
        injecting gradients of the classifier model into the sampling process. Bad: Requires
        training a classifier on noisy data.

        \textbf{Classifier-free guidance}: Jointly train a class-conditional and unconditional diffusion model. It then guides the generation process by \[
            \vec{\epsilon}^\star(\vec{x}, y; t) = (1 + \rho) \vec{\epsilon}_{\vec{\theta}}(\vec{x}, y; t) - \rho \vec{\epsilon}_{\vec{\theta}}(\vec{x}; t)
        \]
        In practice, we usually train a single model and just set the conditioning variable to all zero for
        the unconditional generation. Guidance improves the quality, but reduces the diversity of outputs
        ($\rho \uparrow$: more guidance).

        \textbf{Latent diffusion models}: Train VAE and perform diffusion on latent space for
        efficiency. The diffusion model then only needs to focus on the ``semantic'' aspect of
        generation.

    \end{topic}

    \begin{topic}{Reinforcement learning}

        \textbf{MDP}: $(\mathcal{S}, \mathcal{A}, P, r, \gamma)$. \textbf{Policy}: $\pi: \mathcal{S} \to
            \Delta(\mathcal{A})$. At every point, the goal of the agent is to maximize: \[
            G_t = \sum\nolimits_{k=0}^{\infty} \gamma^k r(s_{t+k}, a_{t+k}) = r(s_t, a_t) + \gamma G_{t+1}.
        \]
        \textbf{Value function} and \textbf{Bellman equation}:
        \begin{align*}
            V^\pi(s) & = \E_{\pi} [G_0 \mid S_0 = s]                                                                                                               \\
                     & = \sum\nolimits_{a \in \mathcal{A}} \pi(a \mid s) \lft[ r(s,a) + \gamma \sum\nolimits_{s' \in \mathcal{S}} P(s' \mid s, a) V^\pi(s') \rgt].
        \end{align*}
        \textbf{Bellman optimality operator}: \[
            (\mathcal{T} V) (s) \doteq \max\nolimits_{a \in \mathcal{A}} \lft\{ r(s,a) + \gamma \sum\nolimits_{s' \in \mathcal{S}} P(s' \mid s, a) V(s') \rgt\}.
        \]
        This function is a $\gamma$-contraction and monotonic:
        \begin{gather*}
            \max\nolimits_{s \in \mathcal{S}} |(\mathcal{T}V')(s) - (\mathcal{T}V)(s)| \leq \gamma \max\nolimits_{s \in \mathcal{S}} |V'(s) - V(s)| \\
            V(s) \leq V'(s) \implies (\mathcal{T}V)(s) \leq (\mathcal{T}V')(s).
        \end{gather*}
        The optimal value function $V^\star$ is the fixed-point of $\mathcal{T}$.

        \textbf{Value iteration}: Iteratively apply $\mathcal{T}$. Linear convergence: \[
            \max\nolimits_{s \in \mathcal{S}} |V_t(s) - V^\star(s)| \leq \gamma^t \max\nolimits_{s \in \mathcal{S}} |V_0(s) - V^\star(s)|.
        \]
        After convergence, we can get the optimal policy by acting greedily: \[
            \pi^\star(s) \in \argmax\nolimits_{a \in \mathcal{A}} \lft\{ r(s,a) + \gamma \sum\nolimits_{s' \in \mathcal{S}} P(s' \mid s, a) V^\star(s') \rgt\}.
        \]

        \textbf{Policy iteration}: Alternates between computing the greedy policy and the value
        function of the policy.

        \textbf{Model-based and model-free}: MB: Learn the underlying MDP and solve it.
        MF: Learn the policy or value function directly.

        \textbf{On-policy and off-policy}: On-policy: Must learn from policy's own data. Off-policy:
        Can learn from any data.

        \textbf{Monte Carlo}: On-policy method that learns from full trajectory and estimates the value function empirically: \[
            V^\pi(s) \approx \frac{1}{n} \sum\nolimits_{i=1}^{n} G(s)^i,
        \]
        where $G(s)^i$ is the return of episode $i$, starting from $s$.

        \textbf{TD-learning}: Learn from transitions $(s, a, r, s')$ using bootstrapping: \[
            V(s) \gets \alpha V(s) + (1- \alpha) (r + \gamma V(s')).
        \]
        % This can also be interpreted as updating the value by the TD error:
        % \begin{align*}
        %     \delta & = r + \gamma V(s') - V(s)   \\
        %     V(s)   & \gets V(s) + \alpha \delta.
        % \end{align*}
        To find the optimal value function, we must visit all states sufficiently often. For this, we can
        use $\epsilon$-greedy with Robbins-Monro conditions.

        \textbf{SARSA}: Learns Q-values of a policy $\pi$ (on-policy): \[
            Q^\pi(s,a) \gets \alpha Q^\pi(s,a) + (1-\alpha) (r + \gamma Q^\pi(s', a')), \quad a' \sim \pi(s').
        \]

        \textbf{Q-learning}: Learns optimal Q-values (off-policy): \[
            Q(s,a) \gets (1-\alpha) Q(s,a) + \alpha (r + \gamma Q(s',a')), \quad a' \in \argmax\nolimits_{a \in \mathcal{A}} Q(s', a).
        \]

        \textbf{DQN}: Large state spaces $\Rightarrow$ Function approximation with loss: \[
            \ell(\vec{\theta}) = (Q_{\vec{\theta}}(s,a) - (r + \gamma Q_{\bar{\vec{\theta}}}(s', a')))^2, \quad a' \in \argmax\nolimits_{a \in \mathcal{A}} Q_{\bar{\vec{\theta}}}(s', a).
        \]
        We train as in supervised learning. Data is not i.i.d. $\Rightarrow$ Replay buffer.

        \textbf{Sample inefficiency of deep RL}: As the policy improves, we can collect better data.
        So, we have to keep training on new better data.

        \textbf{Policy search}: Large or infinite action spaces $\Rightarrow$ Parametrize $\pi_{\vec{\theta}}(\cdot \mid s) = \mathcal{N}(\vec{\mu}_{\vec{\theta}}(s), \mathrm{diag}(\vec{\sigma}_{\vec{\theta}}^2(s)))$.
        Probability of trajectory $\tau$ can be computed by $\pi_{\vec{\theta}}(\tau) = P(s_0) \prod\nolimits_{t=0}^T \pi_{\vec{\theta}}(a_t \mid s_t) P(s_{t+1} \mid s_t, a_t)$.
        Want trajectories with high return more likely $\Rightarrow$ Training objective:
        \begin{align*}
            J(\vec{\theta}) & = \E_{\tau \sim \pi_{\vec{\theta}}} \lft[ \sum\nolimits_{t=0}^{T} \gamma^t r(s_t, a_t) \rgt]                     \\
            % & = \E_{\tau \sim \pi_{\vec{\theta}}} [ r(\tau) ]                                                      \\
                            & = \E_{\tau \sim \pi_{\vec{\theta}}} [ (r(\tau) - b(\tau)) \nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(\tau) ],
        \end{align*}
        using chain rule $\nabla \log f(\vec{x}) = \nicefrac{\nabla f(\vec{x})}{f(\vec{x})}$. We have \[
            \nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(\tau) = \sum\nolimits_{t=0}^{T} \nabla_{\vec{\theta}} \log \pi_{\vec{\theta}}(a_t \mid s_t).
        \]
        Thus, the gradient does not depend on the MDP. This is on-policy.

        \textbf{REINFORCE}: The above is unbiased, but has high variance. We can reduce variance by
        introducing a baseline $b_t(\tau) = \sum_{t'=0}^{t-1} \gamma^{t'} r(s_{t'}, a_{t'})$, which
        turns $r(\tau) - b_t(\tau)$ into $G_t$. This is on-policy.

        \textbf{Actor-critic}: We can make it off-policy by estimating $G_t$ by bootstrapping using a value network. This introduces bias, but reduces variance: \[
            \nabla_{\vec{\theta}} J(\vec{\theta}) = \nabla_{\vec{\theta}} \log \pi_{\vec{\theta}} (a_t \mid s_t) (V(s_t, a_t) - (r(s_t, a_t) + \gamma V(s_{t+1}))).
        \]

    \end{topic}

    \begin{topic}{Implicit surfaces and neural radiance fields}
        Voxels are bad because of $\mathcal{O}(n^3)$ memory cost. 3D points are bad because they do not
        model connectivity. Meshes are bad because of approximation error. We use learned implicit
        functions:
        \begin{itemize}
            \item \textit{Occupancy network}: $f_{\vec{\theta}}: \R^3 \times \mathcal{Z} \to [0,1]$ outputting probability of
                  being inside mesh.
            \item \textit{DeepSDF}: $f_{\vec{\theta}}: \R^3 \times \mathcal{Z} \to \R$ outputting distance to surface.
        \end{itemize}
        \textbf{(Dis)Advantages}: $+$ Non-rigid transformation, Normal vectors defined both
        continuously and correctly, Easily derive from point clouds, Infinite precision, Learnable,
        Less storage. $-$ Computing intersections is expensive, Requires root-finding algos, UV
        space is ill-defined.

        \textbf{Training with watertight meshes}: Simplest case. Sample $n$ points that are
        inside/outside the mesh. Train occupancy network by BCE. Train DeepSDF by
        regression of dist. to mesh of the sampled points.

        \textbf{Training with point clouds}: Train to make sure distance is 0 at points. Problem:
        Model that always outputs 0 is optimal $\Rightarrow$ Eikonal term: \[
            \mathcal{L}(\vec{\theta}) = \sum\nolimits_{i=1}^{n} |f_{\vec{\theta}}(\vec{x}_i)|^2  + \lambda \E_{\vec{x}} \lft[ (\| \nabla_{\vec{x}} f_{\vec{\theta}}(\vec{x}) \| - 1)^2 \rgt].
        \]
        This makes sense from a ``distance'' perspective, since we want to increase the distance by 1 when
        we move 1 unit away.

        \textbf{Training with images}: Exponentially more data. High-level idea: Render model in same
        view as image and use photometric loss (e.g. $\ell_1$): \[
            \ell(\hat{\tens{I}}, \tens{I}) = \sum\nolimits_{\vec{u}} \| \hat{\tens{I}}_{\vec{u}} - \tens{I}_{\vec{u}} \| .
        \]
        For this, we need a texture network $\vec{t}_{\vec{\theta}}: \R^3 \times \mathcal{Z} \to \R^3$ that
        outputs color. This requires the rendering pipeline to be differentiable.

        \textit{Forward pass}: For every pixel $\vec{u}$, determine the first intersection with surface
        $\hat{\vec{p}}$ using the secant method, which iteratively finds the linear intersection of the
        line connecting the points and the $x$-axis. $\hat{\tens{I}}_{\vec{u}} =
            \vec{t}_{\vec{\theta}}(\hat{\vec{p}})$.

        \textit{Backward pass}: Compute gradients (using implicit differentiation) \[
            \pdv{\ell(\vec{\theta})}{\vec{\theta}} = \sum\nolimits_{\vec{u}} \pdv{\ell(\vec{\theta})}{\hat{\tens{I}}_{\vec{u}}} \lft( \pdv{^+ \vec{t}_{\vec{\theta}}(\hat{\vec{p}})}{\vec{\theta}} - \pdv{\vec{t}_{\vec{\theta}}(\hat{\vec{p}})}{\hat{\vec{p}}} \vec{w} \lft( \pdv{f_{\vec{\theta}}(\hat{\vec{p}})}{\hat{\vec{p}}} \vec{w} \rgt)^{-1} \pdv{^+ f_{\vec{\theta}}(\hat{\vec{p}})}{\vec{\theta}} \rgt),
        \]
        where $\vec{r}(d) = \vec{r}_0 + d \vec{w}$ is the ray connecting camera origin to $\vec{u}$.\\
        $\blacksquare:$ $\sum_{\vec{u}}$ $\Rightarrow$ $\hat{\tens{I}}_{\vec{u}} =
            \vec{t}_{\vec{\theta}}(\hat{\vec{p}})$ through $\vec{\theta}$ and $\hat{\vec{p}}$ $\Rightarrow$
        $\vec{\theta}$ through $\hat{\vec{p}}$ $\Rightarrow$ $\hat{\vec{p}} = \vec{r}_0 + \hat{d}\vec{w} =:
            \vec{r}(\hat{d})$ $\Rightarrow$ CR: $\vec{w} \nicefrac{\partial \hat{d}}{\partial \vec{\theta}}$
        $\Rightarrow$ Implicit diff. on $f_{\vec{\theta}}(\hat{\vec{p}}) = \tau$ to get $\nicefrac{\partial
                \hat{d}}{\partial \vec{\theta}}$.

        \textbf{NERF}: Problem: Implicit surfaces are not good at complex scenes, especially with
        transparency or thin structures. NERF takes as input $(x,y,z,\theta,\phi)$ and outputs
        $(r,g,b,\sigma)$, where $(\theta,\phi)$ are the view direction and $\sigma$ is the density,
        allowing the modeling of transparency.

        In the architecture, we must make sure that $\sigma$ does not depend on $(\theta,\phi)$. If
        $\sigma$ depends on $(\theta,\phi)$, it will overfit on the images and learn to remember images,
        rather than reconstruct the geometry. This is because the loss is based on the images.

        \textit{Rendering}: We render ``volumetric clouds''. Instead of only sampling at the surface, we sample
        along the whole ray and compute a weighted average. Let $\alpha_i = 1 - \exp(-\sigma_i \delta_i)$
        with $\delta_i = t_{i+1} - t_i$ be the prob. of light stopping at point $i$. Then, we can compute
        the prob. of light reaching $i$ by $T_i = \prod_{j=1}^{i-1} 1 - \alpha_j$. Then, we compute
        the final color: \[
            \vec{c} = \sum\nolimits_{i=1}^{n} T_i \alpha_i \vec{c}_i.
        \]
        The fundamental difference with differentiable rendering is that we backpropagate through multiple
        points rather than a single point.

        In general, NNs are biased toward low frequency functions, while we need high frequency functions.
        The solution is positional encodings. The low frequency function w.r.t. the encoding is then a high
        frequency function w.r.t. $(x,y,z,\theta,\phi)$. $\gamma(t) = [\sin(t\cdot 2^0 \pi), \ldots, \cos(t
            \cdot 2^{L-1} \pi)]$.

        \textbf{Gaussian splatting}: Problem with NERF: Slow, because of the high number of samples.
        We can reduce by estimating a set of primitives around the object boundary and only sampling
        within these shapes. Cubes are hard to optimize. Spheres are bad at modeling thin structures.
        Solution: Model by many 3D Gaussians. The scene is initialized by a point cloud. Then,
        iteratively, we project the Gaussians onto the image plane and weight them similarly to NERF,
        compute the loss, and backpropagate to update the Gaussians. Note that there are no
        NNs. The weight of a Gaussian at a pixel $\vec{u}$ is computed by \[
            \alpha_i(\vec{u}) = o_i \cdot \exp ( -\nicefrac{1}{2} (\vec{x} - \vec{\mu}_i')^\top \mat{\Sigma}_i'^{-1} (\vec{x} - \vec{\mu}_i')), \quad \mat{\Sigma}' = \mat{J} \mat{W} \mat{\Sigma} \mat{W}^\top \mat{J}^\top,
        \]
        where $o_i$ is the opacity, and $\vec{\mu}_i', \mat{\Sigma}'$ are the parameters of the 2D
        projection of the $i$-th 3D Gaussian. Simple covariance matrix: \[
            \mat{\Sigma} = \mat{R} \mat{S} \mat{S}^\top \mat{R}^\top, \quad \mat{R} \text{ as quaternion (4 numbers)}, \mat{S} \in \R^3.
        \]

    \end{topic}

    \begin{topic}{Parametric human body models}
        \textbf{2D poses}: Body modeling: Use the pictorial structure model, which models the body
        as a graph. Given an image $\tens{I}$ and vertex locations $L = [\ell_1, \ldots, \ell_k]$, we want
        to minimize score: \[
            S(\tens{I}, L) = \sum\nolimits_{i\in V} \alpha_i \cdot \phi(\tens{I}, \ell_i) + \sum\nolimits_{i,j \in E} \beta_{ij} \psi(\ell_i, \ell_j).
        \]
        Generalize by assigning mixture components $m_i$: \[
            S(\tens{I}, L, M) = \sum_{i\in V} \alpha_i^{m_i} \phi(\tens{I}, \ell_i) + \sum_{i,j \in E} \beta_{ij}^{m_im_j} \psi(\ell_i,\ell_j) + \sum_{i,j \in E} b_{ij}^{m_im_j}.
        \]
        where $\alpha^{m_i}_i$ is the ``local appearance template'' for part $i$ with type $m_i$,
        $\beta^{m_im_j}_{ij}$ expresses the likelihood of having template $m_i$ for part $i$ and template
        $m_j$ for part $j$ given the distance between $\ell_i$ and $\ell_j$, and $b_{ij}^{m_im_j}$ is the
        pairwise co-occurrence prior.

        Feature learning: Use deep learning to either regress the locations (DeepPose) or output heatmaps
        for each vertex (Convolutional Pose Machine). They both use architectures with a refinement
        process.

        The two can also be combined by first using feature learning and then refining with body modeling.

        \textbf{Linear-blend skinning}: Simplest method that transforms vertices as a weighted linear combination of global joint transformations: \[
            \bar{\vec{t}}_i' = \lft( \sum\nolimits_{k} w_{ki} \mat{G}'_k(\vec{\theta}, \mat{J}) \rgt) \bar{\vec{t}}_i, \quad \mat{G}'_k(\vec{\theta}, \mat{J}) = \mat{G}_k(\vec{\theta}, \mat{J}) \mat{G}_k(\vec{\theta}_0, \mat{J})^{-1},
        \]
        where $k$ are bones and $i$ are vertices, $\mat{G}_k(\vec{\theta})$ is the rigid bone
        transformation for bone $k$. % , which is weighted by $w_{ki}$ which determines the influence of bone
        % $k$ on vertex $i$.

        \textbf{Skinned multi-person linear model}: Problem with LBS: Does not account for variation
        in body shape and poses often result in unwanted deformations. SMPL solves this by encoding a
        body shape parameter $\vec{\beta} \in \R^{10}$ and pose parameter $\vec{\theta} \in \R^{9K}$.

        \begin{enumerate}
            \item Translate template to identity mesh: $\bar{\mat{T}} + \mat{B}_S(\vec{\beta})$.
            \item Correct for future deformations: $\bar{\mat{T}} + \mat{B}_S(\vec{\beta}) +
                      \mat{B}_P(\vec{\theta})$.
            \item Linear-blend skinning: \[
                      \bar{\vec{t}}_i' = \lft( \sum\nolimits_{k} w_{ki} \mat{G}'_k(\vec{\theta}, \mat{J}(\vec{\beta})) \rgt) \lft(\bar{\vec{t}}_i + \vec{b}_{S,i}(\vec{\beta}) + \vec{b}_{P,i}(\vec{\theta})\rgt).
                  \]
        \end{enumerate}

        We learn $\mat{B}_S(\vec{\beta})$ by PCA on a body shape dataset. $\vec{\beta}$ are the linear
        weights of the largest principal components: \[
            \mat{B}_S(\vec{\beta}; \mathcal{S}) = \sum\nolimits_{n=1}^{|\vec{\beta}|} \beta_n \mat{S}_n.
        \]

        We learn $\mat{B}_P(\vec{\theta})$ from a body pose dataset: \[
            \mat{B}_P(\vec{\theta}; \mathcal{P}) = \sum\nolimits_{n=1}^{9K} (\mat{R}_n(\vec{\theta}) - \mat{R}_n(\vec{\theta}_0)) \mat{P}_n,
        \]
        where $\vec{\theta}_0$ is the template pose, $\mat{R}(\vec{\theta})$ maps pose vectors to vectors
        of rotation matrices, and $\mat{P}_n \in \R^{3N}$ is a vector of vertex displacements

        \textbf{Learned gradient descent}: Method for fitting 3D human shapes to images by combining
        gradient-based optimization with NNs. It leverages a NN to predict
        the parameter update rule for each optimization iteration.
        \begin{align*}
            \mathcal{L}(\Theta_n) & = L_{\mathrm{reproj}}(\hat{\vec{x}}_n, \vec{x}_{\mathrm{GT}})                                             \\
            \Delta                & = \mathcal{N}_{\vec{w}}\lft( \pdv{\mathcal{L}(\Theta_n)}{\Theta_n}, \Theta_n, \vec{x}_{\mathrm{GT}} \rgt) \\
            \Theta_{n+1}          & = \Theta_n + \Delta,
        \end{align*}
        where $\mathcal{N}_{\vec{w}}$ is a NN that predicts the update.
    \end{topic}

\end{multicols*}

\end{document}
