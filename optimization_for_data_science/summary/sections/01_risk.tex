\section{Risk minimization}

\subsection{Algorithms in data science}

In classical algorithm theory, an optimization problem solves a well-defined problem. For example,
Kruskal's algorithm computes the minimum spanning tree of a graph. In data science, it is not as
well-defined. The starting point is a learning problem, and the optimization typically happens on
training data. However, even a perfect result may fail to solve the learning problem, which is a
failure of the model in which the optimization algorithm was applied, rather than the optimization
algorithm itself.

\subsection{Empirical and expected risk}

Typically, we have a data source $\mathcal{X}$, equipped with an unknown probability distribution
$P$. However, we do have access to independent samples $X_1,\ldots,X_n \sampleiid \mathcal{P}$,
which we call the dataset. The goal is to ``explain'' $\mathcal{X}$ through these samples. More
specifically, we have a class of hypotheses $\mathcal{H}$, which are possible explanations of
$\mathcal{X}$. The goal is then to select the hypothesis $H\in \mathcal{H}$ that best ``explains''
$\mathcal{X}$, which we measure by a \textit{loss function} $\ell: \mathcal{H}\times \mathcal{X}\to
    \R$.

The expected risk can be computed by \[
    \ell(H) \doteq \E_{\mathcal{X}}[\ell(H, X)].
\]
The hypothesis that explains best is the \textit{expected risk minimizer}, \[
    H^\star \in \argmin_{H\in \mathcal{H}} \ell(H).
\]

However, since we do not have access to the distribution over $\mathcal{X}$, we cannot compute
$\ell(H)$ or $H^\star$.\sidenote{In some cases, we might be able to argue mathematically about
    expected risk.} Thus, we need to compute the \textit{probably approximately correct} (PAC)
hypothesis. This means that given $\delta,\epsilon > 0$, we want to produce, with probability at
least $1-\delta$, a hypothesis $\tilde{H}\in \mathcal{H}$ such that \[
    \ell(\tilde{H}) \leq \inf_{H\in \mathcal{H}} \ell(H) + \epsilon,
\]
meaning that with high probability, we approximately solve the expected risk minimization problem.

However, we can still not compute this, thus we must use our training data to compute the
\textit{empirical risk}, \[
    \ell_n(H) = \frac{1}{n} \sum_{i=1}^{n} \ell(H,X_i). \margintag{This is a random variable, because it depends on the training data, which are all random variables, distributed according to the probability distribution of $\mathcal{X}$.}
\]

\begin{lemma}[Weak law of large numbers]
    Let $H \in \mathcal{H}$ be a fixed hypothesis. For any $\delta,\epsilon > 0$, there exists
    $n_0\in \mathbb{N}$ such that for $n \geq n_0$, \[
        | \ell_n(H) - \ell(H) | \leq \epsilon,
    \]
    with probability at least $1-\delta$.
\end{lemma}

Given $n\in \mathbb{N}$ and training data $X_1,\ldots,X_n$ from $\mathcal{X}$, we want to produce a
data-dependent hypothesis $\tilde{H}_n$ that is optimal for the data-dependent risk, \[
    \ell_n(\tilde{H}_n) \leq \inf_{H \in \mathcal{H}} \ell_n(H) + \epsilon. \margintag{$\tilde{H}_n$ is a random variable that depends on the training data.}
\]
In an ideal world, the data-dependent hypothesis $\tilde{H}_n$ is also (almost) the best
explanation for the data source $\mathcal{X}$ with probability at least $1-\delta$, \[
    \ell(\tilde{H}_n) \leq \inf_{H \in \mathcal{H}} \ell(H) + \epsilon.
\]

\margintag{Note that the weak law of large numbers can only be applied to a \textit{fixed}
    hypothesis, but not to the data-dependent hypothesis $\tilde{H}_n$. So, we cannot conclude
    $\ell(\tilde{H}_n) \leq \ell_n(\tilde{H}_n) + \epsilon$. Thus, we are not always in an ideal
    world scenario.}

A sufficient condition for an ideal world scenario is that the weak law of large numbers uniformly
holds for all hypotheses with high probability. This leads us to the following theorem.

\begin{theorem} \label{thm:weaker-llm}
    Assume that for any $\delta,\epsilon > 0$, there exists $n_0 \in \mathbb{N}$ such that for $n \geq
        n_0$, \[
        \sup_{H\in \mathcal{H}} |\ell_n(H) - \ell(H)| \leq \epsilon,
    \]
    with probability at least $1-\delta$. Then, for $n \geq n_0$, an approximate empirical risk
    minimizer $\tilde{H}_n$ is PAC for expected risk minimization, meaning that it satisfies \[
        \ell(\tilde{H}_n) \leq \inf_{H\in \mathcal{H}} \ell(H) + 3\epsilon,
    \]
    with probability at least $1-\delta$.
\end{theorem}

\begin{proof}
    Let $\tilde{H}_n$ be the minimizer of $\ell_n$. This is a random variable, but the weak law of
    large numbers holds for all $H \in \mathcal{H}$. Thus,
    \begin{align*}
        \ell(\tilde{H}_n) & \leq \ell_n(\tilde{H}_n) + \epsilon \margintag{Follows from $\sup_{H\in \mathcal{H}} |\ell_n(H) - \ell(H)| \leq \epsilon$.}                                                                                  \\
                          & \leq \inf_{H \in \mathcal{H}} \ell_n(H) + 2 \epsilon \margintag{$\tilde{H}_n$ is an almost best explanation of the training data: $\ell_n(\tilde{H}_n) \leq \inf_{H \in \mathcal{H}} \ell_n(H) + \epsilon$.} \\
                          & \leq \inf_{H \in \mathcal{H}} \ell(H) + 3 \epsilon, \margintag{Follows from $\sup_{H\in \mathcal{H}} |\ell_n(H) - \ell(H)| \leq \epsilon$.}
    \end{align*}
    with probability at least $1-\delta$.
\end{proof}

It turns out that the assumption made by \Cref{thm:weaker-llm} holds in many relevant cases, but it
is not always true.

In this course, we will not learn how to pick the theory --- $\mathcal{H}$ and $\ell$ --- but
rather how to solve the optimization problems that arise in empirical risk minimization after the
theory has been chosen.

\subsection{The map of learning}

\begin{marginfigure}[9cm]
    \centering
    \incfig{map-of-learning}
    \caption{The map of learning. $H_n$ depends on the training data and is generally found by an optimization algorithm. The training data is used to find and compute the empirical risk $\ell_n(H_n)$. We estimate the expected risk $\ell(H_n)$ by held-out validation data.}
    \label{fig:map-of-learning}
\end{marginfigure}

The map of learning can be seen in \Cref{fig:map-of-learning}. It plots the empirical risk ---
$\ell_n(H_n)$ --- against the expected risk --- $\ell(H_n)$, estimated by a validation set. $H_n$
is found by mapping the training data to the hypothesis, which is done by an optimization
algorithm. With high probability, we are in the area denoted by ``empirical risk minimization'',
because
\begin{align*}
    \ell_n(\tilde{H}_n) & \leq \inf_{H\in \mathcal{H}} \ell_n(H) + \epsilon                                 \\
                        & \leq \ell_n(\tilde{H}) + \epsilon \margintag{This holds for any $\tilde{H}$.}     \\
                        & \leq \ell(\tilde{H}) + 2 \epsilon \margintag{Weak law of large numbers (w.h.p.).} \\
                        & \leq \ell(\tilde{H}_n) + 3 \epsilon.
\end{align*}

A model is overfit when we have low empirical risk, while having high expected risk. This means
that the explanation quality on the data source is much worse than on the training data. The main
reason for this is that the theory is so complex that it can explain almost anything.

A model is underfit when we have high empirical risk, while having high expected risk. This means
that we neither explain the training data nor the data source. The main reason for this is that the
theory is too simple to capture the nature of the data.

The model is learning when we have low empirical risk and low expected risk. This means that the
training was successful. Generalization occurs when the expected risk is close to the empirical
risk. Note that this does not mean that the explanation is good, since any ``blind explanation''
will generalize well due to the weak law of large numbers. Generalization means that the empirical
performance is similar the expected performance. Ideally, we want generalization and learning.

Regularization can improve performance by introducing a function $r$ that punishes complex
hypotheses, \[
    \ell' = \ell + \lambda r.
\]
As $\lambda$ grows, more bias is introduced and the empirical risk increases. At the same time, the
sensitivity to the training data decreases, which may lead to a decreased expected risk.
