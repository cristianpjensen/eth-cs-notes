\section{Semantic parsing} \label{sec:semantic_parsing}

Language cannot only be syntactically ambiguous, but also semantically
ambiguous. Also, syntactically valid sentences do not necessarily mean anything.
A good example of this is Chomsky's famous sentence ``Colorless green dreams
sleep furiously.`` In semantic analysis, we want to be able to reduce a natural
language sentence, such as ``Everyone loves someone else``, to its logical form,
\[
  \forall p [ \mathrm{Person}(p) \to \exists q [ \mathrm{Person}(q) \land p \neq
  q \land \mathrm{Loves}(p, q) ]]
.\]
Note that this sentence is ambiguous, because we can swap the order of the
quantifiers to a different logical form, \[
  \exists p [ \mathrm{Person}(p) \to \forall q [ \mathrm{Person}(q) \land p \neq
  q \land \mathrm{Loves}(q, p) ]]
.\]

The challenge of semantic analysis, as always, is parsing a natural language
sentence to its logical form. For this, we must use the \textit{principle of
compositionality}, which states that the meaning of a complex expression is a
function of the meanings of that expression's constituent parts.\sidenote{This
must hold, because otherwise we would not know the meaning of most sentences,
since we have not heard most possible order of words that form a sentence.

Proverbs are edge cases to this principle, since those words have meaning
together that are independent of its parts.}

\subsection{Lambda calculus}

\textit{Lambda calculus} \citep{church1932set} is a model for semantic
analysis, based on the principle of compositionality. On a high level, lambda
calculus first parses a sentence into simpler constituents, and then constructs
the semantic representations using a bottom-up approach.

\begin{figure}[h!]
  \centering
  \incfig{lambda-calculus-example}
  \caption{Lambda calculus on ``Alex likes Bob.``}
  \label{fig:lambda-calculus-example}
\end{figure}

Lambda calculus has only the following rules,
\begin{itemize}
  \item $x,y,z,\ldots$ are variables;
  \item $(\lambda x. f(x))$, which is the \textit{lambda operator}, where $x$ is
    a variable and $f(x)$ an expression;
  \item $(M\:N)$, which is an application of function $M$ to an argument $N$,
    where they are both lambda terms.
\end{itemize}
A variable is bound if it belongs to a scope of abstraction holding its name.
Otherwise, a variable is free. \Eg, $x$ is bound to $\lambda x$ and $z$ is free
in $((\lambda x. \lambda y.  \mathrm{Likes}(x,y)) z)$.

To be able to represent natural language sentences, we also need the following,
\begin{itemize}
  \item Constants that represent objects, denoted by \eg
    $\smallcaps{Alex},\smallcaps{Bob},\ldots$;
  \item Predicates that represent relations between objects, denoted by \eg
    $\mathrm{Teacher}(\cdot),\mathrm{Likes}(\cdot, \cdot), \ldots$;
  \item Quantifiers $\exists$ and $\forall$.
\end{itemize}

Furthermore, lambda calculus has two operations,
\begin{itemize}
  \item $\alpha$-conversion is the process of renaming the variable of a lambda
    operator and all of its bound occurrences. \Eg, \[
      (\lambda x.x) \to (\lambda y. y)
    ;\]
  \item $\beta$-reduction is the process of applying one lambda term to another,
    \ie, in $(\lambda x. M \: N)$, we replace all occurrences of $x$ in $M$ by
    $N$, and remove the lambda operator. \Eg, \[
      (\lambda x. (\lambda y. x y) \: z) \to (\lambda y. z y)
    .\]
\end{itemize}

However, sometimes a $\beta$-reduction would result in a free variable becoming
bound. Then, we would first need to apply an $\alpha$-conversion. Two lambda
terms are equivalent if one can be obtained from the other after a series of
$\alpha$-conversions and $\beta$-reductions.

\subsection{Combinatory logic}

\textit{Combinatory logic} \citep{curry1958combinatory} is an alternative to
lambda calculus that formalizes the concept of computation and the construction
of computable functions. Unlike lambda calculus, combinatory logic does not use
abstractions. Instead, it uses complex functions using a few primitive higher
order functions.

The basic terms of combinatory logic are
\begin{itemize}
  \item $\bm{x},\bm{y},\bm{z},\ldots$ are variables;
  \item $\mathbf{I},\mathbf{S},\mathbf{K}$ are the primitive combinators, which
    are functions that map functions to functions.
\end{itemize}

Terms are then recursively constructed using the rule of application, where
$(\mathbf{A} \: \mathbf{B})$ denotes applying $\mathbf{A}$ to $\mathbf{B}$.

The following are the primitive combinators,\sidenote{In combinatory logic,
parentheses are left-associative, \eg, $(\mathbf{K} \: \bm{x} \: \bm{y})$ means
$((\mathbf{K} \: \bm{x}) \: \bm{y})$ and $(\mathbf{S} \: \bm{x} \: \bm{y} \:
\bm{z})$ means $((\mathbf{S} \: \bm{x}) \: \bm{y} \: \bm{z})$.}
\begin{itemize}
  \item $(\mathbf{K} \: \bm{x} \: \bm{y}) = \bm{x}$ manufactures constant functions;
  \item $(\mathbf{S} \: \bm{x} \: \bm{y} \: \bm{z}) = (\bm{x} \: \bm{z} \:
    (\bm{y} \: \bm{z}))$, which applies $\bm{x}$ to $\bm{y}$ after first substituting
    $\bm{z}$ into each of them;
  \item $(\mathbf{I} \: \bm{x}) = \bm{x}$ works as the identity
    function.\sidenote{$\mathbf{I}$ as a primitive combinator is not necessary,
    since it can be constructed from $\mathbf{S}$ and $\mathbf{K}$,
    \begin{align*}
      ((\mathbf{S} \: \mathbf{K} \: \mathbf{K}) \: \bm{x}) &= (\mathbf{S} \: \mathbf{K} \: \mathbf{K} \: \bm{x}) \\
      &= (\mathbf{K} \: \bm{x} \: (\mathbf{K} \: \bm{x})) \\
      &= \bm{x}
    .\end{align*}}
\end{itemize}
Any lambda term is equivalent to a combinatory term that only uses the
$\mathbf{S}$ and $\mathbf{K}$ combinators.

Further, there are the following combinators that are introduced for
convenience,
\begin{align*}
  (\mathbf{C} \: \bm{x} \: \bm{y} \: \bm{z}) &= ((\bm{x} \: \bm{z}) \: \bm{y}) & \text{\footnotesize cross} \\
  (\mathbf{B} \: \bm{x} \: \bm{y} \: \bm{z}) &= (\bm{x} \: (\bm{y} \: \bm{z})) & \text{\footnotesize composition} \\
  (\mathbf{T} \: \bm{x} \: \bm{y}) &= (\bm{y} \: \bm{z}) & \text{\footnotesize type-raising}
.\end{align*}
The $\mathbf{B}$ and $\mathbf{T}$ combinators will be used in combinatory
categorial grammars.

\subsection{Combinatory categorial grammars}

\textit{Combinatory categorical grammars} (CCG) are an efficiently parsable
group of grammars that are mildly context-sensitive, which means that they have
more expressive power than context-free grammars. CCGs allow us to model
\textit{coordination} and \textit{cross-serial dependencies} in language, which
CFGs cannot.

\begin{definition}[Combinatory categorial grammar]
  A combinatory categorial grammar is a 5-tuple $\langle V_T, V_N, \cat{S}, f,
  R\rangle$, such that
  \begin{itemize}
    \item $V_T$ is a finite set of terminals;
    \item $V_N$ is the finite set of atomic categories;
    \item $\cat{S} \in V_N$ is the distinguished start category;
    \item $f$ is a function mapping terminals $V_T \cup \{ \epsilon \}$ to
      finite subsets of $C(V_N)$;
    \item $R$ is a finite set of combinatory rules.
  \end{itemize}

  $C(V_N)$ is the infinite set of categories that contains all elements of $V_N$
  and recursively contains all elements such that if $c_1,c_2\in C(V_N)$, then
  $c_1 \fs c_2, c_1 \bs c_2 \in C(V_N)$.
\end{definition}

CCGs have two main parts: a lexicon that associates words with categories and
rules that specify how categories can be combined into other categories. The
lexicon contains all information specific to a given language, \ie, valency,
word order, and semantics. The structure information is encoded in the
categories.\sidenote{Unlike CFGs, which encode structure in their rules.}

The rules $R$ are the following (inspired by combinatory logic),
\begin{align*}
  \cat{X \fs Y} \hspace{1em} \cat{Y} &\implies \cat{x} & \text{\footnotesize $(>)$} \\
  \cat{Y} \hspace{1em} \cat{X \bs Y} &\implies \cat{X} & \text{\footnotesize $(<)$} \\
  \cat{X \fs Y} \hspace{1em} \cat{Y \fs Z} &\implies \cat{X \fs Z} & \text{\footnotesize $(\mathbf{B}_>)$} \\
  \cat{Y \bs Z} \hspace{1em} \cat{X \bs Z} &\implies \cat{X \bs Z} & \text{\footnotesize $(\mathbf{B}_<)$} \\
  \forall \cat{T}\in V_N : \cat{X} &\implies \cat{T \fs (T \bs X)} & \text{\footnotesize $(\mathbf{T}_>)$} \\
  \forall \cat{T}\in V_N : \cat{X} &\implies \cat{T \bs (T \fs X)} & \text{\footnotesize $(\mathbf{T}_<)$}
,\end{align*}
of which there are also generalized versions.

\begin{figure*}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \begin{ccg}{3}{\smallcaps{mary} & \smallcaps{likes} & \smallcaps{john}}
      {
        \cat{NP} & \cat{(S \bs NP) \fs NP} & \cat{NP} \\
        $\smallcaps{mary}$ & $\lambda x. \lambda y. \mathrm{Likes}(y,x)$ & $\smallcaps{john}$ \\
        & \cgline{2}{\cgfa} \\
        & \mc{2}{\cat{S \bs NP}} \\
        & \mc{2}{$\lambda y. \mathrm{Likes}(y, \smallcaps{john})$} \\
        \cgline{3}{\cgba} \\
        \mc{3}{\cat{S}} \\
        \mc{3}{$\mathrm{Likes}(\smallcaps{Mary}, \smallcaps{John})$}
      }
    \end{ccg}
    \caption{CCG derivation of ``Mary likes John.``}
    \label{fig:ccg-simple}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.65\textwidth}
    \centering
    \begin{ccg}{4}{\smallcaps{what} & \smallcaps{states} & \smallcaps{border} & \smallcaps{texas}}
      {
        \cat{(S \fs (S \bs NP)) \fs N} & \cat{N} & \cat{(S \bs NP) \fs NP} & \cat{NP} \\
        $\lambda f. \lambda g.  \lambda x. f(x) \land g(x)$ & $\lambda x.
        \mathrm{State}(x)$ & $\lambda x. \lambda y. \mathrm{Borders}(y, x)$ &
        $\smallcaps{Texas}$ \\
        \cgline{2}{\cgfa} & \cgline{2}{\cgfa} \\
        \mc{2}{\cat{S\fs (S \bs NP)}} & \mc{2}{\cat{S \bs NP}} \\
        \mc{2}{$\lambda g. \lambda x. \mathrm{State}(x) \land g(x)$} & \mc{2}{$\lambda y. \mathrm{Borders}(y, \smallcaps{Texas})$} \\
        \cgline{4}{\cgfa} \\
        \mc{4}{\cat{S}} \\
        \mc{4}{$\lambda x. \mathrm{State}(x) \land \mathrm{Borders}(x,
          \smallcaps{Texas})$}
      }
    \end{ccg}
    \caption{CCG derivation of ``What states border Texas?\,``}
    \label{fig:ccg-query}
  \end{subfigure}
  \caption{Simple example CCG derivations.}
\end{figure*}

\subsection{Linear-indexed grammar}

TODO
