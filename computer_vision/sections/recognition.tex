\section{Recognition} \label{sec:recognition}

In \textit{recognition}, we want to be able to recognize whatever (parts of) an
image depicts. For example, determining where all the humans are in an image. The
challenges of this task are the following,
\begin{itemize}
  \item Background clutter;\sidenote{\Eg if the background is similar to the
    object, the object may not be recognized.}
  \item Intra-class variations.\sidenote{\Eg if you want to recognize whether
    an object is a chair, you need to be able to detect any type of chair,
    which can vary greatly.}
\end{itemize}

At first (\approxtext 1960--1990), the geometric area saw recognition as an
alignment problem, where everything consists of shape primitives. Then, you
only had to recognize the primitives and figure out their alignment to figure
out what object is depicted. Later (\approxtext 1990--2000), appearance-based
models estimate the appearance by combining a lot of images that depict the
same thing, \eg, human faces. Then, new data points are compared to this
estimate of appearance. Sliding window approaches were also used, which compare
every patch of an image to a template. Currently, image classifiers are
implemented as machine learning problems that take an image as input and
outputs one of a set of predefined labels.

\subsection{Bag-of-words}

An example of a machine learning algorithm for classification is
\textit{bag-of-words}. At a high level, it extracts local features from an
image, which are compared to the local features of the training images in an
efficient and robust way. Each matching patch is weighted according to the
TF-IDF metric and used to determine a classification of the input image.

\marginnote{\textit{$k$-means clustering} is a method of partitioning
observations into $k$ clusters. It does so by iteratively alternating between
updating the clusters and assigning points to clusters. It represents the
clusters by their mean, and it assigns points by assigning it to the cluster
representation with the smallest distance. It repeats this until convergence,
which is when none of the points update their cluster assignment.}

\begin{enumerate}
  \item \textit{Feature extraction}: The goal of this step is to find patches
    that would be interesting for the classification of its image. For this,
    algorithms such as SIFT and Harris detection are used to find such
    interesting patches. These patches are then extracted and used in the
    following step;
  \item \textit{Dictionary learning}: Patches that depict the same usually do
    not contain the exact same pixels. Thus, we need some way of mapping
    patches that depict the same together. This is done by $k$-means
    clustering, which iteratively updates its $k$ clusters by assigning each
    object to the cluster with the nearest centroid and updating their
    representation (centroid) to be the mean of its objects. This step is
    repeated until convergence. Thus, now each feature is mapped to its closest
    centroid, which means that similar features are mapped together;
  \item \textit{Feature encoding}: The images are encoded by the bag-of-words
    vectors of their features, \ie, an histogram of the counts of the number of
    feature occurrences;
  \item \textit{Classification}: The classification of an image is the class
    that maximizes the TF-IDF metric,\sidenote{Note that each feature maps to
    the class of which its image is part.} which weights each feature by its
    inverse document frequency. \Ie if a feature occurs in a lot of images, it
    has a lower weight, since it is less discriminative.
\end{enumerate}
