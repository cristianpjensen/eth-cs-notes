\section{Gradient-based learning}

\subsection{Backpropagation}

In order to make use of gradient-based learning, we first need to compute the gradient.
Backpropagation is an algorithm that allows the computation of any function, if we know the
gradient of all basic blocks of the function.

We assume that we are differentiating the following function, \[
    F[\vec{\theta}](\vec{x}) \doteq \lft( F_L \circ \cdots \circ F_1 \rgt)(\vec{x}),
\]
with the following hidden layer, \[
    \vec{h}_{\ell} \doteq F_{\ell} \lft[ \vec{\theta}_{\ell} \rgt] \lft(\vec{h}_{\ell-1} \rgt), \quad \vec{h}^{0} = \vec{x}.
\]
The following intermediate gradient is essential for computing the gradients of the parameters, \[
    \vec{\delta}_{\ell} = \pdv{\ell(\vec{y}, F[\vec{\theta}](\vec{x}))}{\vec{h}_{\ell}}.
\]
It has the following recurrence relationship (and base case),
\begin{align*}
    \vec{\delta}_{L}    & = \pdv{\ell(\vec{y}, \hat{\vec{y}})}{\hat{\vec{y}}}, \quad \hat{\vec{y}} = F[\vec{\theta}](\vec{x}) \\
    \vec{\delta}_{\ell} & = \transpose{\lft[ \frac{\vec{h}_{\ell+1}}{\vec{h}_{\ell}} \rgt]} \vec{\delta}_{\ell+1}.
\end{align*}
These can thus be computed efficiently in linear time with dynamic programming. Then, to compute the
parameter gradient of the $\ell$-th layer, we use the chain rule, \[
    \pdv{\ell(\vec{y}, F[\vec{\theta}](\vec{x}))}{\vec{\theta}_{\ell}} = \vec{\delta}_{\ell} \pdv{\vec{h}_{\ell}}{\vec{\theta}_{\ell}}.
\]

\subsection{Gradient descent}

Gradient descent is a gradient-based learning algorithm with the following update rule, \[
    \vec{\theta}^{t+1} = \vec{\theta}^t - \eta \grad{h\lft( \vec{\theta}^t \rgt)}{}, \quad \eta > 0 \margintag{$h \doteq \ell \circ F$.}
\]
One can think of this as a discretization of the following ODE, \[
    \mathrm{d}\vec{\theta} = -\grad{h(\vec{\theta})}{} \mathrm{d}t.
\]
The solution to this ODE is called the gradient flow. The gradient flow is---in a local view--- the
ideal trajectory to follow. However, there is a dependency on the initial condition, which results
in different trajectories with different stationary points, where $\grad{h(\vec{\theta})}{} =
    \vec{0}$.

A key insight of analysis into the behavior of gradient descent is that it can only be successful
if the gradients change slowly. This is formalized by smoothness.

\begin{definition}[Smoothness]
    A function $h$ is $L$-smooth if there exists $L > 0$ such that \[
        \| \grad{h(\vec{\theta}_1)}{} - \grad{h(\vec{\theta}_2)}{} \| \leq L \| \vec{\theta}_1 - \vec{\theta}_2 \|, \quad \forall \vec{\theta}_1, \vec{\theta}_2 \in \Theta.
    \]
    This is equivalent to the following condition, \[
        \| \hess{h(\vec{\theta})}{} \|_2 \leq L, \quad \forall \vec{\theta} \in \Theta.
    \]
\end{definition}

From the Taylor series expansion, we have
\begin{align*}
    h(\vec{\theta}_2) - h(\vec{\theta}_1) & = \transpose{\grad{h(\vec{\theta}_1)}{}} (\vec{\theta}_2 - \vec{\theta}_1) + \frac{1}{2} \transpose{(\vec{\theta}_2 - \vec{\theta}_1)} \hess{h(\vec{\theta}_1)}{} (\vec{\theta}_2 - \vec{\theta}_1)           \\
                                          & = -\eta \| \grad{h(\vec{\theta}_1)}{} \|^2 + \frac{1}{2} \transpose{(\vec{\theta}_2 - \vec{\theta}_1)} \hess{h(\vec{\theta}_1)}{} (\vec{\theta}_2 - \vec{\theta}_1) \margintag{Gradient descent update rule.} \\
                                          & \leq -\eta \| \grad{h(\vec{\theta}_1)}{} \|^2 + \frac{L}{2} \| \vec{\theta}_2 - \vec{\theta}_1 \|^2 \margintag{Spectral norm condition of smoothness.}                                                        \\
                                          & = -\eta \| \grad{h(\vec{\theta})_1}{} \|^2 + \frac{L \eta^2}{2} \| h(\vec{\theta})_1 \|^2 \margintag{Update rule of gradient descent.}                                                                        \\
                                          & = - \eta \lft( 1 - \frac{L \eta}{2} \rgt) \| \grad{h(\vec{\theta}_1)}{} \|^2.
    \intertext{A strict decrease in $h$ is guaranteed if $\eta < \nicefrac{2}{L}$, hence we choose $\eta = \nicefrac{1}{L}$,}
                                          & = -\frac{1}{2L} \| \grad{h(\vec{\theta}_1)}{} \|^2.
\end{align*}
As a result, we obtain sufficient decrease, \[
    h(\vec{\theta}_2) \leq h(\vec{\theta}_1) - \frac{1}{2L} \| \grad{h(\vec{\theta}_1)}{} \|^2.
\]

\begin{lemma}[Convergence of gradient descent on smooth functions]
    Let $h$ be $L$-smooth, then gradient descent with stepsize $\eta = \nicefrac{1}{L}$ will find an
    $\epsilon$-critical point ($\| \grad{h(\vec{\theta})}{} \| \leq \epsilon$) in at most \[
        T = \frac{2L}{\epsilon^2}( h(\vec{\theta}_0) - h(\vec{\theta}^{\star}) ).
    \]
\end{lemma}

\begin{proof}
    As shown, we have sufficient decrease, \[
        h(\vec{\theta}_t) - h(\vec{\theta}_{t+1}) \geq \frac{1}{2L} \| \grad{h(\vec{\theta}_t)}{} \|^2.
    \]
    Summing up both sides and telescoping the sum, we get \[
        h(\vec{\theta}_0) - h(\vec{\theta}^\star) \geq h(\vec{\theta}_0) - h(\vec{\theta}_T) \geq \frac{1}{2L} \sum_{t=0}^{T-1} \| \grad{h(\vec{\theta}_{t})}{} \|^2 \geq \frac{1}{2L} \min_{t=0}^{T-1} \| \grad{h(\vec{\theta}_t)}{} \|^2.
    \]
    Thus, we get \[
        \min_{t=0}^{T-1} \| \grad{h(\vec{\theta}_t)}{} \| \leq \sqrt{\frac{2L (h(\vec{\theta}_0) - h(\vec{\theta}^\star))}{T}} \overset{!}{\leq} \epsilon.
    \]
    The claim follows by solving for $T$ with this condition.
\end{proof}

\begin{definition}[PL-inequality]
    A function $h$ satisfies the PL-inequality with $\mu > 0$ if \[
        \frac{1}{2} \| \grad{h(\vec{\theta})}{} \|^2 \geq \mu(h(\vec{\theta}) - h(\vec{\theta}^\star)), \quad \forall \vec{\theta} \in \Theta.
    \]
\end{definition}

Intuitively, the PL-inequality means that if $\vec{\theta}$ has a small gradient, then
$\vec{\theta}$ is nearly optimal.

\begin{lemma}
    Let $h$ be differentiable, $L$-smooth, and $\mu$-PL. Then, gradient descent with stepsize $\eta = \nicefrac{1}{L}$ converges at a geometric rate, \[
        h\lft( \vec{\theta}_T \rgt) - h(\vec{\theta}^\star) \leq \lft( 1 - \frac{\mu}{L} \rgt)^T \lft( h\lft( \vec{\theta}_0 \rgt) - h(\vec{\theta}^\star) \rgt).
    \]
\end{lemma}

\begin{proof}
    We have
    \begin{align*}
        h \lft( \vec{\theta}_{t+1} \rgt) & \leq h \lft( \vec{\theta}_t \rgt) -\frac{1}{2L} \lft\| \grad{h\lft( \vec{\theta}_t \rgt)}{} \rgt\|^2 \margintag{Sufficient decrease.}         \\
                                         & \leq h \lft( \vec{\theta}_t \rgt) -\frac{\mu}{L} \lft( h \lft( \vec{\theta}_t \rgt) - h(\vec{\theta}^\star) \rgt). \margintag{PL-inequality.}
    \end{align*}
    Subtracting $h(\vec{\theta}^\star)$ from both sides yields \[
        h \lft( \vec{\theta}_t \rgt) - h(\vec{\theta}^\star) \leq \lft( 1 - \frac{\mu}{L} \rgt) \lft( h \lft( \vec{\theta}_t \rgt) - h(\vec{\theta}^\star) \rgt).
    \]
    The result follows from a trivial induction.
\end{proof}

\subsection{Acceleration, adaptivity, and momentum}

\paragraph{Acceleration.}

Nesterov acceleration is a method that achieves better theoretical guarantees than vanilla gradient
descent,
\begin{align*}
    \vec{\chi}^{t+1}   & = \vec{\theta}^t + \beta \lft( \vec{\theta}^t - \vec{\theta}^{t-1} \rgt) \margintag{Extrapolation step.} \\
    \vec{\theta}^{t+1} & = \vec{\chi}^{t+1} - \eta \grad{h \lft( \vec{\chi}^{t+1} \rgt)}{}. \margintag{Gradient descent step.}
\end{align*}

\paragraph{Momentum.}

The intuition behind momentum is that if the gradient is stable, gradient descent can make bolder
steps. A simple method making use of this is the Heavy Ball method, \[
    \vec{\theta}^{t+1} = \vec{\theta}^t - \eta \grad{h\lft( \vec{\theta}^t \rgt)}{} + \beta \lft( \vec{\theta}^t - \vec{\theta}^{t-1} \rgt), \quad \beta \in [0,1].
\]
Assuming a constant gradient $\vec{\delta}$, we have the following update, \[
    \vec{\theta}^{t+1} = \vec{\theta}^t - \eta \lft( \sum_{\tau=1}^{t-1} \beta^{\tau} \rgt) \vec{\delta}.
\]
Thus, we see that that the learning rate increases in the case of a constant gradient.

\paragraph{Adaptivity.}

In adaptivity, we realize that we want parameter-specific learning rates, since different
parameters behave differently. It defines the following, \[
    \gamma_i^t \doteq \gamma_i^{t-1} + \lft[ \partial_i h \lft( \vec{\theta}^t \rgt) \rgt]^2.
\]
We then have a parameter-specific update rule, \[
    \theta_i^{t+1} = \theta_i^t - \eta_i^t \partial_i h\lft( \vec{\theta}^t \rgt), \quad \eta_i^t \doteq \frac{\eta}{\sqrt{\gamma_i^t} + \delta}.
\]
Here, we see that the gradients of previous iterates influence the effective stepsize. Parameters
with small gradient magnitudes will be updated with an effectively larger step size.

\paragraph{Adam.}

Adam (\textit{\textbf{Ada}ptive \textbf{M}oment Estimation}) \citep{kingma2014adam} combines
adaptivity and momentum,
\begin{align*}
    \vec{g}_t      & = \beta \vec{g}_{t-1} + (1-\beta) \grad{h(\vec{\theta}_t)}{}, \quad \beta \in [0,1] \margintag{Moving average (smooth gradient estimator).}                                                    \\
    \vec{\gamma}_t & = \alpha \vec{\gamma}_{t-1} + (1-\alpha) \grad{h(\vec{\theta}_t)}{}^{\odot 2}, \quad \alpha \in [0,1]. \margintag{Exponential averaging (measure of stability in the optimization landscape).}
\end{align*}
The update rule is then \[
    \vec{\theta}_{t+1} = \vec{\theta}_t - \vec{\eta}_t \odot \vec{g}_t, \quad \vec{\eta}_t = \frac{1}{\sqrt{\vec{\gamma}_t} + \delta}.
\]

\subsection{Stochastic gradient descent}

When the dataset is too large, computing the full gradient is infeasible. Stochastic gradient
descent solves this by computing the gradient only \wrt a small batch of data points at each
timestep. One can prove that such an algorithm converges under certain conditions. In general, we
are forced to use SGD due to the size of modern datasets. However, practically SGD outperforms GD,
because it has a lower chance of getting stuck in a local optimum due to variance in the gradient
estimator.
