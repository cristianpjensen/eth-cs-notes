\section{Object detection}

In \textit{object detection}, we want to detect objects within an environment.
We are given an image as input, and the output should be a set of objects with
their location within the image.

\subsection{Detection via classification}

The most naive idea is to run a classification model on every patch within the
image. We can use all previously discussed idea for the representation of the
patches, such as a gradient historgram or pixel intensities.

\subsection{Boosting with weak classifiers}

An example of a weak classifier is the rectangle filter. It is essentially a
large gradient calculation, where we compute the sum of one patch minus the sum
of another next to it. We can compute these very quickly using a sliding window
approach, for which we only need to compute a cumulative sum over the integral
image in $\bigo{H\cdot W}$. Then, we can compute the sum of any patch in
$\bigo{1}$. We then classify the patch based on the difference of sums of the
patches.

This will result in a single very weak classifier, but if we combine a lot of
them, we can use boosting to combine them linearly to be a strong classifier.
This is fast, because each weak classifier is very simple. Boosting works by
iteratively finding the weak classifier that achieves the lowest weighted
training error (weights are initialized to be uniform). Then, we raise the
weights of training samples that were misclassified by the best weak
classifier. We repeat this, while collecting the best weak classifier every
iteration to get a subset of weak classifier that collectively are strong. We
weight each weak classifier by their accuracy on the training dataset.

This will result in a subset of weak classifiers that each might have a little
better than expected accuracy, but together they form a strong classifier that
has good accuracy.

In more details, AdaBoost \citep{freund1996experiments} runs for $M$
iterations, each resulting in a weak classifier. Each iteration $m$, it first
trains a new weak classifier $h_m$ that minimizes the weighted error function,
\[
  J_m = \sum_{i=1}^n w_i^{(m)} \mathbb{1} \{ h_m(x_i) \neq y_i \}
.\]
Then, it estaimtes the weighted error of this classifier on the dataset, \[
  \epsilon_m = \frac{1}{\sum_{i=1}^n w_i^{(m)}} \sum_{i=1}^n w_i^{(m)} \mathbb{1} \{ h_m(x_i) \neq y_i \}
.\]
Then, it calculates the weighting coefficient for $h_m(x)$, which is large if
the weak classifier is good, and otherwise small, \[
  \alpha_m = \log \lft( \frac{1 - \epsilon_m}{\epsilon_m} \rgt)
.\]
Lastly, it updates the weighting coefficients, \[
  w_i^{(m+1)} = w_i^{(m)} \exp\lft( \alpha_m \mathbb{1} \{ h_m(x_i) \neq t_i \} \rgt)
.\]
The final classifier is then the weighted linear combination, according to
$\alpha_m$, of the weak classifiers.

\subsection{Implicit shape model}

An implicit shape model (ISM) learns a star-topology structural model, which
means that features are considered independent given the center of the object.
It learns a visual codebook with displacement vectors that encode where the
center of the object must be that this visual word is a part of. For example,
we might encode that a car must have two visible wheels. Then, if two wheels
point at the same center with their displacement vectors, there is a car object
at that center.

\subsection{Learning-based approaches}

The difficulty in using learning-based approaches in object recognition is that
the output has a variable size, since an image may contain any number of
objects. Thus, we must design new models that can output a variable amount of
outputs. Also, there are an exponential amount of possible regions within an
image, which makes classifying every possible patch infeasible.

The solution to this is selective search \citep{uijlings2013selective}.
Selective search first segments the image into a large number of regions using
a graph-based segmentation technique. Then, it iteratively alternates between
collecting segmentation regions as region proposals, and combining segmentation
regions into larger ones. This results in a wide spectrum of region proposal
sizes. This enables learning-based approaches, because now we only need to
classify a small number of patches.

\paragraph{R-CNN.}

R-CNN \citep{girshick2014rich} works by first extracting approximately 2000
region proposals using selective search. Then, it warps the region proposals to
constant-sizes images patches and passes them to a classification network,
which classifies the region (CNN followed by support vector machine).

The problem with R-CNN is that the training and testing is still slow (47
seconds per test image).

TODO: R-CNN figure architecture figure.

\paragraph{Fast R-CNN.}

Fast R-CNN \citep{girshick2015fast} solves the problem of R-CNN by first
feeding the entire input image into a CNN to generate a feature map. Then, the
region proposals are identified within this feature map. This is then reshaped
to a constant-sized representation which is passed to a fully connected layer
that outputs a probability distribution over classifications. The reason for
that this model is faster is because we only need to run a single CNN, instead
of 2000 for each region proposal.

TODO: Fast R-CNN figure architecture figure.

\paragraph{Faster R-CNN.} 

The largest bottleneck in Fast R-CNN is the selective search, which is very
slow. Therefore \citet{ren2015faster} came up with Faster R-CNN that uses a
region proposal network (RPN) that lets the model learn the region proposals.
The RPN works by taking the feature map as input and outputting a bounding box
and ``objectness`` score.

This network has four loss functions that need to be optimized: Region proposal
``objectness`` classification loss, Region proposal bounding-box regression
loss, Region classification loss, and another bounding-box regression loss on
the eventual classification network.

TODO: Faster R-CNN figure architecture figure.

\paragraph{You only look once.}

All R-CNN approaches require a call to a classifcation network for every region
proposal. However, this slows down the algorithm. YOLO \citep{redmon2016you}
predicts bounding boxes and class probabilities in a single pass.
It works by dividing the input image into an $S\times S$ grid. If the center of
an object falls into a grid cell, that grid cell is responsible for detecting
that object. Each grid cell then predicts $B$ bounding boxes ($(x,y,w,h,c)$,
where $c$ is the confidence score), and $C$ conditional class
probabilities.\sidenote{Thus, the output of the model has dimensionality
$S\times S \times (5B+C)$.} It then picks the boxes with the greatest
confidence and removes boxes if their IoU is lower than some threshold.

YOLO is much faster than the R-CNN approaches with a real-time performance of
approximately 45 frames per second. A minor problem is that it struggles with
small objects and it imposes strong spatial constraints (since each grid cell
can only have one class prediction), which limits the number of nearby objects
it can predict.
