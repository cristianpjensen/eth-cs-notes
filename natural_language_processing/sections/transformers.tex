\section{Transformers} \label{sec:transformers}

Attention is a mechanism in neural networks that a model can learn to make
predictions by selectively attending to a given set of data by using query
$\vec{q}$, key $\vec{k}$, and value $\vec{v}$ vector representations. The query
and key vectors are used to determine how much weight should be given to the
value vector.\sidenote{Note the parallel with dictionaries/hashmaps in
programming languages, but, in the attention mechanism, we do a
``soft-lookup``.} The weights are computed by
$\mat{a}_{ij}=\softmax(\transpose{\vec{q}_j}\vec{k}_i)$, so the values after the
attention block can be computed as follows, \[
  \text{att}(\vec{x}_i) = \sum_{j} \vec{a}_{ij} \vec{v}_i
,\]
which is a linear combination of the values according to the attention weights
computed by the query and keys.

\begin{marginfigure}
    \centering
    \incfig{attention}
    \caption{Self-attention mechanism.}
    \label{fig:attention}
\end{marginfigure}

Self-attention blocks learn the query, key, and value representations from
data. More specifically, it learns matrices $\bm{W}_Q$, $\bm{W}_K$, and
$\bm{W}_V$ and computes the vectors from these matrices:
\begin{align*}
  \vec{q}_i &= \transpose{\mat{W}_Q} \vec{x}_i \\
  \vec{k}_i &= \transpose{\mat{W}_K} \vec{x}_i \\
  \vec{v}_i &= \transpose{\mat{W}_V} \vec{x}_i
.\end{align*}
Then, we can use these to compute the output of the self-attention block: \[
  \text{self-att}(\mat{X}) = \softmax \lft( \frac{\transpose{(\transpose{\mat{W}_Q} \mat{X})}(\transpose{\mat{W}_K}\mat{X})}{\sqrt{d_q}} \rgt) \transpose{\mat{W}_V}\mat{X}
,\]
where $d_q$ is the square root of the dimensionality of the query and key
vectors. Furthermore, we need to add a positional encoding to provide ordering
information to the model.\sidenote{The self-attention operation is permutation
equivariant.} This is done by a sinusoidal positional encoding and are simply
combined with $\mat{X}$ by addition.

\begin{marginfigure}
    \centering
    \incfig{transformer}
    \caption{Transformer encoder architecture.}
    \label{fig:transformer}
\end{marginfigure}

Transformers \citep{vaswani2017attention} use multi-headed self-attention,
which is a module where self-attention is applied $M$ times independently to
the data. Thus, this module learns $M$ different ways of looking at the same
dataset. The outputs of each self-attention block is concatenated and linearly
transformed to the expected dimensionality. Transformers follow this by
normalization and MLP layers, as can be seen in \Cref{fig:transformer}.

\subsection{Translation}

Translation is a sequence-to-sequence problem, where we want to compute the
probability that $\vec{y}$ is the translation of $\vec{x}$. We can do this with
transformers by encoding the input sequence $\vec{x}$ using encoders, and
feeding this representation of the input to a decoder. The decoder takes as
input the input sequence and the already generated (incomplete) sequence
$\vec{y}_{<i}$. It then runs the encoder as in \Cref{fig:transformer} and
projects the output to a probability distribution over tokens using a linear
layer, followed by softmax.

\begin{figure}[ht]
    \centering
    \incfig{translation-architecture}
    \caption{Translation architecture}
    \label{fig:translation-architecture}
\end{figure}

In previous problem statements, we were usually able to compute the globally
maximum output using some dynamic program. However, in this case, that is not
possible. This model is only locally normalized. But, we can do something more
optimal than greedily picking the next token. We can view this problem as a
graph search problem over possible output sentences. We still cannot explore
every path of the $\bigo{|\Sigma|^{N_{\max}}}$ paths, but we use beam search.
Beam search keeps track of a maximum of $k$ paths at any given time. Then, it
prunes paths that are the worst. This is still not globally optimal, but it is
a bit better than the greedy approach.
