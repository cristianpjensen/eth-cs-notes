\section{Backpropagation} \label{sec:backpropagation}

\textit{Backpropagation} is the single most important algorithm in modern
machine learning, because it is used to compute gradients of composite
functions efficiently. It is a linear-time dynamic program for computing
derivatives, \ie, it stores intermediate results to be as fast as forward
propagation.

In machine learning, most of the time, we have inputs $\vec{x}\in\mathcal{X}$
and outputs $\vec{y}\in\mathcal{Y}$ from a dataset
$\mathcal{D}\subseteq\mathcal{X}\times\mathcal{Y}$, and we want to fit some
function $f_{\vec{\theta}}: \mathcal{X} \to \mathcal{Y}$ such that it minimizes
some loss function, \[
  \sum_{(\vec{x},\vec{y})\in\mathcal{D}} \ell(f_{\vec{\theta}}(\vec{x}),\vec{y})
.\]

We need this function's gradient to be able to use an algorithm such as
gradient descent to optimize it.\sidenote{Most of the time, this function
cannot be solved in closed form.} For a composite function, it is time
consuming to derive the gradient by hand. Thus, we use backpropagation to
automatically compute the gradients, as long as we have access to the
derivatives of its primitive functions.

\begin{example}
  \caption{Computation graph.}
  \label{ex:comp-graph}
  A composite function can be represented using intermediate variables such
  that each variable is computed by a single primitive function. Let's say we
  have the following function, \[
    f(x,y)=\sin(xy + \exp(y))
  .\]

  Then, we can represent the intermediate variables as the following,
  \begin{align*}
    z_1 &= xy \\
    z_2 &= \exp(y) \\
    z_3 &= z_1 + z_2 \\
    z_4 &= \sin(z_3)
  .\end{align*}

  \incfig{computation-graph}
\end{example}

We could also describe a function as a labeled, directed acyclic\sidenote{The
fact that our computation graph is acyclic makes it possible for
backpropagation to be linear.} hypergraph\sidenote{A hypergraph allows the
edges to have multiple sources and targets. This is needed because functions
can have multiple inputs and multiple outputs.}, where each node is a variable
and each hyperedge is labeled with a function. Given a function
$f:\R^n\to\R^m$, $\vec{y}=f(\vec{x})\in\R^m$, for input $x_j$ and outputs
$y_i$, Bauer's formula gives
\begin{equation} \label{eq:bauer}
  \frac{\partial y_i}{\partial x_j} = \sum_{p\in P(j,i)} \prod_{(k,l)\in p} \frac{\partial z_l}{\partial z_k}
,\end{equation}
where $P(j,i)$ is the set of paths from vertex $j$ to vertex $i$ and $p\in
P(j,i)$ is the set of edges that make up the path $p$. \Ie, the partial
derivative is a sum over all paths in the computation graph, where the
derivative over each path is computed by the chain
rule.\sidenote{$\odv*{f(g(x))}{x}= \odv*{f(g(x))}{g(x)} \cdot \odv*{g(x)}{x}$.} When
computing the partial derivatives for functions with dense computation graphs
naively, we are typically summing over an exponential number of paths, because
many of these partial derivatives are recomputed many times. We can use dynamic
programming to store these values and avoid computing them again. In this case,
the amount of computation scales linearly with the number of edges.

\begin{algorithm}
  \caption{Forward propagation algorithm that assumes that the edges are
  topologically sorted so $i<j$ implies that $z_i$ is computed before $z_j$.}
  \label{alg:forward-propagation}

  \begin{algorithmic}[1]
    \Function{ForwardPropagation}{$f, \bm{x}$}
      \State $z_i \gets \begin{cases}
        x_i & \textbf{if } i \leq m \\
        0 & \textbf{otherwise}
      \end{cases}$ \Comment{Initialize input variables}
      \For {$i=m+1,\ldots,n$}
        \State $z_i \gets g_i(\vec{z}_{\mathrm{Parents}(i)})$ \Comment{Set intermediate variables}
      \EndFor
      \State \Return $\bm{z}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Backpropagation algorithm that assumes that the edges are
  topologically sorted so $i<j$ implies that $z_i$ is computed before $z_j$.}
  \label{alg:backpropagation}

  \begin{algorithmic}[1]
    \Function{Backpropagation}{$f,\bm{x}$}
      \State $\bm{z} \gets \Call{ForwardPropagation}{f,\bm{x}}$
      \State $\frac{\partial f}{\partial z_i} \gets \begin{cases}
        1 & \textbf{if } i=n \\
        0 & \textbf{otherwise}
      \end{cases}$ \Comment{Base case}
      \For {$i=n-1,\ldots,1$} \Comment{$O(n)$}
        \State $\pdv{f}{z_i} \gets \sum_{z_p\in\mathrm{Parents}(z_i)} \pdv{f}{z_p} \pdv{z_p}{z_i}$ \Comment{Chain rule}
      \EndFor
      \State \Return $\nabla_{\bm{x}} f$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

The general framework that any backpropagation framework uses is the following,
\begin{enumerate}
  \item Write down a composite function as a hypergraph with intermediate
    variables as nodes and hyperedges labeled with primitive functions;
  \item Given a set of inputs, perform forward propagation through the graph to
    compute the function's value (\Cref{alg:forward-propagation});
  \item Run backpropagation on the graph using the stored forward values
    (\Cref{alg:backpropagation}). Intuitively, we set up a dynamic programming
    table using \Cref{eq:bauer}.
\end{enumerate}

\begin{example}
  \caption{Backpropagation computation for the function in \Cref{ex:comp-graph}.}
  \begin{align*}
    \pdv{f}{z_4} &= 1 \\
    \pdv{f}{z_3} &= \pdv{f}{z_4} \pdv{z_4}{z_3} = \cos(z_3) \\
    \pdv{f}{z_2} &= \pdv{f}{z_3} \pdv{z_3}{z_2} = \cos(z_3) \\
    \pdv{f}{z_1} &= \pdv{f}{z_3} \pdv{z_3}{z_1} = \cos(z_3) \\
    \pdv{f}{x} &= \pdv{f}{z_1} \pdv{z_1}{x} = \cos(z_3) y \\
    \pdv{f}{y} &= \pdv{f}{z_1} \pdv{z_1}{y} + \pdv{f}{z_2} \pdv{z_2}{z_1} = \cos(z_3) x + \cos(z_3) \exp(y)
  .\end{align*}
\end{example}
