\section{Constituency parsing} \label{sec:constituency_parsing}

\begin{figure*}[h!]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \Tree [.\cat{S}
      [.\cat{NP} \smallcaps{I} ]
      [.\cat{VP}
        [.\cat{VP}
          [.\cat{V} \smallcaps{shot} ]
          [.\cat{NP}
            [.\cat{Det} \smallcaps{an} ]
            [.\cat{N} \smallcaps{elephant} ]
          ]
        ]
        [.\cat{PP}
          [.\cat{P} \smallcaps{in} ]
          [.\cat{NP}
            [.\cat{Det} \smallcaps{my} ]
            [.\cat{N} \smallcaps{pajamas} ]
          ]
        ]
      ]
    ]
    \caption{The shooter is wearing the pajamas.}
    \label{fig:ambiguous-tree1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \Tree [.\cat{S}
      [.\cat{NP} \smallcaps{I} ]
      [.\cat{VP}
        [.\cat{V} \smallcaps{shot} ]
        [.\cat{NP}
          [.\cat{Det} \smallcaps{an} ]
          [.\cat{N} \smallcaps{elephant} ]
          [.\cat{PP}
            [.\cat{P} \smallcaps{in} ]
            [.\cat{NP}
              [.\cat{Det} \smallcaps{my} ]
              [.\cat{N} \smallcaps{pajamas} ]
            ]
          ]
        ]
      ]
    ]
    \caption{The elephant is wearing the pajamas.}
    \label{fig:ambiguous-tree2}
  \end{subfigure}
  \label{fig:ambiguous-sentence}
  \caption{Two possible constituency trees of the ambiguous sentence ``I shot an
  elephant in my pajamas.``}
\end{figure*}

\begin{figure}[ht]
\end{figure}

A parse tree is a hierarchy of constituents, where a constituent is a
multi-word unit that functions as a single unit. Each constituent encapsulates
all of its leaf descendants, which are the words of the sentence. We say that a
tree yields the sentence that can be found on its leaves. However, language is
ambiguous, so some sentences have multiple trees that yield it. The goal is to
compute the best parse tree that yields a given natural language input
sentence.

\subsection{Context-free grammars}

Intuitively, a grammar defines a set of sentences that are deemed
grammatical. Any sentence that can be yielded by a tree that consists of
rules defined by the grammar is deemed grammatical.

\begin{definition}[Context-free grammar]
  A context-free grammar (CFG) is a 4-tuple $G=\langle \mathcal{N}, \cat{S},
  \Sigma,\mathcal{R} \rangle$, such that
  \begin{enumerate}
    \item $\mathcal{N}$ is a set of non-terminal symbols, written as uppercase
      letters $\cat{N}_1, \cat{N}_2, \ldots$;
    \item $\cat{S} \in \mathcal{N}$ is a distinguished start non-terminal.
      Every complete parse tree must have this symbol at its root;
    \item $\Sigma$ is an alphabet of terminal symbols, written as lowercase
      letters $a_1,a_2,\ldots$;
    \item $\mathcal{R}$ is a set of production rules of the following form, \[
        \cat{N} \to \bm{\alpha}
      ,\]
      where $\cat{N}\in\mathcal{N}$ and $\bm{\alpha}\in (\mathcal{N} \cup
      \Sigma)^*$.
  \end{enumerate}
\end{definition}

A \textit{context-free grammar} (CFG) encodes a subset of $\Sigma^*$, where a
sentence is only part of the subset if we can construct a tree from
$\mathcal{R}$ that yields the sentence, starting from \cat{S}.

\subsection{Parsing}

We might be able to assign multiple trees to a single sentence. To be able to
pick the best tree, we can assign a probability to each rule, and pick the
tree with the highest probability.

\begin{definition}[Probabilistic context-free grammar]
  A probabilistic CFG is a 5-tuple $G=\langle \mathcal{N}, S,
  \Sigma,\mathcal{R}, p \rangle$, where $p: \mathcal{R}\to [0,1]$ is a locally
  normalized probability distribution over rules. The probability of a tree
  under a PCFG is defined as follows, \[
    p(\bm{t} \mid s) = \prod_{r\in\bm{t}} p(r)
  .\]
\end{definition}

Instead of probabilities, we could also, more generally, assign weights to each
production rule. The score of assigning a tree $\vec{t}$ to a sentence
$\vec{w}$ then decomposes over the production rules, \[
  \score(\vec{t},\vec{w}) \doteq \sum_{r\in\vec{t}} \score(r)
,\]
where a tree $\vec{t}$ is simply a multiset of rules. However, we run into the
problem that the normalizer $Z(\vec{w})$ will diverge if the ruleset contains a
cycle rule, \eg, $\cat{N}\to\cat{N}$.

\begin{definition}[Chomsky normal form]
  A grammar is in Chomsky normal form if all rules are of the following form,
  \begin{align*}
    \cat{N}_1 &\to \cat{N}_2 \cat{N}_3 \\
    \cat{N} &\to a
  .\end{align*}
\end{definition}

\begin{theorem}[CNF theorem]
  For any grammar $G$, we can find another grammar $G'$ that accepts the same
  set of strings and probabilities as $G$ and is in CNF.
\end{theorem}

A CFG in \textit{Chomsky normal form} (CNF) does not contain any cyclic rules,
since they are not allowed by the permitted rule forms. Thus, the normalizer
can no longer diverge, since there are not an infinite amount of trees that
yield the same string $\vec{w}$. Furthermore, the CNF theorem guarantees that
we can create all the same CFGs in CNF as if we did not constrain them to be in
CNF.

\subsection{Cocke-Kasami-Younger algorithm}

\begin{figure}[h!]
    \centering
    \incfig{cky-chart}
    \caption{The CKY chart of the ambiguous sentence ``I shot an elephant in my
    pajamas.`` See \Cref{fig:ambiguous-sentence} for the resulting trees.}
    \label{fig:cky-chart}
\end{figure}
\vfill
\begin{algorithm}[h!]
  \caption{Semiringified CKY algorithm.}
  \label{alg:cky}

  \begin{algorithmic}[1]
    \Function{WeightedCKY}{$\vec{w},\langle \mathcal{N},\cat{S},\Sigma,\mathcal{R} \rangle, \score$}
      \State {$N \gets |\vec{w}|$}
      \State {$\tens{C} \gets \tens{0}$} \Comment{Chart}
      \For {$i=1,\ldots,N$}
        \For {$\cat{X} \to w_i \in \mathcal{R}$}
          \State {$\tens{C}[i, i+1, \cat{X}] \gets \tens{C}[i, i+1, \cat{X}] \oplus \exp{\score(\cat{X} \to w_i)}$}
        \EndFor
      \EndFor

      \LComment{Compute score of every span in $\bigo{N^3\cdot |\mathcal{R}|}$}

      \For {$\ell=2,\ldots,N$}
        \For {$i=1,\ldots,N-\ell+1$}
          \State {$k \gets i + \ell$}
          \For {$j=i+1,\ldots,k-1$}
            \For {$\cat{X} \to \cat{Y}\cat{Z} \in \mathcal{R}$}
              \State {$\tens{C}[i,k,\cat{X}] \gets \tens{C}[i,k,\cat{X}] \oplus \exp \score(\cat{X} \to \cat{Y}\cat{Z}) \otimes \tens{C}[i,j,\cat{Y}] \otimes \tens{C}[j,k,\cat{Z}]$}
            \EndFor
          \EndFor
        \EndFor
      \EndFor

      \State \Return {$\tens{C}[1,N+1,\cat{S}]$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Despite there not being an infinite amount of trees in CNF form, there are
still an exponential amount of trees. Thus, we need to design an algorithm to
efficiently compute the normalizer. The Cocke-Kasami-Younger (CKY)
\citep{cocke1969programming,kasami1966efficient,younger1967recognition}
algorithm provides an efficient dynamic program to compute the normalizer
$Z(\vec{w})$ of CFGs in CNF. It works by looking at iteratively larger spans,
and the subtrees that make up these spans, since a span from $i$ to $j$ can
only be made up of smaller spans within this span. \Eg, in
\Cref{fig:cky-chart}, \cat{PP} covers the subtrees with root \cat{P} that
covers $\smallcaps{in}$ and \cat{NP}, which covers $\smallcaps{my pajamas}$.
Furhter see \Cref{fig:cky-chart} for an illustration of how the algorithm works
fully. It starts from the bottom of the chart and works its way up using
dynamic programming.

This algorithm runs in $\bigo{N^3\cdot |\mathcal{R}|}$, where $N$ is the length
of the sentence. We can semiringify this algorithm to compute the best parse,
where the Viterbi semiring finds the maximally scoring tree, and the real
semiring computes the normalizer.
