\section{Generative adversarial networks}

So far, we have only seen generative models that optimize the likelihood. This has a nice
interpretation and leads to nice theory, but there are cases where optimizing the likelihood will
not give good results.

In the first case, we might have a good likelihood score, but poor samples. Let's say we have a
model $p$ that generates high-quality samples and a model $q$ that generates noise. Now construct
the mixture model $0.01 p + 0.99 q$.\sidenote{This means that $99\%$ of the time, the generated
    samples will be noise.} The log-likelihood of this model is then \[
    \log(0.01 p(\vec{x}) + 0.99 q(\vec{x})) \geq \log (0.01 p(\vec{x})) = \log(p(\vec{x})) - \log 100.
\]
The term $\log p(\vec{x})$ will be proportional to the dimensionality $d$, while $\log 100$ remains
constant. For high-dimensional data, this results in a high log-likelihood for the mixture model.

In the second case, we might have a low likelihood score with high-quality samples. This occurs
when the model overfits on the training data, meaning that it only outputs data points from the
training dataset. This will result in low log-likelihood on the validation dataset, despite the
samples being high-quality.

\textit{Generate adversarial networks} (GAN) solve this by introducing a \textit{discriminator} ($D$),
whose job it is to differentiate between real and fake images. The objective of the
\textit{generator} ($G$) is then to maximize the discriminator's classification loss by generating
images similar to the dataset.

Specifically, the generator $G: \R^d \to \R^n$ maps a simple $d$-dimensional distribution to a
sample from the data distribution. The discriminator $D: \R^n \to [0,1]$ assigns a probability to
samples. Its objective is to assign probability 1 to samples from the dataset and probability 0 to
samples generated by the generator.

This leads us to the following value function, \[
    V(D,G) = \log D(\vec{x}) + \log (1 - D(G(\vec{z}))), \quad \vec{x} \in \mathcal{D}, \vec{z} \sim p_{\mathrm{prior}}(\cdot).
\]
The discriminator aims to maximize it, while the generator aims to minimize it, which gives the
following optimization problem, \[
    G^\star, D^\star \in \argmin_G \argmax_D V(D,G).
\]

\subsection{Theoretical analysis}

\paragraph{Optimal discriminator.}

Given access to $p_{\mathrm{model}}$ and $p_{\mathrm{data}}$,\sidenote{In practice, we do not have
    access to these distributions.} we can compute a closed form solution for $D^\star$ by the
following theorem,

\begin{theorem}
    For any generator $G$ that induces $p_{\mathrm{model}}$, the optimal discriminator is \[
        D^\star(\vec{x}) = \frac{p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})}.
    \]
\end{theorem}

\begin{proof}
    Let $G$ be a generator, then $D$ is computed by
    \begin{align*}
        D^\star & = \argmax_D V(G,D)                                                                                                                                                                                                       \\
                & = \argmax_D \E_{\vec{x} \sim p_{\mathrm{data}}}[\log D(\vec{x})] + \E_{\vec{z} \sim p_{\mathrm{prior}}}[\log(1 - D(G(\vec{z})))]                                                                                         \\
                & = \argmax_D \int_{\vec{x}} p_{\mathrm{data}}(\vec{x}) \log D(\vec{x}) \mathrm{d}\vec{x} + \int_{\vec{z}} p_{\mathrm{prior}}(\vec{z}) \log(1 - D(G(\vec{z}))) \mathrm{d}\vec{z}                                           \\
                & = \argmax_D \int_{\vec{x}} p_{\mathrm{data}}(\vec{x}) \log D(\vec{x}) \mathrm{d}\vec{x} + \int_{\vec{x}} p_{\mathrm{model}}(\vec{x}) \log(1 - D(\vec{x})) \mathrm{d}\vec{z} \margintag{Law of unconscious statistician.} \\
                & = \argmax_D \int_{\vec{x}} p_{\mathrm{data}}(\vec{x}) \log D(\vec{x}) + p_{\mathrm{model}}(\vec{x}) \log(1 - D(\vec{x})) \mathrm{d}\vec{x}.
    \end{align*}

    Let $a, b > 0$ and consider $f(y) = a \log(y) + b \log(1-y)$, then $f$'s maximum is achieved at $y
        = \frac{a}{a+b}$, which we can easily prove by
    \begin{align*}
        f'(y)  & = \frac{a}{y} - \frac{b}{1-y} = 0 \iff y = \frac{a}{a+b} \margintag{Critical point.}        \\
        f''(y) & = -\frac{a}{y^2} - \frac{b}{(1-y)^2} < 0, \quad \forall a,b > 0. \margintag{Maximum point.}
    \end{align*}

    Thus, we obtain \[
        D^\star(\vec{x}) = \frac{p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})}.
    \]
\end{proof}

\paragraph{Global optimality.}

Now that we have found the optimal theoretical value for the discriminator, we want to know when we
have found a global optimum of the value function. \Ie, when is $G^\star$ obtained.

\begin{theorem}
    \label{thm:generator-optimality}
    The generator is optimal if $p_{\mathrm{model}} = p_{\mathrm{data}}$ and at optimum, we have \[
        V(G^\star, D^\star) = -\log 4.
    \]
\end{theorem}

\begin{proof}
    We have already found the optimal $D^\star$, thus we substitute it into the value function,
    \begin{align*}
        V(G, D^\star) & = \E_{\vec{x} \sim p_{\mathrm{data}}} \lft[ \log \lft( \frac{p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt]                                                                          \\
                      & \quad\quad + \E_{\vec{z} \sim p_{\mathrm{prior}}} \lft[ \log \lft( 1 - \frac{p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt]                                                          \\
                      & = \E_{\vec{x} \sim p_{\mathrm{data}}} \lft[ \log \lft( \frac{p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt]                                                                          \\
                      & \quad\quad + \E_{\vec{z} \sim p_{\mathrm{prior}}} \lft[ \log \lft( \frac{p_{\mathrm{model}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt]                                                             \\
                      & = \E_{\vec{x} \sim p_{\mathrm{data}}} \lft[ \log \lft( \frac{2 p_{\mathrm{data}}(\vec{x})}{2 (p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x}))} \rgt) \rgt]                                                                    \\
                      & \quad\quad + \E_{\vec{z} \sim p_{\mathrm{prior}}} \lft[ \log \lft( \frac{2 p_{\mathrm{model}}(\vec{x})}{2 (p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x}))} \rgt) \rgt]                                                       \\
                      & = \E_{\vec{x} \sim p_{\mathrm{data}}} \lft[ \log \lft( \frac{2 p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt] - \log 2                                                               \\
                      & \quad\quad + \E_{\vec{z} \sim p_{\mathrm{prior}}} \lft[ \log \lft( \frac{2 p_{\mathrm{model}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt] - \log 2                                                  \\
                      & = \E_{\vec{x} \sim p_{\mathrm{data}}} \lft[ \log \lft( \frac{2 p_{\mathrm{data}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt]                                                                        \\
                      & \quad\quad + \E_{\vec{z} \sim p_{\mathrm{prior}}} \lft[ \log \lft( \frac{2 p_{\mathrm{model}}(\vec{x})}{p_{\mathrm{data}}(\vec{x}) + p_{\mathrm{model}}(\vec{x})} \rgt) \rgt] - \log 4                                                  \\
                      & = D_{\mathrm{KL}} \lft( p_{\mathrm{data}} \middle\lVert \frac{p_{\mathrm{data}} + p_{\mathrm{model}}}{2} \rgt) + D_{\mathrm{KL}} \lft( p_{\mathrm{model}} \middle\lVert \frac{p_{\mathrm{data}} + p_{\mathrm{model}}}{2} \rgt) - \log 4 \\
                      & = 2 D_{\mathrm{JS}}(p_{\mathrm{data}} \lVert p_{\mathrm{model}}) - \log 4,
    \end{align*}
    where the Jensen-Shannon divergence is a symmetric and smoothed version of the KL divergence, defined as \[
        D_{\mathrm{JS}}(p \lVert q) \doteq \frac{1}{2} D_{\mathrm{KL}}\lft(p \middle\lVert \frac{p + q}{2} \rgt) + \frac{1}{2} D_{\mathrm{KL}}\lft( q \middle\lVert \frac{p + q}{2} \rgt).
    \]
    This divergence is non-negative, and equals 0 if and only if $p = q$. Thus, $G^\star \in \argmin_G
        V(D^\star, G)$ must satisfy $p_{\mathrm{data}} = p_{\mathrm{model}}$, and we obtain \[
        V(G^\star, D^\star) = - \log 4.
    \]
\end{proof}

\paragraph{Convergence.}

\begin{theorem}
    \label{thm:gan-convergence}

    Assume that $G$ and $D$ have sufficient capacity, at each update step $D \to D^\star$, and
    $p_{\mathrm{model}}$ is updated to improve
    \begin{align*}
        V(p_{\mathrm{model}}, D^\star) & = \E_{\vec{x} \sim p_{\mathrm{data}}}[\log D^\star(\vec{x})] + \E_{\vec{x} \sim p_{\mathrm{model}}}[\log (1 - D^\star(\vec{x}))] \margintag{Notice that $p_{\mathrm{model}}$ is updated directly here, rather than indirectly by optimizing $G$, which is actually what happens.} \\
                                       & \propto \sup_D \int_{\vec{x}} p_{\mathrm{model}}(\vec{x}) \log (1 - D(\vec{x})) \mathrm{d}\vec{x}.
    \end{align*}
    Then, $p_{\mathrm{model}}$ converges to $p_{\mathrm{data}}$.
\end{theorem}

\begin{proof}
    The argument of the supremum is convex in $p_{\mathrm{model}}$ and the supremum preserves
    convexity. Thus, $V(p_{\mathrm{model}}, D^\star)$ is convex in $p_{\mathrm{model}}$ with global
    optimum as in \Cref{thm:generator-optimality}.
\end{proof}

However, \Cref{thm:gan-convergence} is a very weak result, because of how strong the assumptions
are. In practice, $G$ and $D$ have finite capacity, $D$ is optimized for only $k$ steps and does
not converge to $D^\star$, and due to the neural network parametrization of $G$, the objective is
no longer convex. However, despite this, GANs work well in practice, because $D$ does stay close to
$D^\star$, providing meaningful gradients for $G$ to optimize its generations.

\subsection{Training}

\begin{algorithm}
    \begin{algorithmic}[1]
        \While{not converged}
        \RepeatN{$k$}
        \State $\vec{x}_1, \ldots, \vec{x}_n \sim p_{\mathrm{data}}$
        \State $\vec{z}_1, \ldots, \vec{z}_n \sim p_{\mathrm{prior}}$
        \State $\mathcal{L}_D = \frac{1}{n} \sum_{i=1}^{n} \log (D(\vec{x}_i)) + \log(1 - D(G(\vec{z}_i)))$
        \State {perform a gradient ascent step on $\mathcal{L}_D$}
        \End

        \State $\vec{z}_1, \ldots, \vec{z}_n \sim p_{\mathrm{prior}}$
        \State $\mathcal{L}_G = \frac{1}{n} \sum_{i=1}^{n} \log(1 - D(G(\vec{z}_i)))$
        \State {perform a gradient descent step on $\mathcal{L}_G$}
        \EndWhile
    \end{algorithmic}
    \caption{Generative adversarial network training algorithm.}
    \label{alg:gan}
\end{algorithm}

A possible issue with this training algorithm is that, early in learning, $G$ is poor, which means
that $D$ can easily reject samples with high confidence. In this case, $\log(1 - D(G(\vec{z})))$
saturates, meaning that it approaches $-\infty$ as $D(G(\vec{z})) \to 1$. Instead, we can train $G$
to maximize $\log D(G(\vec{z}))$, which does not have this problem.

\paragraph{Mode collapse.}

We speak of \textit{mode collapse} when the generator learns to produce high-quality samples with
very low variability, covering only a fraction of the data distribution. A simple example that
explains this phenomenon is a generator that generates temperature values. The generator may learn
to only output cold temperatures, which the discriminator counters by predicting all cold
temperatures as ``fake'' and all warm temperatures as ``real''. Then, the generator exploits this
by only generating warm temperatures. And, again the discriminator can counter this, which the
generator counters, repeating cyclically.

The most common solution to mode collapse is the \textit{unrolled GAN} \citep{metz2017unrolled}.
The idea is to optimize the generator \wrt the last $k$ discriminators. This results in the above
not being able to occur, since the generator must not only fool the current discriminator, which
might be unstable, but also the previous $k$ ones.

\paragraph{Training instability.}

Since we optimize GANs as a two-player game, we need to find a Nash-Equilibrium, where, for both
players, moving anywhere will only be worse than the equilibrium. However, this can lead to
training instabilities, since making progress for one player may mean the other player being worse
off.

\paragraph{Optimizing Jensen-Shannon divergence.}

It might be the case that the supports of $p_{\mathrm{data}}$ and $p_{\mathrm{model}}$ are
disjoint. In this case, it is always possible to find a perfect discriminator with $D(\vec{x}) = 1,
    \forall \vec{x} \in \mathrm{supp}(p_{\mathrm{data}})$ and $D(\vec{x}) = 0, \forall \vec{x} \in
    \mathrm{supp}(p_{\mathrm{model}})$. Then, the loss function equals zero, meaning that there will be
no gradient to update the generator's parameters. A possible solution is to optimize a different
metric than the Jensen-Shannon divergence. \Eg, optimizing the Wasserstein distance does not fall
to zero in this case, since it measures divergence by how different they are horizontally, rather
than vertically. Intuitively, it measures how much ``work'' it takes to turn one distribution into
the other.
