\section{Structure from motion} \label{sec:sfm}

In \textit{Structure from Motion} (SfM), we want to recover the 3-dimensional
structure from $m$ images that depict $n$ 3-dimensional points. Recovering the
structure involves estimating the camera matrices, $\mat{P}_1, \ldots,
    \mat{P}_m$, and the positions of the points $\vec{X}_1, \ldots, \vec{X}_n$. The
fact that we have more than 2 viewpoints enables us to reveal and remove more
mismatches.

\begin{figure}
    \centering
    \incfig{structure-from-motion}
    \caption{Structure from motion. We iteratively extend and refine the
        recovered structure.}
    \label{fig:structure-from-motion}
\end{figure}

In \textit{sequential SfM}, we initialize the 3-dimensional structure from
two images, using the same technique as in the previous section. Then, for each
additional view, make a reconstruction with all previous views. In the new
view, there are old points that we had already reconstructed, but also new
points that are new to the reconstruction. We refine the old points by
considering the new view, and we extend the structure by adding the new points.

Afterward, we refine the recovered structure. We use \textit{bundle
    adjustment}, which is a non-linear method that minimizes the sum of squared
reprojection errors, \[
    \sum_{i=1}^m \sum_{j=1}^n d \lft( \vec{x}_{ij}, \mat{P}_i \hat{\mat{X}_j} \rgt)^2.
\]

In general, bundle adjustment involves a large amount of parameters. However,
there are many methods for solving this problem efficiently.

\subsection{Robust estimation}

When optimizing the geometric error when backprojecting, the squared error can
be a source of bias for outliers. One possible solution is to use an
M-estimator that gives more weight to small errors and less weight to large
errors. For example, we can use the robust norm function $\rho$ to give
outliers less influence over the fit, \[
    \rho(\epsilon; \sigma) = \frac{\epsilon^2}{\sigma^2 + \epsilon^2}.
\]
Choosing the correct scale $\sigma$ is critical. A popular choice is $1.4826$,
which is plotted in \Cref{fig:robust-norm}.

\begin{marginfigure}
    \begin{tikzpicture}
        \begin{axis}[
                width=\textwidth,
                samples=300,
                xlabel=$\epsilon$,
                domain=-12:12,
                xmin=-10,
                xmax=10,
                scale only axis,
            ]
            \addplot [mark=none,color=black,thick] {x^2/(x^2+(1.4826)^2)};
        \end{axis}
    \end{tikzpicture}

    \caption{Robust norm function $\rho(\epsilon;\sigma)$ with $\sigma=1.4826$.}
    \label{fig:robust-norm}
\end{marginfigure}

Another solution is to use the \textit{random sample consensus} (RANSAC)
algorithm to robustly compute the best fit. For $k$ iterations, the algorithm
computes a minimum model with $n$ data points. It then checks how many
``inliers`` this model has, which, intuitively, means how many points does this
model explain well enough. If the amount of inliers is above the threshold $d$,
we fit the model to all inliers, and compute the error on all points. We return
the model with the lowest error on its inliers.

\begin{algorithm}
    \begin{algorithmic}
        \Function{RANSAC}{$\mathcal{P}$,$n$,$k$,$t$,$d$}
        \State {$\epsilon_{\min} \gets \infty$}
        \State {initialize $\mathcal{M}_{\mathrm{best}}$}
        \RepeatN {$k$}
        \State {$\mathcal{Q} \gets \text{$n$ points randomly sampled from $\mathcal{P}$}$}
        \State {$\mathcal{M}_{\mathrm{random}} \gets \Call{FitModel}{\mathcal{Q}}$}
        \State {$\mathcal{I} \gets \emptyset$}

        \For {$p \in \mathcal{P}$} \Comment{Compute inliers}
        \If {$\Call{Error}{\mathcal{M}_{\mathrm{random}}, p} \leq t$}
        \State {$\mathcal{I} \gets \mathcal{I} \cup \{ p \}$}
        \EndIf
        \EndFor

        \If {$|\mathcal{I}| \geq d$}
        \State {$\mathcal{M}_{\mathrm{inliers}} \gets \Call{FitModel}{\mathcal{I}}$}
        \State {$\epsilon \gets \Call{Error}{\mathcal{M}_{\mathrm{inliers}}, \mathcal{I}}$}
        \If {$\epsilon < \epsilon_{\min}$}
        \State {$\mathcal{M}_{\mathrm{best}} \gets \mathcal{M}$}
        \EndIf
        \EndIf
        \End
        \EndFunction
    \end{algorithmic}
    \caption{Random sample consensus algorithm.}
\end{algorithm}

Let $w$ be the probability of a point being an in inlier. Then, $w^n$ is the
probability that all $n$ points used to fit the model are inliers. $1-w^n$ is
the probability that at least one of the points is not an inlier, which would
result in a bad model. $(1-w^n)^k$ is the probability that RANSAC does not
select a single subset with all inliers. If we let $p$ be the probability that
RANSAC will return a good model, then we have the following identity, \[
    1-p = (1-w^n)^k,
\]
which means that if we want probability $p$ to get a good model, then we can
compute the necessary $k$ as follows, \[
    k = \frac{\log(1-p)}{\log(1-w^n)}.
\]
