\section{Stable diffusion}

\begin{figure}
    \centering
    \incfig{stable-diffusion-architecture}
    \caption{Architecture of the stable diffusion denoising model.}
    \label{fig:stable-diffusion-architecture}
\end{figure}

\subsection{Diffusion models}

An SDE (\textit{\textbf{S}tochastic \textbf{D}ifferential \textbf{E}quation}) is a differential
equation in which one or more terms is a stochastic process, resulting in a solution that is also
stochastic. Typically, the SDE of a diffusion process is of the following form, \[
    \mathrm{d}\vec{x}_t = \vec{\mu}(\vec{x}_t, t) \mathrm{d}t + \sigma(\vec{x}_t, t) \mathrm{d}\omega_t,
\]
where $\omega_t$ is a Wiener process (or Brownian motion).\sidenote{Wiener process:
    $\mathrm{d}\omega_t \sim \mathcal{N}(\vec{0}, \mathrm{d}t \mat{I})$.} This equation tells us that
the change in $\vec{x}_t$ is driven by a deterministic factor $\vec{\mu}(\vec{x}_t,t)$ and a
stochastic factor $\sigma(\vec{x}_t, t) \mathrm{d}\omega_t$. Note that $\vec{\mu}(\cdot, \cdot)$
and $\sigma(\cdot, \cdot)$ induce a probability distribution over time, $p_t$ of $\vec{x}_t$.

\citet{anderson1982reverse} showed that the reverse SDE of the diffusion process can be computed as follows, \[
    \mathrm{d}\vec{x}_t = \lft[ \vec{\mu}(\vec{x}_t, t) - \sigma^2(\vec{x}_t, t) \grad{\log p_t(\vec{x}_t)}{\vec{x}} \rgt] \mathrm{d}t + \sigma(\vec{x}_t, t)\mathrm{d}\bar{\omega}_t,
\]
where $\bar{\omega}_t$ is a standard Wiener process when time flows backwards from $T$ to $0$, and
$\mathrm{d}t$ is an infinitesimal negative timestep. Thus, in order to do the backward process, we
need to learn the score function $\grad{\log p_t(\vec{x})}{\vec{x}}$ somehow.

Using the DDPM scheduler, diffusion models have the following forward process, \[
    \vec{x}_{t+1} = \sqrt{1-\beta_t} \vec{x}_t + \sqrt{\beta_t} \vec{\epsilon}, \quad \vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I}).
\]
Conditioned on $\vec{x}_t$, we can reconstruct $\vec{x}_{t-1}$ as follows with a predicted noise
$\vec{\epsilon}_{\vec{\theta}}$,
\begin{equation}
    \label{eq:denoising}
    \vec{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \lft( \vec{x}_t - \frac{1-\alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \vec{\epsilon}_{\vec{\theta}}(\vec{x}_t, t) \rgt) + \sqrt{1-\alpha_t} \vec{z}, \quad \vec{z} \sim \mathcal{N}(\vec{0}, \mat{I}),
\end{equation}
where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{\tau=1}^t \alpha_t$. Thus, we need to
learn this function.

It can be shown that a diffusion model with the DDPM scheduler is an approximation of a
discretization of the following SDE, \[
    \mathrm{d}\vec{x}_t = -\frac{1}{2}\beta_t \vec{x}_t \mathrm{d}t + \sqrt{\beta_t} \mathrm{d}\omega_t. \margintag{Drift term $\vec{\mu}(\vec{x}_t, t) \doteq -\frac{1}{2}\beta_t \vec{x}_t$; diffusion term $\sigma(t) \doteq \sqrt{\beta_t}$. We can show that this approximates the diffusion model by discretizing, \begin{align*} \vec{x}_{t+1} - \vec{x}_t & = -\frac{1}{2}\beta_t \vec{x}_t + \sqrt{\beta_t} \vec{\epsilon}, \quad \vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I}) \\ \vec{x}_{t+1} & = \lft( 1 - \frac{1}{2} \beta_t \rgt) \vec{x}_t + \sqrt{\beta_t} \vec{\epsilon} \\ & \approx \sqrt{1-\beta_t} \vec{x}_t + \sqrt{\beta_t} \vec{\epsilon}. \end{align*}}
\]
The reverse process is thus given by the following reverse SDE, \[
    \mathrm{d}\vec{x}_t = \lft[ -\frac{1}{2} \beta_t \vec{x}_t - \beta_t \grad{\log p(\vec{x}_t)}{\vec{x}} \rgt] \mathrm{d}t + \sqrt{\beta_t} \mathrm{d}\bar{\omega}_t.
\]

In practice, we train a diffusion model by randomly sampling $\vec{x}_0 \sim p_0, t \sim
    \mathrm{Unif}([T]), \vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I})$ and performing a gradient
step on the following loss function, \[
    \ell = \lft\| \vec{\epsilon} - \vec{\epsilon}_{\vec{\theta}}(\vec{x}_t) \rgt\|^2. \margintag{$\vec{x}_t = \sqrt{\bar{\alpha}_t} \vec{x}_0 + \sqrt{1-\bar{\alpha}_t} \vec{\epsilon}$, where $\bar{\alpha}_t = \prod_{\tau=1}^t (1-\beta_{\tau})$.}
\]
We can sample by iteratively denoising using \Cref{eq:denoising}, starting from $\vec{x}_T \sim
    \mathcal{N}(\vec{0}, \mat{I})$. It can be shown that $\vec{\epsilon}_t \approx \grad{\log
        p_t(\vec{x}_t)}{\vec{x}}$.

\subsection{U-net}

U-nets \citep{ronneberger2015u} are models used for image-to-image translation tasks. In the case
of diffusion models, we have such a task, where we get $\vec{x}_t$ as input and want to predict
$\vec{\epsilon}$. This framework is used to model $\vec{\epsilon}_{\vec{\theta}}$. U-nets work by
downsampling the input in stages and then upsampling back to the original space in the same stages.
At every step of upsampling, the output of the corresponding downsampling step is concatenated to
its input. As such, each decoding step receives low-level and high-level information as input.

\subsection{Latent diffusion models}

Stable diffusion \citep{rombach2022high} performs diffusion modeling in the latent space of a
pretrained VAE \citep{kingma2013auto}. During training, we thus first map the input image
$\vec{x}_0 \in \R^d$ to its latent encoding, \[
    \vec{z}_0 = \bm{\mathcal{E}}(\vec{x}_0), \quad \vec{z}_0 \in \R^{d'}, \quad d' \ll d.
\]
Then, we use the same loss function as above, where we sample a random timestep and noise, \[
    \ell = \lft\| \vec{\epsilon} - \vec{\epsilon}_{\vec{\theta}}(\vec{z}_t) \rgt\|^2, \quad \vec{\epsilon} \sim \mathcal{N}(\vec{0}, \mat{I}_{d'}).
\]
Then, during inference, we sample a noise vector in the latent space $\vec{z}_T \sim
    \mathcal{N}(\vec{0}, \mat{I}_{d'})$ and denoise using \Cref{eq:denoising} to get
$\tilde{\vec{z}}_0$. Lastly, we decode the latent vector back into pixel space, \[
    \tilde{\vec{x}}_0 = \bm{\mathcal{D}}(\tilde{\vec{z}}_0).
\]
The reconstruction term of this VAE is most important, because the model will be searching through
the latent space anyway. Stable diffusion uses a latent space that is downsampled $8$ times and has
$4$ channels. This results in significant savings on computation time.

\subsection{Text embeddings}

In order to perform text-to-image generation, we will need a continuous high-dimensional
representation of the input text. For this, we use CLIP \citep{radford2021learning}. CLIP trains
image and text transformer models to align text-image pairs. Because of this, they contain more
semantic embeddings than other methods of training models to obtain text embeddings. Given a text
input sequence of size $T_c$, the CLIP text model returns a sequence of embeddings, \[
    \mat{C} \in \R^{T_c \times d_c}. \margintag{We denote this matrix by $\mat{C}$, because we use it for conditioning.}
\]
This sequence is contextualized, because CLIP makes use of self-attention.

\subsection{Cross-attention}

Now the question becomes how to condition a U-net on the input text sequence $\mat{C} \in \R^{T_c
        \times d_c}$. Stable diffusion \citep{rombach2022high} does this by making use of cross-attention
blocks, which it places at the end of downsampling and upsampling stages of the U-net. It first
rearranges the output of the downsampling block into timesteps, \[
    \mat{X} \in \R^{T \times d}.
\]
Then, it computes queries from $\mat{X}$, and keys and values from $\mat{C}$,
\begin{align*}
    \mat{Q} & = \mat{X} \mat{W}_Q, \quad \mat{W}_Q \in \R^{d_k \times d}    \\
    \mat{K} & = \mat{C} \mat{W}_K, \quad \mat{W}_K \in \R^{d_k \times d_c}  \\
    \mat{V} & = \mat{C} \mat{W}_V, \quad \mat{W}_V \in \R^{d_v \times d_c}.
\end{align*}
It uses these to perform the attention mechanism with a residual connection, \[
    \mat{\Xi} = \mat{X} + \mathrm{softmax} \lft( \frac{\mat{Q} \transpose{\mat{K}}}{\sqrt{d_k}} \rgt) \mat{V},
\]
where $\mat{\Xi}$ denotes the conditioned representation of $\mat{X}$. Note that this architecture
is agnostic to the type of the condition. As long as we can embed the conditioning variable, we can
condition on it in this way---\eg, we can additionally condition on images \citep{ye2023ip}.
