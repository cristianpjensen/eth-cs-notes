\section{Variational inference} \label{sec:vi}

\begin{remark}
  From now on, $\mat{X}$ will be omitted and treated as a constant.
\end{remark}

In Bayesian learning, we want to compute the following probability
distribution to make predictions given the data, \[
  p(y^\star \mid \vec{x}^\star, \vec{y})  = \int p(y^\star\mid\vec{x}^\star,\theta)p(\theta\mid\vec{y})d\theta
,\]
where we marginalize over all possible models $\theta$. Furthermore, we
compute the posterior as follows, \[
  p(\theta\mid\vec{y}) = \frac{1}{Z}p(\theta)\prod_{i=1}^n p(y_i\mid \vec{x}_i,\theta)
.\]
However, in general, these equations are intractable.\sidenote{The integral is
intractable, because distributions are not conjugate in general. The posterior
is intractable, because of the normalizer $Z$.} Gaussian processes solved this
problem by assuming that the prior and likelihood are
Gaussian.\sidenote{Because the Guassian has a conjugate prior.} But, in some
cases, it is not realistic to assume Gaussian distributions.\sidenote{\Eg in
logistic regression, the likelihood is modeled by a Bernoulli distribution.}
\textit{Variational inference} solves this problem by approximating the
intractable distribution $p$ by a simpler one $q$ that is ``as close as
possible``, \[
  p(\theta\mid\vec{y}) = \frac{1}{Z} p(\theta,\vec{y}) \approx q(\theta\mid\lambda) = q_\lambda (\theta)
,\]
where $\lambda$ are called the variational parameters. Thus, we have reduced
the problem to optimization, \ie, maximizing the similarity between
distributions $p$ and $q$, where $q$ is part of a variational family
$\mathcal{Q}$ that is easy to work with.

\begin{example}
  \caption{Laplace approximation.}

  A simple way of approximating intractable integrals is \textit{Laplace
  approximation} which is a Gaussian approximation to the posterior. Let's
  define a function $\psi(\theta)\doteq\log p(\theta\mid \vec{y})$, then the
  Laplace approximation of $\psi$ can be computed from the second-order Taylor
  expansion around the posterior mode,
  \begin{align*}
    \psi(\theta) &\approx \psi(\hat{\theta}) + (\theta - \hat{\theta})^\top \nabla \psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta})^\top \bm{H}_{\psi}(\hat{\theta}) (\theta - \hat{\theta}) \\
    &= \psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta})^\top \bm{H}_{\psi}(\hat{\theta}) (\theta - \hat{\theta})
  .\end{align*}
  Then,
  \begin{align*}
    p(\theta\mid\vec{y}) &= \exp(\psi(\theta)) \\
    &\approx \exp\lft(\psi(\hat{\theta}) + \frac{1}{2} (\theta-\hat{\theta})^\top \bm{H}_{\psi}(\hat{\theta}) (\theta - \hat{\theta})\rgt) \\
    &= \exp (\psi(\hat{\theta})) \cdot \exp\lft(\frac{1}{2} (\theta-\hat{\theta})^\top \bm{H}_{\psi}(\hat{\theta}) (\theta - \hat{\theta})\rgt) \\
    &= \frac{1}{Z} \cdot \exp\lft(-\frac{1}{2} (\theta-\hat{\theta})^\top \bm{H}^{-1}_{\psi}(\hat{\theta}) (\theta - \hat{\theta})\rgt) \\
    &= \mathcal{N}(\theta ; \hat{\theta}, \bm{\Lambda}^{-1}) \\
    &\doteq q(\theta)
  ,\end{align*}
  where $\hat{\theta}=\argmax_{\theta} p(\theta\mid\vec{y})$ and $\bm{\Lambda} =
  -\bm{H}_{\theta} \log p(\theta\mid\vec{y})$

  \vspace{1em}

  Intuitively, the Laplace approximation matches the shape of the true
  posterior around its mode, but may not represent it accurately elsewhere.
  Often, this leads to extremely overconfident predictions.
\end{example}

\subsection{Training}

When training, we want to find the distribution in the variational family
$\mathcal{Q}$ that minimizes the KL divergence with $p$,\sidenote[][-2.5cm]{When
approximating $p$ with $q$, we can either minimize the reverse or the forward
KL divergence. The reverse KL divergence $KL(q\| p)$ typically acts more
greedily and places most of its mass where $p$ has a lot of mass, while the
forward KL divergence $KL(p \| q)$ tries to cover most of the probability mass
of $p$. Thus, the forward KL divergence is more desirable, but it requires
sample from $p$, which is intractable (the whole reason we are doing this).
Thus, we have to resort to using the reverse KL divergence.}
\begin{align*}
  q^\star &= \argmin_{q_\lambda\in\mathcal{Q}} KL(q_\lambda \| p) \\
  &= \argmin_\lambda \int q_\lambda(\theta) \log \frac{q_\lambda(\theta)}{p(\theta\mid\vec{y})} d\theta \\
  &= \argmin_\lambda \int q_\lambda(\theta) \log \frac{q_\lambda(\theta)}{\frac{1}{Z}p(\vec{y}\mid\theta)p(\theta)} d\theta \\
  &= \argmax_\lambda \int q_\lambda(\theta) (\log p(\vec{y}\mid\theta) + \log p(\theta) - \log Z - \log q_\lambda(\theta)) d\theta \\
  &= \argmax_\lambda \int q_\lambda(\theta) \lft(\log p(\vec{y}\mid\theta) - \log \frac{q_\lambda(\theta)}{p(\theta)} \rgt) d\theta \\
  &= \argmax_\lambda \int q_\lambda(\theta) \log p(\vec{y}\mid\theta) - \int q_\lambda(\theta) \log \frac{q_\lambda(\theta)}{p(\theta)} d\theta \\
  &= \argmax_\lambda \underbrace{\E_{\theta \sim q_\lambda}[\log p(\vec{y}\mid\theta)]}_{\text{expected likelihood}} - \underbrace{KL(q_\lambda \| p_{\text{prior}})}_{\text{``stay close to prior``}}
.\end{align*}
Thus, minimizing $KL(q_\lambda \| p)$ is equivalent to maximizing the expected
likelihood, while remaining close to the prior distribution.

To show that minimizing the KL divergence is an adequate method of model
selection, we will show that it lower bounds the evidence,
\begin{align*}
  \log p(\vec{y}) &= \log \int p(\vec{y},\theta) d\theta & \text{\footnotesize sum rule} \\
  &= \log \int q_\lambda(\theta) \frac{p(\vec{y},\theta)}{q_\lambda(\theta)} d\theta \\
  &= \log \E_{\theta\sim q_\lambda} \lft[ \frac{p(\vec{y},\theta)}{q_\lambda(\theta)} \rgt] \\
  &\geq \E_{\theta\sim q_\lambda} \lft[ \log \frac{p(\vec{y},\theta)}{q_\lambda(\theta)} \rgt] & \text{\footnotesize Jensen's inequality} \\
  &= \E_{\theta\sim q_\lambda} \lft[ \log \frac{p(\vec{y}\mid \theta) p(\theta)}{q_\lambda(\theta)} \rgt] \\
  &= \E_{\theta\sim q_\lambda} [ \log p(\vec{y}\mid \theta) ] - KL(q_\lambda \| p_{\text{prior}})
.\end{align*}
Thus, minimizing \[
  L(\lambda) = \E_{\theta\sim q_\lambda} [ \log p(\vec{y}\mid \theta) ] - KL(q_\lambda \| p_{\text{prior}})
\]
can safely be used to find an appropriate model for the data, since a lower
bound on the evidence gets maximized.

However, there is one problem: we want to compute the gradient of an
expectation \wrt $q$, but $q$ depends on $\lambda$. Thus, we cannot compute
the gradient in its current form. To solve this we use the reparameterization
trick.

\begin{definition}[Reparameterization trick]
  Suppose we have a random variable $\epsilon \sim \phi$ sampled from a
  base distribution, and consider $\theta\doteq g(\epsilon,\lambda)$ for some
  invertible function $g$. Then, the following holds: \[
    \E_{\theta\sim q_\lambda} [f(\theta)] = \E_{\epsilon\sim\phi}[f(g(\epsilon,\lambda))]
  .\]
  Thus, after reparameterization, the expectation is \wrt to distribution
  $\phi$ that does not depend on $\lambda$. Thus, we can compute the gradient
  $\nabla_\lambda L(\lambda)$.
\end{definition}

\begin{example}
  \caption{Reparameterization trick for Gaussians.}

  Suppose we use a Gaussian variational approximation, \[
    q_\lambda(\theta) \doteq \mathcal{N}(\theta;\bm{\mu},\bm{\Sigma})
  .\]
  Then, we can reparameterize $\theta = g(\bm{\epsilon},\lambda) =
  \bm{\Sigma}^{1/2} \bm{\epsilon} + \bm{\mu}$, where $\phi =
  \mathcal{N}(\bm{0},\bm{I})$, because \[
    \theta = \bm{\Sigma}^{\nicefrac{1}{2}} \bm{\epsilon} + \bm{\mu} \sim \mathcal{N}(\bm{\mu},\bm{\Sigma}^{\nicefrac{1}{2}}\bm{I}\bm{\Sigma}^{1/2\top}) = \mathcal{N}(\bm{\mu},\bm{\Sigma})
  .\]
\end{example}

\subsection{Inference}

To perform inference using the variational approximation, we need to compute
the integral over all models $\theta$,\sidenote{The key insight here is that
parameters $\theta$ do not matter, only their results $f^\star$.}
\begin{align*}
  p(y^\star\mid\vec{x}^\star, \vec{y}) &= \int p(y^\star\mid\vec{x}^\star,\theta) p(\theta\mid \vec{y}) d\theta \\
  &\approx \int p(y^\star\mid \vec{x}^\star, \theta) q_{\lambda}(\theta) d\theta \\
  &= \int \int p(y^\star\mid \vec{x}^\star,\theta,f^\star) q_\lambda (\theta\mid f^\star) p(f^\star\mid \vec{x}^\star, \theta) d\theta df^\star & \text{\footnotesize marginalize over $f^\star$} \\
  &= \int \int p(y^\star\mid f^\star) p(f^\star\mid \vec{x}^\star, \theta) q_\lambda (\theta) d\theta df^\star \\
  &= \int p(y^\star\mid f^\star) \int p(f^\star\mid \vec{x}^\star, \theta) q_\lambda (\theta) d\theta df^\star \\
  &= \int p(y^\star\mid f^\star) q_\lambda (f^\star\mid \vec{x}^\star) df^\star & \text{\footnotesize marginalize out $\theta$}
.\end{align*}
Thus, we have reduced the high-dimensional integral over the parameters
$\theta$ to a one-dimensional integral over $f^\star$. While this integral is
generally still intractable, it can be approximated efficiently.
