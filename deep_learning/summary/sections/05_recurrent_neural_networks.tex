\section{Recurrent neural networks}

Typically, networks cannot process variable-sized data, such as sequences. Further, convolutional
networks constrain the range of the dependencies between timesteps of a sequence, and linear layers
would explode in the number of parameters. RNNs (\textit{\textbf{R}ecurrent \textbf{N}eural
    \textbf{N}etworks}) process the data sequentially, where each timestep depends on its entire
history. Let $\vec{x}^1, \ldots, \vec{x}^T$ denote the observed input sequence, RNNs compute the
sequence of activations recursively, \[
    \vec{z}^t \doteq F[\vec{\theta}]\lft( \vec{z}^{t-1}, \vec{x}^t \rgt), \quad \vec{z}^0 = \vec{0}.
\]
Dependent on the application, we can compute output variables from these activations, \[
    y^t \doteq G[\vec{\varphi}]\lft( \vec{z}^t \rgt).
\]
For example, in same length sequence-to-sequence prediction, $y^t$ will denote the output token at
the $t$-th timestep; in autoregressive modeling, $y^t$ predicts the next input token $x^{t+1}$; and
in sequence classification, the final output $y^T$ predicts the classification of the entire
sequence.

The simplest RNN architecture is the Elman RNN \citep{elman1990finding},
\begin{align*}
    F[\mat{U}, \mat{V}](\vec{z}, \vec{x}) & = \phi(\mat{U} \vec{z} + \mat{V} \vec{x}), \quad \mat{U} \in \R^{m \times m}, \mat{V} \in \R^{m \times n} \\
    G[\mat{W}](\vec{z})                   & = \psi(\mat{W} \vec{z}), \quad \mat{W} \in \R^{q \times m}.
\end{align*}
However, this model has difficulties modeling large-range dependencies, as will become apparent from the gradients.

Let \[
    L \doteq \sum_{t=1}^{T} \ell^t, \quad \ell^t \doteq \ell \lft( \hat{\vec{y}}^t, \vec{y}^t \rgt).
\]

\begin{align*}
    \pdv{L}{w_{ki}} & = \sum_{t=1}^{T} \pdv{\ell^t}{y^t_k} \psi'\lft( \transpose{\vec{w}_k} \vec{z}^t \rgt) z^t_i                                             \\
    \pdv{L}{v_{ij}} & = \sum_{t=1}^{T} \pdv{\ell^t}{z^t_i} \phi' \lft( \transpose{\vec{w}}_i \vec{z}^{t-1} + \transpose{\vec{v}_i} \vec{x}^t \rgt) x_j^t      \\
    \pdv{L}{u_{ij}} & = \sum_{t=1}^{T} \pdv{\ell^t}{z^t_i} \phi' \lft( \transpose{\vec{w}}_i \vec{z}^{t-1} + \transpose{\vec{v}_i} \vec{x}^t \rgt) z_j^{t-1}.
\end{align*}

Backpropagation through time \citep{werbos1990backpropagation} for gradient computation...
Gradient...

Instability of BPTT by showing that it will likely explode or vanish...

Bidirectional RNNs \citep{schuster1997bidirectional} to capture full sequence at every timestep...

Stacked RNNs \citep{joulin2015inferring}... Alternatively, replace $F$ by a deep MLP...

\subsection{Gated memory}

Long-range dependencies are hard to memorize for the Elman RNN due to the instability of the
gradient... Idea: Avoid short-term fluctuations by more directly controlling when memory is kept
and when it is overwritten... LSTM \citep{schmidhuber1997long} and GRU \citep{cho2014learning}...
Gating... \[
    \vec{z} = \sigma(\mat{G} \vec{\zeta}) \odot \vec{z}.
\]
Now learning involves understanding what new and old information is relevant for later timesteps...

LSTM...
\begin{align*}
    \vec{z}^t     & = \sigma \lft( \mat{F} \tilde{\vec{x}}^t \rgt) \odot \vec{z}^{t-1} + \sigma \lft( \mat{G} \tilde{\vec{x}}^t \rgt) \odot \tanh \lft( \mat{V}\tilde{\vec{x}}^t \rgt), \quad \tilde{\vec{x}}^t = \lft[ \vec{\zeta}^{t-1}, \vec{x}^t \rgt] \\
    \vec{\zeta}^t & = \sigma \lft( \mat{H} \tilde{\vec{x}}^t \rgt) \odot \tanh \lft( \mat{U} \vec{z}^t \rgt).
\end{align*}
GRU...
\begin{align*}
    \vec{z}^t         & = \vec{\sigma} \odot \vec{z}^{t-1} + (1 - \vec{\sigma}) \odot \vec{\zeta}^t, \quad \vec{\sigma} = \sigma \lft( \mat{G} \tilde{\vec{x}}^t \rgt), \tilde{\vec{x}}^t = \lft[ \vec{z}^{t-1}, \vec{x}^t \rgt] \\
    \tilde{\vec{z}}^t & = \tanh \lft( \mat{V} \lft[ \vec{\zeta}^t \odot \vec{z}^{t-1}, \vec{x}^t \rgt] \rgt)                                                                                                                     \\
    \vec{\zeta}^t     & = \sigma \lft( \mat{H} \lft[ \vec{z}^{t-1}, \vec{x}^t \rgt] \rgt).
\end{align*}
$\vec{\zeta}^t$ can be computed implicitly...

\subsection{Linear recurrent models}

Linear recurrent models \citep{feng2024were}... \[
    \vec{z}^t = \vec{\sigma} \odot \vec{z}^{t-1} + (1-\vec{\sigma}) \odot \mat{V} \vec{x}^t, \quad \vec{\sigma} = \sigma \lft( \mat{G} \vec{x}^t \rgt).
\]
This allows for prefix scan parallelism, which allows for $\bigo{\log n}$ runtime, instead of
$\bigo{n}$.

Diagonalize over complex numbers... Parameterize such that it will not explode or vanish...

The advantage of such a simple recurrence unit is that it provides a clean understanding of long
range and short range dependencies, there is no requirement for mixing of channels, and
parallelization during training. Furthermore, we do not lose any representational power, because we
can move all power to the output map. The resulting model is provably universal as a
sequence-to-sequence map.

\subsection{Sequence learning}

Step-by-step prediction... \[
    p \lft( \vec{y}^{1:T} \mid \vec{x}^{1:T} \rgt) = \prod_{t=1}^T p \lft( y^t \mid \vec{x}^{1:T}, \vec{y}^{1:t-1} \rgt).
\]
Sequence-to-sequence mapping \citep{sutskever2014sequence} is usually done by mapping the input
sequence to a latent representation, \[
    \vec{x}^1, \ldots, \vec{x}^T \mapsto \vec{\zeta},
\]
which can be computed by an encoder RNN. Then, at every timestep, we compute a latent
representation of everything generated until now, \[
    \vec{x}^{1}, \ldots, \vec{x}^{t-1} \mapsto \vec{z}^{t-1}.
\]
These are then combined to compute a distribution over next tokens, \[
    \vec{z}^{t-1} \mapsto \vec{\mu}^t, \quad y^t \sim p\lft( \vec{\mu}^t \rgt).
\]
Teacher forcing \citep{williams1989learning}...

Lossy compression of input sequence... Want to look back at the input sequence while generating
output sequence... Attention \citep{bahdanau2014neural}! Also provides intuition...

