\section{Reinforcement learning} \label{sec:rl}

\begin{marginfigure}
    \centering
    \incfig{reinforcement-learning}
    \caption{In reinforcement learning, an agent interacts with its environment.
    After playing an action $a_t$, it observes reward $r_t$ and its new state
    $x_{t+1}$.}
    \label{fig:reinforcement-learning}
\end{marginfigure}

In \textit{reinforcement learning} (RL), we are concerned with acting in
unknown environments. These environments are still modeled by MDPs, but in RL,
we do not have access to the transition probabilities $p$ and reward function
$r$. Thus, RL is at the intersection of probabilistic planning (MDPs) and
learning, \ie, everything we have learned thus far comes together here.

\begin{remark}
  We will start by assuming that the state-action space is finite. Then, we will
  move on to potentially infinite state spaces. After that, we will learn about
  infinite action spaces.
\end{remark}

Since the environment is unknown, we need to explore the state-action space to
find where the reward lies. However, we also want to act optimally by
exploiting what we have learned thus far. This is called the
exploration/exploitation dilemma and is what algorithms need to solve.

Another way that reinforcement learning differs from supervised learning is
that data depends on past actions. Trajectory data looks like the following, \[
  \tau = (\langle x_0, a_0, r_0, x_1 \rangle, \langle x_1, a_1, r_1, x_2 \rangle, \ldots)
.\]

We differentiate between RL algorithms in two major ways. The first is whether
the algorithm has control over its data: an algorithm is called
\textit{on-policy} if it controls its own actions from which it learns, and
\textit{off-policy} if it can learn from any data. Furthermore, we
differentiate between \textit{model-based} and \textit{model-free} algorithms.
Model-based algorithms learn the underlying MDP and solve it using value or
policy iteration. Model-free algorithms only learn the value function, since,
due to Bellman's theorem, that is all that is needed to act optimally.

\begin{table}[t]
  \caption{RL algorithms covered in this text with their types.}
  \label{tab:rl-algos}
  \centering
  \begin{tabular}[c]{llll}
    \toprule
    \textbf{Algorithm} & \multicolumn{2}{c}{\textbf{Classification}} & \textbf{Space compl.} \\
    \midrule
    $\epsilon$-greedy & On/off-policy & Model-based & $\bigo{|\mathcal{A}| \cdot |\X|^2}$ \\
    $R_{\max}$ & On/off-policy & Model-based & $\bigo{|\mathcal{A}| \cdot |\X|^2}$ \\
    TD-learning & On-policy & Model-free & $\bigo{|\X|}$ \\
    Q-learning & Off-policy & Model-free & $\bigo{|\mathcal{A}| \cdot |\X|}$ \\
    Deep Q Network & Off-policy & Model-free \\
    REINFORCE & On-policy & Model-free \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Model-based}

In model-based RL, we learn the MDP, \ie, we estimate the transition
probabilities $p(x'\mid x,a)$ and reward function $r(x,a)$ from the data,
\begin{align*}
  \hat{p}(x'\mid x, a) &= \frac{\mathrm{count}(x' \mid x, a)}{\mathrm{count}(x, a)} \\
  \hat{r}(x, a) &= \frac{1}{\mathrm{count}(x,a)} \sum_{\substack{t=0 \\ x_t = x \\ a_t = a}}^\infty r_t
\end{align*}
Then, we optimize the policy by value or policy iteration, based on the
estimated MDP.

\paragraph{$\epsilon$-greedy.}

\marginnote{A sequence $x_t$ satisfies the Robbins Monro conditions if \[
  \sum_{t=0}^\infty x_t = \infty, \hspace{1em} \sum_{t=0}^\infty x_t^2 < \infty .\]
\Eg, $x_t = \frac{1}{t}.$}

At iteration $t$, pick random action with probability $\epsilon_t$, or best
action (according to internal MDP) with probability $1-\epsilon_t$. Guaranteed
to converge to optimal policy if the $\epsilon_t$ sequence satisfies the
Robbins Monro conditions. The advantage of this method is that it is extremely
simple and has a clear interpretation \wrt the exploration-exploitation
dilemma. The disadvantage is that it does not quickly eliminate clearly
suboptimal actions. This is because it explores the state space in an
uninformed manner. In other words, it explores while ignoring all past
experience.

\paragraph{$R_{\max}$ algorithm.}

$R_{\max}$ solves the problem of $\epsilon$-greedy by using the
\textit{Optimism in the face of uncertainty} principle. It assumes that any
unexplored states are ``fairy tale`` states with high reward. More formally, if
$r(x,a)$ is unknown, we set $\hat{r}(x,a)=R_{\max}$. Similarly, if $p(x'\mid
x,a)$ is unknown, we set $\hat{p}(x^\star\mid x,a)=1$ for some ``fairy tale`` state,
\begin{align*}
  \hat{p}(x^\star\mid x^\star, a) &= 1 & \forall a \in \mathcal{A} \\
  \hat{r}(x^\star, a) &= R_{\max} & \forall a \in \mathcal{A}
.\end{align*}
This gives us an algorithm that has a bias toward exploring, but once it has
explored a part of the state-action space, and observed it to be suboptimal, it
can quickly eliminate it. Furthermore, the algorithm does not have to
explicitly choose between exploration and exploitation, because it is done by
assuming that the unexplored states are optimal.

\subsection{Model-free}

The problem with model-based RL is that it has high space requirements for
storing the MDP, \ie, $\bigo{|\mathcal{A}|\cdot |\mathcal{X}|^2}$. Furthermore,
it requires repeatedly solving the underlying MDP, which is expensive with
policy or value iteration. In \textit{model-free} RL, we estimate the value
function directly, because that is all we need to act optimally, according to
Bellman's theorem. Thus, we also do not need to do any planning, eliminating
much computational complexity.

\paragraph{Temporal difference-learning.}

TD-learning directly computes the value function. Recall the Bellman
expectation equation, 
\begin{align*}
  V^\pi(x) &= r(x,\pi(x)) + \gamma \sum_{x'\in\X} p(x'\mid x,\pi(x)) V^\pi(x'). \\
  \intertext{Since we do not have access to $r$ and $p$, we have to make a
  Monte Carlo estimate given a single data point $\langle x,a,r,x' \rangle$,}
  &\approx r + \gamma V^\pi(x')
.\end{align*}
The idea is that we make this approximation repeatedly as the agent collects
new data, which achieves the same effect as averaging over many data points.
However, there is still a problem: $V^\pi$ depends on the unknown $V^\pi$.

The key idea is to use a bootstrapping estimate of the value function. In other
words, instead of the true value function $V^\pi$, we will use a running
estimate $\hat{V}^\pi$. However, due to relying on a single sample, the value
function will have a high variance, which is why we mix the new estimate with
the previous one using a learning rate $\alpha_t$, \[
  \hat{V}^\pi(x) \gets (1-\alpha_t) \hat{V}^\pi(x) + \alpha_t(r + \gamma \hat{V}^\pi(x'))
.\]
If the learning rate $\alpha_t$ satisfies the Robbins Monro conditions and all
states are chosen infinitely often, $\hat{V}^\pi$ is guaranteed to converge to
the optimal value function $V^\pi$.

Note that, due to the Monte Carlo approximation \wrt transitions attained by
following policy $\pi$, TD-learning is a fundamentally on-policy method.
Further note that the space requirement of this algorithm is $\bigo{|\X|}$.

\paragraph{Q-learning.}

A generalization of TD-learning is Q-learning. Instead of directly learning the
value function, which makes it inherently on-policy, it learns state-action
values $Q(x,a)$. Then, we can compute $V^\pi(x) = \max_{a} Q(x,a)$. Like in
TD-learning, we mix in the new esimate with the previous one according to
learning rate $\alpha_t$, \[
  \hat{Q}(x,a) \gets (1-\alpha_t) \hat{Q}(x,a) + \alpha_t \lft( r + \gamma \max_{a'\in\mathcal{A}} \hat{Q}(x',a') \rgt)
.\]
The advantage of Q-learning is that it is off-policy, because the value is
conditioned on the action. Thus, we can generate as much data as we need using a
different algorithm, such as $\epsilon$-greedy, and then estimate the Q-values
from there.

Again, Q-learning is optimal if it satisfies the Robbins Monro conditions and
all state-action pairs are chosen infinitely often. The space complexity of
Q-learning is $\bigo{|\mathcal{A}| \cdot |\mathcal{X}|}$.

\subsection{Model-free deep RL}

\marginnote{In deep RL, the input could be anything that machine learning can
process, \eg, video game frames with CNN or language with RNN.}

The problem with all previously discussed methods is that they are only feasible
in a small finite domain. In continuous domains, we would need an infinite
amount of memory to store all values. Thus, we need to approximate this
regression problem with function approximators, a.k.a. machine learning.

Let $V^\pi(x;\vec{\theta})$ be the function approximator that approximates the
value function. Just like in TD-learning, we make the following Monte Carlo
estimation for a given data point $\langle x, a, r, x' \rangle$, \[
  V^\pi(x) \approx r + \gamma V^\pi(x')
.\]
Then, we can define the loss function of our value function as the squared error
from the true value function,
\begin{align*}
  \ell(\vec{\theta} ; x, r, x') &\doteq \frac{1}{2} \lft( V^\pi(x;\vec{\theta}) - V^\pi(x) \rgt), \\
  \intertext{which we estimate by using the Monte Carlo estimation,}
  &\approx \frac{1}{2} \lft( V^\pi(x;\vec{\theta}) - \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt) \rgt)^2
.\end{align*}

The gradient of this loss is equal to the following, \[
  \grad{\ell(\vec{\theta} ; x,r,x')}{V^\pi(x;\vec{\theta})} = V^\pi(x;\vec{\theta}) - \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt)
.\]
Using stochastic gradient descent, we then get the following update rule,
\begin{align*}
  V^\pi(x;\vec{\theta}) &\gets V^\pi(x;\vec{\theta}) - \alpha_t \lft(V^\pi(x;\vec{\theta}) - \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt)\rgt) \\
  &= (1-\alpha_t) V^\pi(x;\vec{\theta}) + \alpha_t \lft( r + \gamma V^\pi\lft(x'; \vec{\theta}^{\mathrm{old}}\rgt)\rgt)
,\end{align*}
which is the same as the TD-learning update rule. Thus, the TD-learning update
rule can be viewed as gradient descent on the squared loss!

\paragraph{Deep Q-network.}

The same result
holds for the Q-learning update rule, where we do gradient descent on \[
  \ell(\vec{\theta}; x,a,x',r) = \frac{1}{2} \lft( Q(x,a;\vec{\theta}) - \lft(r + \gamma \max_{a'} Q\lft(x',a; \vec{\theta}^{\mathrm{old}}\rgt) \rgt) \rgt)^2
.\]
This equation is called the \textit{Bellman error}.

\begin{algorithm}
  \begin{algorithmic}
    \Function{DeepQNetwork}{$\tau$}
      \State {initialize $\vec{\theta}$}
      \While {not converged}
        \State {pop $\langle x,a,r,x' \rangle$ from $\tau$}
        \State {$\vec{\theta} \gets \vec{\theta} - \alpha_t \delta \grad{Q(x,a;\vec{\theta})}{\vec{\theta}}$}
        \State {where $\delta \doteq \lft( Q(x,a;\vec{\theta}) - \lft(r + \gamma \max_{a'} Q\lft(x',a; \vec{\theta}^{\mathrm{old}}\rgt) \rgt) \rgt)$}
      \EndWhile
    \EndFunction
  \end{algorithmic}
  \caption{Q-learning with function approximation.}
\end{algorithm}

However, this algorithm is quite slow to converge. To accelerate, we clone the
network and maintain a constant ``target`` value across episodes, which we
update once in a while.

Furthermore, deep Q networks suffer from ``maximization bias``, which means that
it tends to overestimate the actual Q value. This is caused by the
$\max$-operator used in the update rule, which also maximizes noise in the data.
This is solved by the double deep Q network, where we use two target networks.
We then take the minimum at each iteration of the two predictions by the two
target networks.

\paragraph{Policy search methods.}

The problem with deep Q networks is that if the action-space is large or
infinite, the $\max_{a'} Q(x',a;\vec{\theta})$ is no longer feasible. The
solution to this is learning a parametrized policy $\pi(x;\vec{\theta})$, where
the output is the action. In this case, we want to maximize the expected
trajectory reward,
\begin{align*}
  J(\vec{\theta}) &= \E_{x_{0:T},a_{0:T} \sim \pi_{\vec{\theta}}} \lft[ \sum_{t=0}^T \gamma^t r(x_t,a_t) \rgt] \\
                  &= \E_{\tau \sim \pi_{\vec{\theta}}} \lft[ r(\tau) \rgt]
.\end{align*}

\begin{theorem}
  The following holds, \[
    \grad{J(\vec{\theta})}{\vec{\theta}} = \grad{\E_{\tau \sim \pi_{\vec{\theta}}} [r(\tau)]}{\vec{\theta}} = \E_{\tau\sim \pi_{\vec{\theta}}}[r(\tau)\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}]
  .\]

  \label{thm:reinforce-grad}
\end{theorem}

\begin{proof}
  \begin{align*}
    \grad{J(\vec{\theta})}{\vec{\theta}} &= \grad{\int \pi_{\vec{\theta}}(\tau) r(\tau) d\tau}{\vec{\theta}} \\
    &= \int \grad{\pi_{\vec{\theta}}(\tau) r(\tau)}{\vec{\theta}} d\tau \\
    &= \int \pi_{\vec{\theta}}(\tau) r(\tau) \grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}} d\tau & \text{\footnotesize chain rule} \\
    &= \E_{\tau\sim \pi_{\vec{\theta}}}[r(\tau)\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}]
  .\end{align*}
\end{proof}

Using \Cref{thm:reinforce-grad}, we do not need to use the reparameterization
trick to be able to compute gradients. We only need to compute $\grad{\log
\pi_{\vec{\theta}}(\tau)}{\vec{\theta}}$. We can compute this gradient as
follows,
\begin{align*}
  \pi_{\vec{\theta}}(\tau) &= p(x_0) \prod_{t=0}^T \pi_{\vec{\theta}}(a_t\mid x_t) p(x_{t+1}\mid x_t,a_t) \\
  \grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}} &= \grad{\log p(x_0)}{\vec{\theta}} + \sum_{t=0}^T \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} + \sum_{t=0}^T \grad{\log p(x_{t+1}\mid x_t,a_t)}{\vec{\theta}} \\
  &= \sum_{t=0}^T \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}}
.\end{align*}
So, to be able to compute gradients \wrt $\vec{\theta}$, we do not even need to
know the underlying MDP! Putting this together, we get the following gradient, \[
  \grad{J(\vec{\theta})}{\vec{\theta}} = \E_{\tau\sim \pi_{\vec{\theta}}} \lft[ r(\tau) \sum_{t=0}^T \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} \rgt]
.\]

Even though these gradients are unbiased, they typically have large variance. We
can reduce the variance by introducing \textit{baselines}, \[
  \E_{\tau\sim \pi_{\vec{\theta}}}[r(\tau)\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}] = \E_{\tau\sim \pi_{\vec{\theta}}}[(r(\tau) - b(\tau))\grad{\log \pi_{\vec{\theta}}(\tau)}{\vec{\theta}}]
,\]
which have the same gradient. Thus, we are able to shift the reward up or down
without influencing the gradient.

REINFORCE (\Cref{alg:reinforce}) sets its baseline adaptively to be the
following, \[
  b_t(\tau) = \sum_{t'=0}^{t-1} \gamma^{t'} r_{t'}
.\]
Then, the gradient becomes the following, 
\begin{align*}
  \grad{J(\vec{\theta})}{\vec{\theta}} &= \E_{t\sim\pi_{\vec{\theta}}} \lft[ \sum_{t=0}^T \lft( \sum_{t'=0}^{T} \gamma^{t'} r_{t'} - \sum_{t'=0}^{t-1} \gamma^{t'} r_{t'} \rgt) \grad{\log \pi_{\vec{\theta}}(a_t\mid x_t)}{\vec{\theta}} \rgt] \\
  &= \E_{t\sim\pi_{\vec{\theta}}} \lft[ \sum_{t=0}^T \gamma^t \lft( \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'} \rgt) \grad{\log \pi_{\vec{\theta}}(a_t\mid x_t)}{\vec{\theta}} \rgt] \\
  &= \E_{t\sim\pi_{\vec{\theta}}} \lft[ \sum_{t=0}^T \gamma^t G_t \grad{\log \pi_{\vec{\theta}}(a_t\mid x_t)}{\vec{\theta}} \rgt]
,\end{align*}
where $G_t$ is the reward to go following action $a_t$. REINFORCE is an
on-policy algorithm, because it requires generating an episode for the data.
This is necessary to be able to update its parameters correctly.

\begin{algorithm}[t]
  \begin{algorithmic}[1]
    \Function{REINFORCE}{}
      \State {initialize $\vec{\theta}$}
      \Repeat
        \State {generate an episode $\tau$ using $\pi_{\vec{\theta}}$}
        \For {$t=1,\ldots,T$}
          \State {$G_t \gets r_t$}
          \State {$\vec{\theta} \gets \vec{\theta} + \eta \gamma^t G_t
          \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}}$}
        \EndFor
      \Until {done}
    \EndFunction
  \end{algorithmic}
  \caption{The REINFORCE algorithm, where the baseline at timestep $t$ is set to
  be $\sum_{t'=0}^{t-1} \gamma^{t'}r_{t'}$. \[ r(\tau) - b_t =
  \sum_{t'=t}^T \gamma^{t'-t} r_t \doteq G_t .\] Intuitively, $G_t$ is the reward to go
  following action $a_t$.}
  \label{alg:reinforce}
\end{algorithm}

\paragraph{Actor-critic methods.}

\marginnote{TODO: Actor-critic figure.}

We can reinterpret the REINFORCE gradient as follows,
\begin{align*}
  \grad{J(\vec{\theta})}{\vec{\theta}} &= \E_{\tau\sim\pi_{\vec{\theta}}} \lft[ \sum_{t=0}^T \gamma^t G_t \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} \rgt] \\
  &= \sum_{t=0}^\infty \E_{\tau_{t:\infty}} \lft[ \gamma^t G_t \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} \rgt] \margintag{Linearity of expectation} \\
  &= \sum_{t=0}^\infty \E_{x_t,a_t} \lft[ \gamma^t \E_{\tau_{t:\infty}} \lft[ \sum_{t'=t}^\infty \gamma^{t'-t} r_t \;\middle|\; x_t, a_t \rgt] \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} \rgt] \margintag{$G_t$ depends on everything after $t$, while the other terms depend only on $t$.} \\
  &= \E_{\tau \sim \pi_{\vec{\theta}}} \lft[ \sum_{t=0}^\infty \gamma^t Q^{\pi_{\vec{\theta}}}(x_t, a_t) \grad{\log \pi_{\vec{\theta}}(a_t \mid x_t)}{\vec{\theta}} \rgt] \margintag{The Q value is the value that we get after doing $a_t$ at $x_t$.} \\
  \intertext{Now, we can obtain the following,}
  &= \int \rho_{\vec{\theta}}(x) \E_{a \sim \pi_{\vec{\theta}}(x)} \lft[ Q(x,a) \grad{\log \pi_{\vec{\theta}} (a\mid x)}{\vec{\theta}} \rgt] dx \margintag{This is an abuse of notation, because $\rho_{\vec{\theta}}$ is an unnormalized probability distribution.} \\
  &= \E_{(x,a)\sim\pi_{\vec{\theta}}} \lft[ Q(x,a) \grad{\log \pi_{\vec{\theta}} (a\mid x)}{\vec{\theta}} \rgt]
,\end{align*}
where $\rho_{\vec{\theta}}(x) \doteq \sum_{t=0}^\infty \gamma^t p(x_t = x)$ is
the unnormalized, discounted state occupancy measure.

This result naturally suggests plugging in approximations for
$Q_{\vec{\theta}}(x,a)$ for the action-value function. The idea of actor-critic
networks is to parameterize an actor network that computes the policy and a
critic network that computes the Q-value. They can then be used in each others'
update equations, 
\begin{align*}
  \vec{\theta}_\pi &\gets \vec{\theta}_\pi + \eta_t Q_{\vec{\theta}}(x,a) \grad{\log \pi_{\vec{\theta}}(a\mid x)}{\vec{\theta}} \\
  \vec{\theta}_Q &\gets \vec{\theta}_Q - \eta_t \lft( Q_{\vec{\theta}}(x,a) - r - \gamma Q_{\vec{\theta}}(x',\pi_{\vec{\theta}}(x')) \rgt) \grad{Q_{\vec{\theta}}(x,a)}{\vec{\theta}}
.\end{align*}
Furthemore, we can introduce baselines by adding a value network,
\begin{align*}
  \vec{\theta}_\pi &\gets \vec{\theta}_\pi + \eta_t ( Q_{\vec{\theta}}(x,a) - V_{\vec{\theta}}(x)) \grad{\log \pi_{\vec{\theta}}(a\mid x)}{\vec{\theta}} \\
  &= \vec{\theta}_\pi + \eta_t A(x,a) \grad{\log \pi_{\vec{\theta}}(a\mid x)}{\vec{\theta}}
,\end{align*}
where $A(x,a) \doteq Q(x,a) - V(x)$ is the advantage function, which is
positive if the chosen action is better than expected and negative if worse.
Thus, intuitively, we increase the probability of the chosen action if better
than expected, otherwise we decrease it. This model is called \textit{advantage
actor critic} (A2C) \citep{mnih2016asynchronous}.

All models discussed so far have been on-policy, which often causes sample
inefficiency. Now, we want to move to off-policy methods. Recall that our
initial motivation was that finding the maximum Q value was intractable if the
action space was infinite. But, we could also replace the exact maximum by a
parametrized policy, \[
  J(\vec{\theta}) = \sum_{(x,a,r,x')\in \mathcal{D}} \lft( Q_{\vec{\theta}}(x,a; \vec{\theta}_Q) - \lft( r + \gamma Q \lft( x', \pi(x'; \vec{\theta}_\pi); \vec{\theta}_Q^{\text{old}} \rgt) \rgt) \rgt)^2
,\]
where we jointly optimize over $\vec{\theta}_Q$ and $\vec{\theta}_\pi$. We want
to follow the greedy policy \wrt the Q function, \ie, we want
$\pi_{\vec{\theta}} \approx \pi_Q = \argmax_{a\in\mathcal{A}}
Q(x,a;\vec{\theta}_Q)$. The key idea is that if we use a ``rich enough``
parameterization of policies, selecting the greedy policy \wrt $Q$ is
equivalent to the following, \[
  \vec{\theta}_\pi^\star = \argmax_{\vec{\theta}_\pi} \E_{x\sim \mu} [Q(x, \pi(x; \vec{\theta}_\pi); \vec{\theta}_Q)]
,\]
where $\mu(x) > 0$ is an \textit{exploration distribution} over states with
full support. If we then use differentiable approximations of $Q$ and a
differentiable deterministic policy $\pi$, we can use backpropagation to obtain
gradients, \[
  \grad{Q(x, \pi(x; \vec{\theta}_\pi); \vec{\theta}_Q)}{\vec{\theta}_\pi} = \grad{Q(x,\vec{\theta}_\pi)}{\pi(x; \vec{\theta}_\pi)} \grad{\pi(x; \vec{\theta}_\pi)}{\vec{\theta}_\pi}
.\]
However, policy gradient methods rely on randomized policies for exploration,
but we have deterministic policies. To encourage exploration, we can inject
additional Gaussian action noise to encourage exploration, akin to
$\epsilon$-greedy exploration. This is called the \textit{deep deterministic
policy gradients} (DDPG) algorithm \citep{lillicrap2015continuous}.

\textit{Twin delayed deep deterministic} (TD3) further improves this by
introducing a second critic network to address maximization bias
\citep{fujimoto2018addressing}. \textit{Soft-actor critic} (SAC) further
improves this by adding entropy regularization, \[
  J_{\lambda}(\vec{\theta}) = J(\vec{\theta}) + \lambda H(\pi_{\vec{\theta}})
,\]
which encouraging exploration by giving preference to high-entropy actions
\citep{haarnoja2018soft}.

\subsection{Model-based deep RL}

So far, we have only discussed deep model-free methods. However, if we have an
accurate model of the environment, we can use it for planning. The main benefit
of this is that it dramatically reduces the sample complexity, compared to
model-free techniques. In other words, we need much less data to find a good
policy.

\paragraph{Planning in a known model.}

We assume a continuous state and action space with non-linear transitions,
without constraints. Thus, this is quite a bit more complex than solving MDPs.
We have a deterministic transition function $f$ and a reward function $r$. The
objective then becomes the following, \[
  J_\infty(a_{0:\infty}) = \sum_{t=0}^\infty \gamma^t r(x_t,a_t) \margintag{such that $x_{t+1} = f(x_t,a_t)$ for every $x_t$.}
.\]
However, we cannot plan over an infinite horizon. The key idea is to plan over
a finite horizon $H$, carry out the first action, then replan with a horizon
$H$. Thus, we first optimize over the following \[
  J_H(a_{t:t+H-1}) = \sum_{t'=t}^{t+H-1} \gamma^{t'-t} r(x_t,a_t) \margintag{such that $x_{t+1} = f(x_t,a_t)$ for every $x_t$.}
,\]
carry out action $a_t$ and then replan. We can optimize this function using
gradient methods (backpropagation through time) if the actions, rewards, and
dynamics are differentiable. However, there are often many local minima, and
vanishing/exploding gradients are a problem, because we do backpropagation
through time.\sidenote{This is the same problem that recurrent neural networks
have.} Thus, we often use heuristic global optimization methods. The
\textit{random shooting} method generates $m$ sets of random samples
$a_{t:t+H-1}$, and then picks the sequence that optimizes the objective.

TODO: Figure of finite horizon (see slide 8).

However, if we have sparse rewards, this will inherently not work, because we
might not look far enough into the future. But, if we have access to a value
function estimate, we can use that to see beyond the finite horizon, giving us
the following object function, \[
  J_H(a_{t:t+H-1}) = \sum_{t'=t:t+H-1} \gamma^{t'-t} r_t + \gamma^H V(x_{t+H})
,\]
where, intuitively, $V(x_{t+H})$ summarizes the remaining infinite timesteps.
For $H=1$, this is equal to the greedy policy \wrt $V$, but if we use larger
$H$, this converges faster.

If the transition function is stochastic, rather than deterministic, we have to
optimize over the expected performance, \[
  J_H(a_{t:t+H-1}) = \E_{x_{t+1:t+H}} \lft[ \sum_{t'=t}^{t+H-1} \gamma^{t'-t} r_t + \gamma^H V(x_{t+H}) \;\middle|\; a_{t:t+H-1} \rgt]
.\]
However, the problem is that this expectation requires solving a
high-dimensional integral. A common solution is to use \textit{Monte Carlo
trajectory sampling}. For this we use the reparameterization trick to obtain
unbiased Monte Carlo estimates, \[
  \hat{J}_H(a_{t:t+H-1}) = \frac{1}{m} \sum_{i=1}^m \sum_{t'=t}^{t+H-1} \gamma^{t'-t} r_t(x_t\lft( a_{t:t'-1}, \epsilon^{(i)}_{t:t'-1} \rgt), a_t) + \gamma^H V(x_{t+H}) \margintag{The state is a function of the previous actions $a_{t:t'-1}$ and noise $\epsilon^{(i)}_{t:t'-1}$ that is needed for the Gaussian reparameterization trick.}
.\]

Instead of optimizing over the actions, we could also optimize over
parametrized policies $\pi_{\vec{\theta}}$. This replaces expensive online
planning by offline training of a policy that is fast to evaluate online. The
objective then becomes the following, \[
  J(\vec{\theta}) = \E_{x_0 \sim \mu} \lft[ \sum_{t=0}^{H-1} \gamma^t r_t + \gamma^H Q(x_H, \pi_{\vec{\theta}}(x_H)) \rgt]
.\]
For $H=0$, this is equivalent to the DDPG objective.

\paragraph{Learning the model.}

Until now, we have assumed known $f$ and $r$. In RL, these are unknown, so we
have to learn them. The key insight is that due to the Markovian structure of
the MDP, the observed transitions and rewards are conditionally independent. We
can estimate them off-policy with standard supervised learning techniques from
a dataset of trajectories. We train a model with inputs $(x_t,a_t)$ and the
model outputs $(r_t,x_{t+1})$. For continuous state spaces, this is essentially
just a regression problem.

We could use an MAP estimate, but errors in the model estimate compound, which
the planning algorithms exploit, which results in poor performance. This can
easily be remedied by capturing uncertainty (Gaussian processes, Bayesian
neural networks) in the estimated model and taking it into account in planning.

\paragraph{Exploration and exploitation.}

An algorithm that we can use to balance exploration and exploitation is
Thompson sampling, which we have already seen before. We sample a model, plan a
new policy according to it, roll out policy to collect more data, and finally
update the posterior.
